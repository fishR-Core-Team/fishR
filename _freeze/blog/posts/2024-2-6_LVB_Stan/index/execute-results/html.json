{
  "hash": "3d621546f104707a79e33aad31c82977",
  "result": {
    "markdown": "---\ntitle: Bayesian LVB II - rstan\ndescription: Tutorial on how to estimate parameters of the LVB model with Bayesian inference and rstan\nauthor: Jason Doll\ndate: 2/6/2024\ncategories:\n  - growth\n  - Bayesian\n  - Stan\n---\n\n\n# Introduction\n\nThe use of Bayesian inference in fisheries biology has been increasing. For good reason, as there are many benefits to taking a Bayesian approach. I won't go into those reasons here but you can read about them in @dorazio_2016 and @dolljacquemin_2018. This post assumes you have already decided to use Bayesian methods and will present how to estimate parameters of the von Bertalanffy growth model. Previous posts describe frequentist methods [here](../2019-12-31_vonB_plots_1/) and [here](../2020-1-2_vonB_plots_2/)\n\nThis is the second post where I describe how to fit a model using Bayesian inference. The [first post](../2024-2-5_LVB_brms/) used Stan with the `brms` package. This post will use `rstan` and I will write the full Stan model code.\n\nBoth methods will fit the typical three parameter von Bertalanffy growth model\n\n$$\nTL_i=L_\\infty * (1-e^{(-\\kappa * (t_i-t_0))} )\n$$\n2here $TL_i$ is total length of individual *i*, $L_\\infty$ is the average maximum length obtained, $\\kappa$ is the Brody growth coefficient, $t_i$ is the age of individual *i*, and $t_0$ is the theoretical age at zero length. To finish the model, an error term is added:\n\n$$\nTL_i=L_\\infty * (1-e^{(-\\kappa * (t_i-t_0))} + \\epsilon_i)\n$$\n$$\n\\epsilon_i \\sim normal(0,\\sigma)\n$$\nwhere $\\epsilon_i$ is a random error term for individual *i* with a mean of 0 and standard deviation $\\sigma$.\n\n# Prior probabilities\nAt the heart of Bayesian analysis is the prior probability distribution.This post will use non-informative prior probability distributions. When and how to use informative priors when fitting a von Bertalanffy growth model will be discussed in a future post. However, you can read about it in @dolljacquemin_2018. The prior probability distributions used in this post are:\n\n| Parameter  | Prior Probability Distribution |\n|------------|--------------------------------|\n| $L_\\infty$ | normal(0,1000)                 |\n| $\\kappa$   | normal(0,10)                   |\n| $t_0$      | normal(0,10)                   |\n| $\\sigma$   | student-t(3,0,30)              |\n\nStan parameterizes the normal distribution with the mean and standard deviation and the student-t distribution with the degrees of freedom, mean, and standard deviation\n\n# Preliminaries\nFirst step is to load the necessary packages\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(FSA)\nlibrary(FSAdata)   # for data\nlibrary(dplyr)     # for filter(), select()\nlibrary(tidyr)     # for transposing data wide to long\nlibrary(ggplot2)   # for potting\nlibrary(rstan)     # for fitting Stan models\nlibrary(tidybayes) # for plotting posterior results\nlibrary(bayesplot) # for plotting posterior predictive checks\n```\n:::\n\n\n# Data\n\nThe `WalleyeErie2` data available in the [`FSAdata`](https://fishr-core-team.github.io/FSAdata/) package was used in previous posts demonstrating von Bertalanffy growth models and will once again be used here. These data are Lake Erie Walleye (*Sander vitreus*) captured during October-November, 2003-2014. As before, the primary interest here is in the tl (total length in mm) and age variables. The data will also be filtered to focus only on female Walleye from location `1` captured in 2014.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(WalleyeErie2,package=\"FSAdata\")\nwf14T <- WalleyeErie2 %>%\n  filter(year==2014,sex==\"female\",loc==1) %>%\n  select(-year,-sex,-setID,-loc,-grid)\nheadtail(wf14T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n#R|       tl    w      mat age\n#R|  1   445  737 immature   2\n#R|  2   528 1571   mature   4\n#R|  3   380  506 immature   1\n#R|  323 488 1089 immature   2\n#R|  324 521 1408   mature   3\n#R|  325 565 1745   mature   3\n```\n:::\n:::\n\n\nNext step is to create a list to hold the data used in the model. One important note, the items in this list must match the data used in the model code. For example, we will use `tl` for total length in the data list and `tl` must be specified in the Stan data block exactly as `tl`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndataList=list(\n  tl=wf14T$tl,\n  age=wf14T$age ,\n  N=length((wf14T$age))\n)\n```\n:::\n\n\n# Initial values\nAn optional step is to specify initial values for the parameters. It is always good practice to specify initial values, particularly with non-linear and other complex models. I will fit the model using multiple chains and it is advisable to use different starting values for each chain. To accomplish this, we will specify a function and use a random number generator for each parameter Adjust the range for the uniform distribution to cover a large range of values that make sense for your data. You can use other distributions as long as they match the declared range in the model code. In this example, I am using the random uniform function because $L_\\infty$ and $\\kappa$ are restricted to be positive in the model. Therefore, the starting value must be positive.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninitsLst <- function() list(\n  Linf=runif(1, 200, 800),\n  K=runif(1, 0.05, 3.00),\n  t0 =rnorm(1, 0, 0.5)\n)\n```\n:::\n\n\n# Stan\nNow we can introduce the Stan model code. Stan models are structured with \"blocks\". Generally, the minimum number of blocks used is three; data, parameters, and model blocks. In our example, we will use a fourth block, the generated quantities block. All of the blocks must be in a single .stan file. The blocks are presented separately here for demonstration. Other blocks that can be used in Stan are; functions, transformed data, and transformed parameters. See the Stan manual for a description of each.\n\nThe data block declares all of the data needed for the model and the data object must match how they are entered in the data list above. There are several different types of data that can be declared. For this example, I use `int` to declare `N`, which is the number of observations, as an integer. You'll also notice that I include a lower bounds for this variable. This isn't necessary but good practice if your variable has a floor or ceiling. `tl` is declared as a `real` data type. The `real` data type is typically used for single numbers or a vector of numbers. This example declares the real data type as a vector of length `N`. `age` is another vector of length `N` but it is a vector of integers, `int`.\n\n\n::: {.cell layout-align=\"center\" output.var='LVBmodel'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> N ;  // number of observations\n  real tl[N] ;      // Total lengths of individual fish\n  int age[N] ;      // Age of individual fish\n}\n```\n:::\n\n\nThe second block in our model is a parameters block. This block will declare all the estimated parameters in the model. Our model has four parameters. `sigma_y` is the standard deviation, $\\sigma$, of the individual observations; `t0` is the $t_0$ LVB parameter; `Linf` is the $L_\\infty$ parameter; and `k` is the $\\kappa$ parameter. Each is declared as a single real value. All of the parameters are constrained in at least one direction. `sigma_y` is constrained to be positive using the \\<lower=0\\> argument because standard deviations must be positive. `t0` is being constrained to be between -5 and 5 using the `\\<lower=-5\\>` and `\\<upper=5\\>` arguments. This is declared based on prior knowledge to prevent the sampler from searching outside these bounds. The remaining two, `Linf` and `k`, are only constrained to be positive using `\\<lower=0\\>`.\n\nSetting bounds can help the sampler find the posterior distribution, particularly with complex models. So it is recommended to always set bounds that \"make sense\". Use your knowledge of the system and the model to declare these bounds. This is separate from declare your prior probability distribution but setting bounds will influence your prior probability distributions. The priors are set in the model block.\n\n\n::: {.cell layout-align=\"center\" output.var='LVBmodel'}\n\n```{.stan .cell-code}\nparameters {\n  real<lower=0> sigma_y;              //LVB standard deviation\n  real<lower=-5, upper=5> t0;         //LVB t0 parameter\n  real<lower=0> Linf;                 //LVB Linf parameter\n  real<lower=0> k;                    //LVB k parameter\n}\n```\n:::\n\n\nThe third block declares the model. The model block starts off with declaring any local variables that might be needed. Note that any variables declared in the model block can't be referenced in other blocks. In this case, I need a vector `ypred` of length `N` to hold the mean total length which is a function of the von Bertlanffy growth model. This is followed by our declaration of prior probability distributions. Stan will automatically assign uniform prior probabilities if a prior is not specified here. I don't recommend letting Stan declare uniform priors by default. Always be explicit in the prior probability distribution in your model and assign prior probability distributions that are suitable for the parameter and defensible against a skeptical audience. A future post will dive deeper into setting and selecting informative vs non-informative prior probability distribution.\n\nThe final part of the model block is the likelihood. This will include a for loop that will iterate through each of the observed values and include the model. Following this is the `normal()` distribution which specifies the distribution of our observations and is parameterized with the mean `y_mean` and standard deviation `sigma_y`. This last line can be indexed over individuals and included in the for loop. Or vectorized outside the loop, as it is written here.\n\n\n::: {.cell layout-align=\"center\" output.var='LVBmodel'}\n\n```{.stan .cell-code}\nmodel {\n  vector[N] ypred; //variable used in the model block only\n\n  //The next three lines of code specify reference prior probability distributions.\n  sigma_y ~ student_t(3,0,40);  \n  Linf ~ normal(0, 1000);\n  k ~ normal(0, 100);\n  t0 ~ normal(0, 10); \n  \n  // calculate likelihood of data\n  for(i in 1:N){\n    y_mean[i]= Linf * (1-exp(-(k * (age[i]-t0) )) );\n  }\n  tl~normal(y_mean, sigma_y);\n}\n```\n:::\n\n\nThe last block needed is a generated quantities block. This block is used to calculate derived variables, posterior predictions, and to generate model predictions. I will use it here to generate a vector named `y_rep` of length `N` that will hold individual predicted observations. `y_rep` is the posterior predicted values of total length that will be used to plot the observed and model predicted total length. The posterior predicted distribution is generated using the `normal_rng` function which generates a random value from the normal distribution based on a mean (the model) and standard deviation. The posterior predicted distribution generated in in a for loop to iterate through each observed individual.\n\n\n::: {.cell layout-align=\"center\" output.var='LVBmodel'}\n\n```{.stan .cell-code}\ngenerated quantities{\n  //the next four lines of code generate predicted values to use for inspecting model fit\n  vector[N] y_rep;\n  for(i in 1:N){\n    y_rep[i] = normal_rng(Linf * (1-exp(-(k * (age[i]-t0) )) ) ,sigma_y);\n  }\n}\n```\n:::\n\n\n\n\nBefore sending the model to `rstan`, the full model code needs to be saved into a file with the .stan extension. For example \"Walleye_lvb.stan\". The .stan file also has to be saved in the working directory. \n\nNow it is time to bundle all the pieces together.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Compile and fit model using Stan \nLVBfit <- stan(file='Walleye_lvb.stan', # The file containing the Stan model\n                data=dataList,          # a list containing the data\n                init=initsLst,          # a list containing initial values \n                chains=3,               # number of chains, typically 3 to 4\n                cores=3,                # number of cores for multi-core processing;\n                                        #  Typically set to match number of chains\n                iter=3000 ,             # number of iterations\n                warmup=1000 ,           # number of warm up steps to discard\n                control=list(adapt_delta=0.80,  # Adjustments to the algorithm\n                             max_treedepth=15)) # to improve convergence.\n```\n:::\n\n\n\n\n# Assess convergence\nLet's inspect convergence and how well the model fits the data. The first thing to do is to examine the trace plots and histograms of the important model parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(LVBfit, plotfun=\"trace\", pars=c(\"sigma_y\",\"Linf\",\"k\",\"t0\"), inc_warmup=FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=528}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(LVBfit, plotfun=\"hist\", pars=c(\"sigma_y\",\"Linf\",\"k\",\"t0\"), inc_warmup=FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=528}\n:::\n:::\n\n\nThe chains appear to have mixed well for all parameters and reached a stationary posterior. This is seen by a unimodal distribution in the histograms and caterpillar plots for each parameter appear \"on top\" of each other. We can move on to assessing how well the model fits the data using a posterior predictive check.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny_rep <- as.matrix(LVBfit, pars=\"y_rep\") #Extract the posterior predicted observations\nppc_dens_overlay(wf14T$tl, y_rep[1:50, ])  #Density plot of y and y_rep\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=336}\n:::\n:::\n\n\nThe y_rep curves do a good job at covering our observations and we can conclude the model does a sufficient job at representing the data. Now we can move on to look at individual parameters.\n\n# Posterior summary\nBecause we included a generated quantities block and are calculating the posterior predicted distribution, our output if very long. The `summary(LVBfit)` command would return a vary long summary of all the parameters and individual predicted total lengths. Instead, I use the `print()` function here and only specify the parameters of interest.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(LVBfit, pars=c(\"sigma_y\",\"Linf\",\"k\",\"t0\"), probs=c(0.025,0.5,0.975),digits_summary=6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n#R|  Inference for Stan model: anon_model.\n#R|  3 chains, each with iter=3000; warmup=1000; thin=1; \n#R|  post-warmup draws per chain=2000, total post-warmup draws=6000.\n#R|  \n#R|                mean  se_mean       sd       2.5%        50%      97.5% n_eff\n#R|  sigma_y  18.375510 0.015013 0.728324  16.991915  18.356850  19.863256  2354\n#R|  Linf    648.525800 0.245133 9.889823 629.908299 648.309777 668.551357  1628\n#R|  k         0.361858 0.000531 0.020774   0.321784   0.361293   0.403543  1529\n#R|  t0       -1.286204 0.002160 0.086174  -1.459206  -1.284131  -1.121483  1592\n#R|              Rhat\n#R|  sigma_y 1.002453\n#R|  Linf    1.008203\n#R|  k       1.008982\n#R|  t0      1.007506\n#R|  \n#R|  Samples were drawn using NUTS(diag_e) at Sun Feb  4 20:31:13 2024.\n#R|  For each parameter, n_eff is a crude measure of effective sample size,\n#R|  and Rhat is the potential scale reduction factor on split chains (at \n#R|  convergence, Rhat=1).\n```\n:::\n:::\n\n\nThe summary table provides the point estimates, 95% credible intervals, Rhat values, and n_eff. The Rhat values are another check to assess model convergence. It is generally accepted that you want Rhat values less than 1.10. The n_eff is also used to assess convergence. n_eff refers to \"Effective Sample Size\". Because of the nature of MCMC methods, each successive sample from the posterior will typically be autocorrelated within a chain. Autocorrelation within the chains can increase uncertainty in the estimates. One way to assessing how much autocorrelation is present and big of an effect it might be, is with the \"Effective Sample Size\". ESS represents the number of independent draws from the posterior. The ESS will be lower than the actual number of draws and you are looking for a high ESS. It has been recommended that an ESS of 1,000 for each parameter is sufficient @burkner_2017.\n\n# Posterior plotting\nThe final part is to create a figure with observed and predicted values. This code block is a little different than what was used with brms. Instead of `add_predicted_draw()`, I have to combine the predicted draws manually then convert them from wide to long.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncbind(tl=wf14T$tl, age=wf14T$age,data.frame(t(y_rep))) %>%   # Combine observed data with predictions\n        pivot_longer(col=-c(tl,age),values_to=\".prediction\") %>% \n  ggplot(aes(x=age, y=tl)) +  \n  stat_lineribbon(aes(y=.prediction), .width=c(.95, .80, .50),  # regression line and CI\n                  alpha=0.5, colour=\"black\") +\n  geom_point(data=wf14T, colour=\"darkblue\", size=3) +   # raw data\n  scale_fill_brewer(palette=\"Greys\") +\n  ylab(\"Total length (mm)\\n\") + \n  xlab(\"\\nAge (years)\") +\n  theme_bw() +\n  theme(legend.title=element_blank(),\n        legend.position=c(0.15, 0.85))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=336}\n:::\n:::\n\n\nThe figure above shows the observed data in blue circles, the prediction line as a solid black line, and the posterior prediction intervals (0.50, 0.80, and 0.95) in different shades of gray.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}