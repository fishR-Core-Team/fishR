[
  {
    "objectID": "teaching/resources/YEPsum.html",
    "href": "teaching/resources/YEPsum.html",
    "title": "Yellow Perch Harvest Summaries",
    "section": "",
    "text": "Table 1: Frequency, percent, and reverse cumulative sum of percent of anglers by number of Yellow Perch harvested.\n\n\n\n\n\n\n\nHarvested\nFrequency\n%\nReverse Cum %\n\n\n\n\n0\n277\n17.2%\n100.0%\n\n\n10\n920\n57.0%\n82.8%\n\n\n20\n244\n15.1%\n25.9%\n\n\n30\n87\n5.4%\n10.8%\n\n\n40\n44\n2.7%\n5.4%\n\n\n50\n26\n1.6%\n2.7%\n\n\n60\n5\n0.3%\n1.1%\n\n\n70\n2\n0.1%\n0.7%\n\n\n80\n3\n0.2%\n0.6%\n\n\n90\n2\n0.1%\n0.4%\n\n\n100\n5\n0.3%\n0.3%\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nFigure 1: Percentage (left) and reverse cumulative percentage (right) of anglers by number of Yellow Perch harvested."
  },
  {
    "objectID": "teaching/resources/Sunfishsum.html",
    "href": "teaching/resources/Sunfishsum.html",
    "title": "Sunfish Harvest Summaries",
    "section": "",
    "text": "Table 1: Frequency, percent, and reverse cumulative sum of percent of anglers by number of Sunfish harvested.\n\n\n\n\n\n\n\nHarvested\nFrequency\n%\nReverse Cum %\n\n\n\n\n0\n771\n38.9%\n100.0%\n\n\n3\n451\n22.7%\n61.1%\n\n\n6\n306\n15.4%\n38.4%\n\n\n9\n152\n7.7%\n22.9%\n\n\n12\n134\n6.8%\n15.3%\n\n\n15\n18\n0.9%\n8.5%\n\n\n18\n55\n2.8%\n7.6%\n\n\n21\n24\n1.2%\n4.8%\n\n\n24\n22\n1.1%\n3.6%\n\n\n27\n26\n1.3%\n2.5%\n\n\n29\n10\n0.5%\n1.2%\n\n\n30\n14\n0.7%\n0.7%\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nFigure 1: Percentage (left) and reverse cumulative percentage (right) of anglers by number of Sunfish harvested."
  },
  {
    "objectID": "teaching/resources/LMBsum.html",
    "href": "teaching/resources/LMBsum.html",
    "title": "Largemouth Bass Harvest Summaries",
    "section": "",
    "text": "Table 1: Frequency, percent, and reverse cumulative sum of percent of anglers by number of Largemouth Bass harvested.\n\n\n\n\n\n\n\nHarvested\nFrequency\n%\nReverse Cum %\n\n\n\n\n0\n860\n75.0%\n100.0%\n\n\n1\n210\n18.3%\n25.0%\n\n\n2\n58\n5.1%\n6.6%\n\n\n3\n5\n0.4%\n1.6%\n\n\n4\n7\n0.6%\n1.1%\n\n\n5\n3\n0.3%\n0.5%\n\n\n6\n3\n0.3%\n0.3%\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nFigure 1: Percentage (left) and reverse cumulative percentage (right) of anglers by number of Largemouth Bass harvested."
  },
  {
    "objectID": "teaching/posts/2022-12-23_Removal_EBullhead/index.html",
    "href": "teaching/posts/2022-12-23_Removal_EBullhead/index.html",
    "title": "Abundance from Depletion Data",
    "section": "",
    "text": "Pritchard et al. (2021) used three methods to determine the abundance of benthic European Bullhead (Cottus gobio) in a stony headwater stream in Northern Yorkshire, England. In one part of their study they set block nets at the up- and down-stream ends of a site called “Footbridge.” The researchers then used a backpack electrofishing unit deployed from the lower to the upper limit of the site to capture and remove all Bullheads from the site (the removed fish were placed downstream below the lower block net). After a short break, this process was repeated two more times. A total of 138, 55, and 48 Bullheads were removed on the successive electrofishing passes. Use this information to answer the questions below.\n\nWhat is the best estimate and 95% confidence interval for the population size? How does your estimate compare to the estimate published in Pritchard et al. (2021)?\nWhat is the best estimate and 95% confidence interval for the probability of capture during each electrofishing pass.\nPritchard et al. (2021) estimated a “capture efficiency.” Use your results to show and then describe what they mean by this.\nIs there evidence that the probability of capture differed after the first pass?\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\nReferences\n\nPritchard, E. G., D. D. A. Chadwick, M. A. Chadwick, P. Bradley, C. D. Sayer, and J. C. Axmacher. 2021. Assessing methods to improve benthic fish sampling in a stony headwater stream. Ecological Solutions and Evidence 2(4):e12111.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2022-12-23_MR_SeaLamprey/index.html",
    "href": "teaching/posts/2022-12-23_MR_SeaLamprey/index.html",
    "title": "Abundance from Mark-Recapture Data",
    "section": "",
    "text": "Bergstedt et al. (2003) used the Petersen (with the Chapman modification) mark-recapture method to estimate the population size of parasitic Sea Lamprey (Petromyzon marinus) in Lake Huron. Use their results1 from “Pre” period and the 1991 parasitic feeding year in Table 3 (use the M, C, and R rows) to confirm the population estimate (in the first row of the table) and, hopefully, the confidence interval (in the second and third rows of the table). Show your work and write a formal statement about how your results compare to those in the paper.\n1 Follow link in citation below.\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBergstedt, R. A., R. B. McDonald, M. B. Twohey, K. M. Mullett, R. J. Young, and J. W. Heinrich. 2003. Reduction in Sea Lamprey hatching success due to release of sterilized males. Journal of Great Lakes Research 29:435–444.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2022-12-22_BagLimits_MN/index.html",
    "href": "teaching/posts/2022-12-22_BagLimits_MN/index.html",
    "title": "Bag Limits in Minnesota",
    "section": "",
    "text": "Daily bag or creel limits are the maximum number of fish an angler is allowed to harvest on a daily basis. For example, a bag limit of six Walleye would limit an angler to six or fewer harvested Walleye on a given day. Bag limits are a common management tool in many fisheries, especially in inland freshwater fisheries.\nRestrictive bag limits may be implemented for a variety of reasons. Most commonly it is thought that limiting the maximum number of fish an angler can harvest will reduce harvest, lower fishing mortality, and prevent over-exploitation of the fish population. However, bag limits have also been implemented to attempt to equitably distribute harvest among more anglers, provide anglers the satisfaction of reaching a goal (e.g., “I got my limit”), and to remind anglers that fish populations are a finite resource.\n\n\n\n\n\nAn angler’s harvest of Bluegill.\n\n\n\n\nIn this exercise, you will examine creel data (i.e., number of fish harvested by anglers) to determine how reducing the daily bag limit effects overall fish harvest.\n\n\n\nThe Minnesota Department of Natural Resources (MnDNR) conducted statewide creel surveys over a 17 year period (Cook et al. 2001). The data, available in CreelMN1 records the species and number of fish harvested per angler (in one day). The data are limited to only six species2 – Walleye (WAE), Largemouth Bass (LMB), Northern Pike (NOP), Crappies (Black, White, and hybrids combined), Sunfish (Bluegill, Pumpkinseed, etc.), and Yellow Perch (YEP).\n1 See “CSV file” link in “Source” section of linked page.2 Acronyms are how the species appear in the data.The following daily bag limits were in place during the study:\n\n6 Walleye\n6 Largemouth Bass\n3 Northern Pike\n15 Crappies\n30 Sunfish\n100 Yellow Perch"
  },
  {
    "objectID": "teaching/posts/2022-12-22_BagLimits_MN/index.html#motivation",
    "href": "teaching/posts/2022-12-22_BagLimits_MN/index.html#motivation",
    "title": "Bag Limits in Minnesota",
    "section": "",
    "text": "Daily bag or creel limits are the maximum number of fish an angler is allowed to harvest on a daily basis. For example, a bag limit of six Walleye would limit an angler to six or fewer harvested Walleye on a given day. Bag limits are a common management tool in many fisheries, especially in inland freshwater fisheries.\nRestrictive bag limits may be implemented for a variety of reasons. Most commonly it is thought that limiting the maximum number of fish an angler can harvest will reduce harvest, lower fishing mortality, and prevent over-exploitation of the fish population. However, bag limits have also been implemented to attempt to equitably distribute harvest among more anglers, provide anglers the satisfaction of reaching a goal (e.g., “I got my limit”), and to remind anglers that fish populations are a finite resource.\n\n\n\n\n\nAn angler’s harvest of Bluegill.\n\n\n\n\nIn this exercise, you will examine creel data (i.e., number of fish harvested by anglers) to determine how reducing the daily bag limit effects overall fish harvest."
  },
  {
    "objectID": "teaching/posts/2022-12-22_BagLimits_MN/index.html#data",
    "href": "teaching/posts/2022-12-22_BagLimits_MN/index.html#data",
    "title": "Bag Limits in Minnesota",
    "section": "",
    "text": "The Minnesota Department of Natural Resources (MnDNR) conducted statewide creel surveys over a 17 year period (Cook et al. 2001). The data, available in CreelMN1 records the species and number of fish harvested per angler (in one day). The data are limited to only six species2 – Walleye (WAE), Largemouth Bass (LMB), Northern Pike (NOP), Crappies (Black, White, and hybrids combined), Sunfish (Bluegill, Pumpkinseed, etc.), and Yellow Perch (YEP).\n1 See “CSV file” link in “Source” section of linked page.2 Acronyms are how the species appear in the data.The following daily bag limits were in place during the study:\n\n6 Walleye\n6 Largemouth Bass\n3 Northern Pike\n15 Crappies\n30 Sunfish\n100 Yellow Perch"
  },
  {
    "objectID": "teaching/posts/2022-12-20_SizeStrux_Whittlesey/index.html",
    "href": "teaching/posts/2022-12-20_SizeStrux_Whittlesey/index.html",
    "title": "Size Structure of Species in a Cold-Water Wisconsin Stream",
    "section": "",
    "text": "Fish length is likely most commonly collected data by fisheries biologists and managers. While length is usually not the only information collected by a fisheries biologist, summaries of length information can provide the biologist with a wealth of information on population parameters such as age distributions, growth rates, and mortality rates; the basic biology of the animal; and the effect of management regulations on the fish population. Thus, summarizing length data and then interpreting those summaries is an important task performed by fisheries biologists.\n\n\n\n\n\nMeasuring length of a Rainbow Trout.\n\n\n\n\n \n\n\n\nWhittlesey Creek is a tributary to Chequamegon Bay of Lake Superior. This stream has been severely degraded by past land use practices including clear-cut logging and channelization to drain wetlands for farming. The “Coaster” Brook Trout (Salvelinus fontinalis), a lake-dwelling stream-spawning form of Brook Trout, was largely extirpated from southern Lake Superior and the Whittlesey Creek watershed due to over-fishing and habitat degradation. In 1999, the U.S., Fish and Wildlife Service established the Whittlesey Creek National Wildlife Refuge with a primary goal of reestablishing a population of Coaster Brook Trout in Whittlesey Creek. That effort has included experimental introductions of Brook Trout at various life stages and much work to restore habitat.\nIn July, 2011 a quarter-mile stretch of the lower Whittlesey Creek was modified with the addition of large woody debris with attached “root balls.” The goal of this project was to alter the morphology of this section of the stream to provide better habitat and water conditions for Brook Trout. In May, 2011 a group of students conducted two days of sampling with electrofishing gear to provide baseline information about the fish populations in this section of the stream. Their sampling yielded eight total Brook Trout but relatively larger numbers of Coho Salmon (Oncorhynchus kisutch), Rainbow Trout (Oncorhynchus mykiss), and sculpins (primarily Slimy Sculpin (Cottus cognatus), but some Mottled Sculpin (Cottus bairdii)). Data from catches of Coho Salmon, Rainbow Trout, and sculpins will be analyzed in this case study.\n\n\n\n\n\nStudents backpack electrofishing a stream.\n\n\n\n\n \n\n\n\nData from the sampling collected by the students were stored in Whittlesey2011.csv. The variables in this data set are defined as follows:\n\nstudy: The name of the study.\nnetID: A unique label for each day of sampling.\nsDate: The date of sampling.\nrun: A factor indicating if the sample was the marking or recapture run.\nspecies: A factor indicating the species captured. Options are Species1, Species2, or Species3. This variable will be discussed in more detail later.\nlength: The total length of the sampled fish to the nearest mm.\nclipped: Indicates if the fish was clipped (=1) or not. All fish in the recapture run were not clipped.\nrecap: Indicates if the fish was previously clipped (i.e., a “recapture”; =1) or not. All fish in the marking run could not possibly be a recaptured fish.\nnotes: Any specific notes about that particular fish."
  },
  {
    "objectID": "teaching/posts/2022-12-20_SizeStrux_Whittlesey/index.html#motivation",
    "href": "teaching/posts/2022-12-20_SizeStrux_Whittlesey/index.html#motivation",
    "title": "Size Structure of Species in a Cold-Water Wisconsin Stream",
    "section": "",
    "text": "Fish length is likely most commonly collected data by fisheries biologists and managers. While length is usually not the only information collected by a fisheries biologist, summaries of length information can provide the biologist with a wealth of information on population parameters such as age distributions, growth rates, and mortality rates; the basic biology of the animal; and the effect of management regulations on the fish population. Thus, summarizing length data and then interpreting those summaries is an important task performed by fisheries biologists.\n\n\n\n\n\nMeasuring length of a Rainbow Trout."
  },
  {
    "objectID": "teaching/posts/2022-12-20_SizeStrux_Whittlesey/index.html#setting",
    "href": "teaching/posts/2022-12-20_SizeStrux_Whittlesey/index.html#setting",
    "title": "Size Structure of Species in a Cold-Water Wisconsin Stream",
    "section": "",
    "text": "Whittlesey Creek is a tributary to Chequamegon Bay of Lake Superior. This stream has been severely degraded by past land use practices including clear-cut logging and channelization to drain wetlands for farming. The “Coaster” Brook Trout (Salvelinus fontinalis), a lake-dwelling stream-spawning form of Brook Trout, was largely extirpated from southern Lake Superior and the Whittlesey Creek watershed due to over-fishing and habitat degradation. In 1999, the U.S., Fish and Wildlife Service established the Whittlesey Creek National Wildlife Refuge with a primary goal of reestablishing a population of Coaster Brook Trout in Whittlesey Creek. That effort has included experimental introductions of Brook Trout at various life stages and much work to restore habitat.\nIn July, 2011 a quarter-mile stretch of the lower Whittlesey Creek was modified with the addition of large woody debris with attached “root balls.” The goal of this project was to alter the morphology of this section of the stream to provide better habitat and water conditions for Brook Trout. In May, 2011 a group of students conducted two days of sampling with electrofishing gear to provide baseline information about the fish populations in this section of the stream. Their sampling yielded eight total Brook Trout but relatively larger numbers of Coho Salmon (Oncorhynchus kisutch), Rainbow Trout (Oncorhynchus mykiss), and sculpins (primarily Slimy Sculpin (Cottus cognatus), but some Mottled Sculpin (Cottus bairdii)). Data from catches of Coho Salmon, Rainbow Trout, and sculpins will be analyzed in this case study.\n\n\n\n\n\nStudents backpack electrofishing a stream."
  },
  {
    "objectID": "teaching/posts/2022-12-20_SizeStrux_Whittlesey/index.html#data",
    "href": "teaching/posts/2022-12-20_SizeStrux_Whittlesey/index.html#data",
    "title": "Size Structure of Species in a Cold-Water Wisconsin Stream",
    "section": "",
    "text": "Data from the sampling collected by the students were stored in Whittlesey2011.csv. The variables in this data set are defined as follows:\n\nstudy: The name of the study.\nnetID: A unique label for each day of sampling.\nsDate: The date of sampling.\nrun: A factor indicating if the sample was the marking or recapture run.\nspecies: A factor indicating the species captured. Options are Species1, Species2, or Species3. This variable will be discussed in more detail later.\nlength: The total length of the sampled fish to the nearest mm.\nclipped: Indicates if the fish was clipped (=1) or not. All fish in the recapture run were not clipped.\nrecap: Indicates if the fish was previously clipped (i.e., a “recapture”; =1) or not. All fish in the marking run could not possibly be a recaptured fish.\nnotes: Any specific notes about that particular fish."
  },
  {
    "objectID": "teaching/posts/2022-12-19_SizeStrux_YPerchSB1/index.html",
    "href": "teaching/posts/2022-12-19_SizeStrux_YPerchSB1/index.html",
    "title": "Size Structure",
    "section": "",
    "text": "Diana and Salz (1990) examined the growth and maturity of Yellow Perch captured from multiple sites at two locations (inner and outer) within Saginaw Bay, Lake Huron (MI) from 1983-1985. The primary goal of this research was to determine if the growth and maturation of Yellow Perch showed evidence of stunting. The total lengths and location of capture for this study are in YPerchSB1.1. Use these data to answer the following questions.\n1 See “CSV file” link in “Source” section of linked page.\nIsolate Yellow Perch captured from the inner portion of Saginaw Bay.\n\nDescribe the distribution of these fish using both a histogram and frequency table.2\nCompute and interpret the PSD-Q value (with 95% confidence interval).\nCompute and interpret the PSD-P value (with 95% confidence interval).\n\nIsolate Yellow Perch captured from the outer portion of Saginaw Bay.\n\nDescribe the distribution of these fish using both a histogram and frequency table.\nCompute and interpret the PSD-Q value (with 95% confidence interval).\nCompute and interpret the PSD-P value (with 95% confidence interval).\n\nStatistically compare the following metrics between Yellow Perch captured in the the inner and outer portions of Saginaw Bay.\n\nLength frequency distribution.\nDistributions into the five-cell Gabelhouse length categories.\nPSD-Q.\nPSD-P.\n\nWrite a short paragraph describing what you learned about the size structure of Saginaw Bay Yellow Perch, with specific comments about differences (if any) between the inner and outer locations.\n\n2 Choose an appropriate/reasonable bin/category width. \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nDiana, J. S., and R. Salz. 1990. Energy storage, growth, and maturation of Yellow Perch from different locations in Saginaw Bay, Michigan. Transactions of the American Fisheries Society 119:976–984.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Wrangling_YPerchTL/index.html",
    "href": "teaching/posts/2019-3-8_Wrangling_YPerchTL/index.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Researchers for the North Temperate Lakes Long-Term Ecological Research program have collected fish from a number of lakes in Wisconsin since 1981. The total lengths and weights of Yellow Perch (Perca flavescens) captured from Trout Lake from 1981-2006 are recorded in YPerchTL.1 Use these data to answer the following questions.\n1 See “CSV file” link in “Source” section of linked page.\nRemove the lakeid and spname variables from the data frame as these variables are constant for all records.\nRemove all records for which the gearid is “CRAYTR”, “FYKNED”, “FYKNEL”, “MINNOW”, “TRAMML”, or any of “VGN0XX”, as very few Yellow Perch were captured in these gears.2 [Use this data frame for the remaining questions.]\nCreate a new data frame of Yellow Perch captured in just fyke nets.\nCreate a new data frame of Yellow Perch captured in fyke nets and beach seines.\nCreate a new data frame of Yellow Perch captured in 1998.\nCreate a new data frame of Yellow Perch captured after 1999.\nCreate a new data frame of Yellow Perch captured in the 1990s.\nCreate a new data frame of Yellow Perch captured with only fyke nets in 2005.\nCreate a new data frame of Yellow Perch captured with only beach seines and fyke nets in 2000.\nMake the following changes to the data frame of Yellow Perch captured in fyke nets and beach seines in 2000.\n\nChange the name of the length and weight variables to tl and wt, respectively.\nCreate a new variable that is the total length in inches.\nCreate two new variables that are the common logarithms of the lengths (in mm) and weights.\nAdd a variable that contains 10 mm wide length categories.\nAdd a variable that contains the appropriate five-cell Gabelhouse length categories.\nSort the data by total length within capture year.\n\n\n2 It may be eaiser to look at the list of gearid values and keep the ones that are not listed here.\n\n\n\n\n\nSave Your Script\n\n\n\nSome of these data frames will be used in this graphing, this size structure, and this weight-length relationhip exercises.\n\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_WL_YPerchTL/index.html",
    "href": "teaching/posts/2019-3-8_WL_YPerchTL/index.html",
    "title": "Weight-Length Relationship",
    "section": "",
    "text": "Continuation\n\n\n\nThis exercise is a continuation of this data wrangling exercise and, thus, depends on data frames constructed there. Please load/run your script from that exercise to access the Yellow Perch captured in the 1990s data frame.\n\n\n \n\nBasic Analysis I\n\nConstruct graphs appropriate to answer the following questions.1\n\nDescribe the relationship between weight and length (in mm here and throughout).\nDescribe the relationship between log-transformed weight and length.\n\nFrom the plots above there is a clear minimum length for which the weights were precisely obtained. What is that length? [Reduce the data frame to fish greater than this minimum length for the questions below. You should also remove all fish for which a weight was not recorded.]2\nCompute the weight-length relationship with an appropriate linear regression.\n\nPlot the results (data and the fitted relationship) on both the transformed and raw scales.3 Comment on the fit.\nConstruct a residual plot.4 Comment.\nExpress your results as an equation on the transformed scale.\nExpress your results as an equation on the raw scale.\nCarefully interpret the meaning of the slope of the weight-length relationship.\nIs there statistical evidence for isometric or allometric growth?\n\n\n1 If you completed this graphing exercise then you created the necessary graphs there.2 There are several outliers in this data that should be corrected or removed. For simplicity, leave them in the data for this exercise.3 This post may be useful.4 This post may be useful. \n\n\nBasic Analysis II\n\nRecompute the weight-length relationship using the original length in inches. How do the slope and y-intercept from this model compare to the results from the previous question?\n\n \n\n\nExtended Analysis\n\nConstruct a plot that allows you to qualitatively assess if the weight-length (in mm here and throughout) relationship differs between the three gears.\nFit a model that allows you to determine if there is a statistically significant difference in the weight-length relationship between the three gears.\n\nConstruct a residual plot for this model.5 Comment.\nIs there a statistically significant difference in the weight-length relationship between the three gears? Provide evidence for your findings and be very specific with your conclusions.\nWithout fitting separate regressions for the three gears express the weight-length relationships on the raw scale for all gears (i.e., write three specific equations).\nConstruct a plot that illustrates your findings.6\n\n\n5 This post may be useful.6 This post may be useful. \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_WLBluegill_Inch/index.html",
    "href": "teaching/posts/2019-3-8_WLBluegill_Inch/index.html",
    "title": "Weight-Length Relationship",
    "section": "",
    "text": "Continuation\n\n\n\nThis exercise is a continuation of this data wrangling exercise and, thus, depends on data frames constructed there. Please load/run your script from that exercise to access the Bluegill only data frames.\n\n\n \n\nBasic Analysis I\n\nConstruct graphs appropriate to answer the following questions.1\n\nDescribe the relationship between weight and length (in mm here and throughout).\nDescribe the relationship between log-transformed weight and length.\n\nFrom the plots above there is a clear minimum length for which the weights were precisely obtained. What is that length? [Reduce the data frame to fish greater than this minimum length for the questions below.]\nCompute the weight-length relationship with an appropriate linear regression.\n\nPlot the results (data and the fitted relationship) on both the transformed and raw scales.2 Comment on the fit.\nConstruct a residual plot.3 Comment.\nExpress your results as an equation on the transformed scale.\nExpress your results as an equation on the raw scale.\nCarefully interpret the meaning of the slope of the weight-length relationship.\nIs there statistical evidence for isometric or allometric growth?\n\n\n1 If you completed this graphing exercise then you created the necessary graphs there.2 This post may be useful.3 This post may be useful. \n\n\nBasic Analysis II\n\nRecompute the weight-length relationship using the original length in inches. How do the slope and y-intercept from this model compare to the results from the previous question?\n\n \n\n\nExtended Analysis\n\nConstruct a plot that allows you to qualitatively assess if the weight-length (in mm here and throughout) relationship differs between the two years.\nFit a model that allows you to determine if there is a statistically significant difference in the weight-length relationship between the two years.\n\nConstruct a residual plot for this model.4 Comment.\nIs there a statistically significant difference in the weight-length relationship between the two years? Provide evidence for your findings and be very specific with your conclusions.\nWithout fitting separate regressions for the two sample years, express the weight-length relationships on the raw scale for both years (i.e., write two specific equations).\nConstruct a plot that illustrates your findings.5\n\n\n4 This post may be useful.5 This post may be useful. \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Recruitment_WalleyeEL/index.html",
    "href": "teaching/posts/2019-3-8_Recruitment_WalleyeEL/index.html",
    "title": "Stock-Recruitment",
    "section": "",
    "text": "Background\nHansen et al. (1998) modeled recruitment variation of age-0 Walleye (Sander vitreus) in Escanaba Lake (Wisconsin) to determine factors regulating their abundance. Specifically, they examined the abundance of age-5 and older Walleye (spawning population), variation in May water temperatures, and abundance of 152.4 mm total length and longer Yellow Perch (Perca flavescens) on the abundance of age-0 Walleye. These data are available in WalleyeEL.1\n1 See “CSV file” link in “Source” section of linked page.\n\n\n\n\n\nBeware\n\n\n\nIf you want to compare your results below to those in Hansen et al. (1998), note that they used what is called the second parameterization of the Ricker function in FSA and that they restricted their data to the year-classes prior to 1992 for “model construction.”\n\n\n \n\n\nBasic Analysis\n\nWhich variable should be considered the “recruits” and which variable should be considered the “spawning stock?” Explain.\nFrom an appropriate plot, describe the relationship between “recruits” and “stock.” Do you expect a stock-recruitment model to fit these data well?\nFit the density-independent recruitment function to these data, assuming a multiplicative error structure. Show your results by expressing the equation of the recruitment function with the parameters replaced by their estimated values.\nRepeat the previous question but using the Ricker recruitment function.\nDetermine if the density-dependent parameter is statistically significant in the Ricker model.\nDescribe what proportion of variability in recruitment is explained by the Ricker model.\nConstruct a single plot that shows how well each recruitment function fits these data. Show confidence bands for the Ricker recruitment function.2\nEstimate recruitment for the mean stock level with each recruitment function. How variable are the results among models?\n\n2 This may be useful. \n\n\nExtended Analysis\n\nDetermine if adding the variation in May water temperatures or the abundance of Yellow Perch significantly improves the predictive power of the Ricker model.\nIf either of these two variables improved the predictive power of the model then ….\n\nExpress the best model by replacing parameters with their estimated values.\nSpecifically describe the effect of the other variables on the recruitment of age-0 Walleye.\nDo you feel that this model provides a clear explanation for the variability in recruitment of age-0 Walleye in Escanaba Lake? Explain.\n\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHansen, M., M. Bozek, R. Justine, S. Newman, and M. Staggs. 1998. Factors affecting recruitment of Walleyes in Escanaba Lake, Wisconsin, 1958–1996. North American Journal of Fisheries Management 18:764–774.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_MR_URBrownTrout/index.html",
    "href": "teaching/posts/2019-3-8_MR_URBrownTrout/index.html",
    "title": "Abundance from Mark-Recapture Data",
    "section": "",
    "text": "Young and Hayes (2001) described a study where Brown Trout (Salmo trutta) in several rivers were captured by experienced fly fishers, tagged at the base of the dorsal fin with a colored dart tag, and then observed by divers drifting through the sample area two days later. In the Ugly River, 43 trout were marked, 123 fish were observed by the divers, and 16 fish observed by the divers were tagged.\n\nAssign one of these symbols (\\(N\\), \\(M\\), \\(n\\), \\(m\\)) to each of the numerical results from this study.1\nConstruct a population estimate, with 95% confidence interval, for the Brown Trout in this section of the Ugly River. Carefully interpret the results.\nWhich method did you use to construct the confidence interval? Explain why you chose that method.\n\n1 Symbols are as used in Ogle (2016).\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\n\nYoung, R. G., and J. W. Hayes. 2001. Assessing the accuracy of drift‐dive estimates of brown trout ( Salmo trutta ) abundance in two New Zealand rivers: A mark‐resighting study. New Zealand Journal of Marine and Freshwater Research 35(2):269–275.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_MR_TanRiffleshell/index.html",
    "href": "teaching/posts/2019-3-8_MR_TanRiffleshell/index.html",
    "title": "Abundance from Mark-Recapture Data",
    "section": "",
    "text": "Rogers (1999) studied the endangered freshwater mussel Tan Riffleshell (Epioblasma florentina walkeri) in Indian Creek, Virginia. Part of the study included a detailed analysis of the population dynamics of mussels in a 100 m stretch of stream. In this stream, mussels were captured by teams of snorkelers on six occasions (June 1996; August 1996; June 1997; October 1998; May 1999; and June 1999). Mussels were tagged by attaching a numbered tag to the left valve of the mussel. The number of mussels captured, the number of marked mussels observed, and the number of tagged mussels returned to the population were recorded for each sample time. These data are in Riffleshell.1. Use these data to answer the following questions.\n1 See “CSV file” link in “Source” section of linked page.\nWhat is the only method that can be used to estimate abundance given the way these data were collected? Comment on the validity of the assumptions for this method for these data.\nRegardless of your thoughts on the validity of the assumptions, use your chosen method to estimate the number, with 95% confidence interval, of Tan Riffleshell mussels in the population at the beginning of the study. Carefully interpret the result.\n\n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nRogers, S. O. 1999, December 6. Population Biology of the Tan Riffleshell (Epioblasma florentina walkeri) and the Effects of Substratum and Light on Juvenile Propagation. Thesis, Virginia Tech.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Mortality_LSKBLakeTrout/index.html",
    "href": "teaching/posts/2019-3-8_Mortality_LSKBLakeTrout/index.html",
    "title": "Mortality",
    "section": "",
    "text": "The population biology of Lake Superior Lake Trout (Salvelinus namaycush) prior to 1950 was examined in detail by Sakagawa and Pycha (1971). In Table 1 of their paper, they presented the number of Lake Trout by age (from scales) collected in 4.5-inch mesh gillnets that were set between the Keweenaw Peninsula and Munising, MI in 1948. The numbers of Lake Trout caught for ages 3 to 14 were 5, 18, 21, 10, 45, 109, 95, 63, 42, 25, 13, and 4. Use these data to answer the questions below.\n\nIs this an example of a cross-sectional or longitudinal catch curve?\nPlot log(catch) versus age. Which ages best represent the descending portion of the catch curve? Explain.\nUsing the unweighted regression method, find the following (with 95% confidence intervals):\n\nInstantaneous total mortality rate.\nAnnual total mortality rate.\nAnnual survival rate.\n\nRepeat the previous question using the weighted regression method.\nRepeat the previous question using the Chapman-Robson method.\nMathematically show how to convert the instantaneous mortality rate to an annual mortality rate.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\nReferences\n\nSakagawa, G. T., and R. L. Pycha. 1971. Population biology of Lake Trout (Salvelinus namaycush) of Lake Superior before 1950. Journal of the Fisheries Research Board of Canada 28:65–71.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Growth_RockBassLO/index.html",
    "href": "teaching/posts/2019-3-8_Growth_RockBassLO/index.html",
    "title": "Individual Growth",
    "section": "",
    "text": "Background\nWolfert (1980) measured the total length (TL) of 1288 Rock Bass (Ambloplites rupestris) from Eastern Lake Ontario in the late 1970s. In addition, scales were removed for age estimation from as many as 10 specimens from each 10 mm length interval. All data are recorded in RockBassLO2.1\n1 See “CSV file” link in “Source” section of linked page. Also note that the filename contains an “oh” not a “zero.”\n\n\n\n\n\nContinuation\n\n\n\nThis exercise requires the data frame that contains length and ages, both estimated and assigned from an age-length-key, for all sampled fish. This data frame was constructed in this age-length key exercise. Please load/run your script from that exercise that produces the data frame with ages for all sampled fish.\n\n\n \n\n\nFit Traditional VBGF\n\nExamine the plot of TL versus age.2 Make observations regarding the “shape” of the data (do the results look linear or like a von Bertalanffy growth curve, is there an obvious asymptote, are young fish well represented, how variable are lengths within ages).\nFit the typical parameterization of the von Bertalanffy growth function (VBGF).\n\nHow realistic do the point estimates of \\(L_{\\infty}\\), \\(K\\), and \\(t_{0}\\) seem?\nWrite the typical VBGF with parameters replaced by their estimated values.\nCarefully interpret the meaning of each parameter.\nConstruct 95% bootstrapped confidence intervals for each parameter. Comment on the widths of these confidence intervals. What explains this?\nPredict the mean TL, with 95% confidence interval, for an age-6 Rock Bass. Comment on the width of this confidence interval. What explains this?\nPlot TL versus age and superimpose the best-fit VBGF.3 Comment on model fit.\nConstruct a residual plot. Comment on model fit.\nCompute the correlation between parameter values. Comment.\n\n\n2 This plot was made in this exercise.3 This post may be useful. \n\n\nAlternative Parameterization\n\nFit the Gallucci and Quinn (1979) parameterization.4\n\nInterpret the interval estimate for the \\(\\omega\\) parameter.\nWrite the Gallucci and Quinn VBGF with parameters replaced by their estimated values.\nConstruct 95% bootstrapped confidence intervals for each parameter. Comment on the widths of these confidence intervals. What explains this?\nPredict the mean TL, with 95% confidence interval, for an age-6 Rock Bass. Comment on the width of this confidence interval. What explains this?\nPlot TL versus age and superimpose the best-fit VBGF. Comment on model fit.\nCompute the correlation between parameter values. Comment\nHow does the estimate of \\(K\\) from fitting this parameterization compare to that from the typical VBGF fit above. Explain your observation.\n\n\n4 See growthFunShow(\"vonBertalanffy\",param=\"GQ\",plot=TRUE)) and this. \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nWolfert, D. R. 1980. Age and growth of Rock Bass in eastern Lake Ontario. New York Fish and Game Journal 27:88–90.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Graphing_YPerchTL/index.html",
    "href": "teaching/posts/2019-3-8_Graphing_YPerchTL/index.html",
    "title": "Graphing",
    "section": "",
    "text": "Continuation\n\n\n\nThis exercise is a continuation of this data wrangling exercise and, thus, depends on data frames constructed there. Please load/run your script from that exercise to access the data frames required below.\n\n\n \n\nDescribe the following from plots constructed with the Yellow Perch captured in fyke nets in 2005 data frame.\n\nThe distribution of lengths.\nThe distribution of weights.\nThe relationship between weight and length.\nThe relationship between the natural logs of weight and length.\n\nDescribe the following from plots constructed with the Yellow Perch captured in the 1990s data frame.\n\nThe distribution of lengths separately for each gear (without using separate data frames for each gear).\nThe relationship between the natural logs of weight and length separately for each gear (without using separate data frames for each gear).\nThe difference in mean lengths among the gears (plot should include confidence intervals).\nDescribe the difference in numbers caught among the gears.\n\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Depletion_LGLargemouth/index.html",
    "href": "teaching/posts/2019-3-8_Depletion_LGLargemouth/index.html",
    "title": "Abundance from Depletion Data",
    "section": "",
    "text": "Maceina et al. (1995) examined the population of harvestable Largemouth Bass (Micropterus salmoides) in Conner Cove of Lake Guntersville, Alabama (a 28,000 ha impoundment of the Tennessee River) in March, 1992. Their objective was to estimate the abundance of harvestable bass through the depletion of catches due to electrofishing. In their study, they electrofished for 15 minutes at each of four depth strata in Conner Cove. The total number of bass captured and removed from the population in the full hour of electrofishing at the four depths was recorded. Thus, a total of one hour of electrofishing constituted one unit of effort. Six one-hour periods were used to deplete the population of bass in one Conner Cove. The number of bass caught in each successive sampling period was 23, 12, 13, 14, 9, and 7. Enter these data into vectors in R and use them to answer the following questions.\n\nWhat is the best estimate and 95% confidence interval for the catchability coefficient? Very carefully interpret what this estimate means relative to Conner Cove Largemouth Bass.\nHow does your catchability estimate compare to that published in Maceina et al. (1995)? Explain discrepancies (if any).\nWhat is the best estimate and 95% confidence interval for the population size?\nHow does your populations size estimate compare to that published in Maceina et al. (1995)? Explain discrepancies (if any).\nEstimate the expected number of bass that would be caught in the first sampling period given your best estimates of \\(q\\) and \\(N_{0}\\), and the amount of effort used in each sample period.1\nEstimate the expected number of bass that would be caught in the second sampling period, again given your best estimates of \\(q\\) and \\(N_{0}\\), and the amount of effort used in each sample period.\n\n1 Symbols are as used in Ogle (2016).\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nMaceina, M., W. Wrenn, and D. Lowery. 1995. Estimating harvestable Largemouth Bass abundance in a reservoir with an electrofishing catch depletion technique. North American Journal of Fisheries Management 15:103–109.\n\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_ALK_StripedBass/index.html",
    "href": "teaching/posts/2019-3-8_ALK_StripedBass/index.html",
    "title": "Age-Length Key",
    "section": "",
    "text": "Background\nResearchers at the Center for Quantitative Fisheries Ecology at Old Dominion University in collaboration with the Virginia Marine Resources Commission annually collect Striped Bass (Morone saxatilis) from Virginia waters of the Atlantic Ocean for age assessments. The total lengths of 1201 Stiped Bass collected in 2003 and the ages estimated from otoliths for as many as 10 fish per 1 inch length interval are recorded in StripedBass3.1\n1 See “CSV file” link in “Source” section of linked page. \n\n\nConstruct an ALK\n\nAdd a variable to the data frame that contains the 1 in TL categories and then separate the observed data into age- and length-samples. How many fish are in each sample?\nConstruct a table of the number (not proportion) of fish in each age and 1 inch TL category in the age-sample. From these results, compute each of the following by hand (i.e., not using R, but you may use a calculator).\n\nHow many fish are in the 30 in TL category?\nHow many fish are age 10?\nWhat proportion of fish in the 35 in TL category are age 9?\nWhat proportion of fish in the 31 in TL category are age 11?\n\nConstruct an observed age-length key from the table above (using R). From these results answer the following questions.\n\nWhat proportion of fish in the 30 in TL category should be assigned age 10?\nHow many of forty fish in the 25 mm TL category should be assigned age 5?\nConstruct a plot of the observed age-length key. Are there any potential anomalies in the plot that would suggest that a smoothed age-length key could be appropriate?\n\nConstruct a smoothed age-length key. From these results answer the following questions.\n\nWhat proportion of fish in the 30 in TL category should be assigned age 10?\nHow many of fourty fish in the 25 mm TL category should be assigned age 5?\n\n\n \n\n\nApply an ALK I\nContinue with the age- and length-sample data frames and the observed age-length key from the previous section.\n\nUse the semi-random age assignment technique from Isermann and Knight (2005) and the observed age-length key to assign ages to the unaged fish in the length-sample. Combine the age-sample and the age-assigned length-sample into a single data frame and answer the following questions.\n\nHow many fish are estimated to be age 8?\nHow many fish are estimated to be age 14?\nPlot the age distribution for all fish.\nHow many fish are in the 30 in TL interval?\nWhat is the mean TL of age-9 fish?\nPlot the length-at-age with the mean length-at-age superimposed for all fish.\n\nCompare your results from the previous question to someone else’s results (or repeat the previous question). Did you both get the exact same results? Why or why not? If not, how different were they?\n\n \n\n\nApply an ALK II\n\nUse the “classical” method to estimate the age distribution (with standard errors) for all sampled fish.\n\nHow many fish are estimated to be age 8?\nHow many fish are estimated to be age 14?\nPlot the age distribution for all fish.\n\nUse the “classical” method to estimate the mean length-at-age (with standard deviations) for all sampled fish.\n\nWhat is the mean TL of age-9 fish?\nPlot the length-at-age with the mean length-at-age superimposed for all fish.\n\nCompare your results to someone else’s results (or repeat the steps above). Did you both get the exact same results? Why or why not? If not, how different were they?\nCompare your results using the “classical” method here to your results from using the Isermann and Knight (2005) method in the previous section.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nIsermann, D., and C. Knight. 2005. A computer program for age–length keys incorporating age assignment to individual fish. North American Journal of Fisheries Management 25:1153–1160.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_ALK_FWDrumLE/index.html",
    "href": "teaching/posts/2019-3-8_ALK_FWDrumLE/index.html",
    "title": "Age-Length Key",
    "section": "",
    "text": "Background\nBur (1984) examined the population dynamics of Freshwater Drum (Aplodinotus grunniens) in Lake Erie in the late 1970s. In one part of his study, he measured the total length (TL) of all 1577 drum sampled and extracted scales for age estimation from a proportionate sample from each 10 mm length interval. The length and age data are recorded in FWDrumLE2.1\n1 See “CSV file” link in “Source” section of linked page. \n\n\nConstruct an ALK\n\nAdd a variable to the data frame that contains the 10 mm TL categories and then separate the observed data into age- and length-samples. How many fish are in each sample?\nConstruct a table of the number (not proportion) of fish in each age and 10 mm TL category in the age-sample. From these results, compute each of the following by hand (i.e., not using R, but you can use a calculator).\n\nHow many Freshwater Drum are in the 230 mm TL category?\nHow many Freshwater Drum are age 5?\nWhat proportion of Freshwater Drum in the 300 mm TL category are age 5?\nWhat proportion of Freshwater Drum in the 200 mm TL category are age 4?\n\nConstruct an observed age-length key from the table above (using R). From these results answer the following questions.\n\nWhat proportion of Freshwater Drum in the 210 mm TL category should be assigned age 5?\nHow many of thirty Rock Bass in the 250 mm TL category should be assigned age 4?\nConstruct a plot of the observed age-length key. Are there any potential anomalies in the plot that would suggest that a smoothed age-length key could be appropriate?\n\nConstruct a smoothed age-length key. From these results answer the following questions.\n\nWhat proportion of Freshwater Drum in the 210 mm TL category should be assigned age 5?\nHow many of thirty Rock Bass in the 250 mm TL category should be assigned age 4?\n\n\n \n\n\nApply an ALK I\nContinue with the age- and length-sample data frames and the observed age-length key from the previous section.\n\nUse the semi-random age assignment technique from Isermann and Knight (2005) and the observed age-length key to assign ages to the unaged fish in the length-sample. Combine the age-sample and the age-assigned length-sample into a single data frame to answer the following questions.\n\nHow many fish are estimated to be age 3?\nHow many fish are estimated to be age 8?\nPlot the age distribution for all fish.\nHow many fish are in the 150 mm TL interval?\nWhat is the mean TL of age-4 fish?\nPlot the length-at-age with the mean length-at-age superimposed for all fish.\n\nCompare your results from the previous question to someone else’s results (or repeat the previous question). Did you both get the exact same results? Why or why not? If not, how different were they?\n\n \n\n\nApply an ALK II\nContinue with the age- and length-sample data frames and the observed age-length key from the first section.\n\nUse the “classical” method to estimate the age distribution (with standard errors) for all sampled fish.\n\nHow many fish are estimated to be age 3?\nHow many fish are estimated to be age 8?\nPlot the age distribution for all fish.\n\nUse the “classical” method to estimate the mean length-at-age (with standard deviations) for all sampled fish.\n\nWhat is the mean TL of age-4 fish?\nPlot the length-at-age with the mean length-at-age superimposed for all fish.\n\nCompare your results to someone else’s results (or repeat the steps above). Did you both get the exact same results? Why or why not? If not, how different were they?\nCompare your results using the “classical” method here to your results from using the Isermann and Knight (2005) method in the previous section.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBur, M. T. 1984. Growth, reproduction, mortality, distribution, and biomass of Freshwater Drum in Lake Erie. Journal of Great Lakes Research 10:48–58.\n\n\nIsermann, D., and C. Knight. 2005. A computer program for age–length keys incorporating age assignment to individual fish. North American Journal of Fisheries Management 25:1153–1160.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2018-4-19_Adding_Zero_Catches/index.html",
    "href": "teaching/posts/2018-4-19_Adding_Zero_Catches/index.html",
    "title": "Adding Zero Catches",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the random number seed for reproducibility of the randomly generated sample data below.\n\n\n\nlibrary(dplyr)    # for group_by(), summarize(), mutate(), right_join()\nlibrary(tidyr)    # for complete(), nesting()\nset.seed(678394)  # for reproducibility of randomly created data\n\n \n\nIntroduction\nMuch of my work is with undergraduates who are first learning to analyze fisheries data. A common “learning opportunity” occurs when students are asked to compute the mean catch (or CPE), along with a standard deviation (SD), across multiple gear sets for each species. The learning opportunity occurs because some species will invariably not be caught in some gear sets. When the students summarize the number of fish caught for each species in each gear set those species not caught in a particular gear set will not “appear” in their data. Thus, when calculating the mean, the student will get the correct numerator (sum of catch across all gear sets) but not denominator (they use number of catches summed rather than total number of gear sets), which inflates (over-estimates) the mean catch and (usually) deflates (under-estimates) the SD of catches. Once confronted with this issue, they easily realize how to correct the mean calculation, but calculating the standard deviation is still an issue. These problems are exacerbated when using software to compute these summary statistics across many individual gear sets.\nIn software, the “trick” is to add a zero for each species not caught in a specific gear set that was caught in at least one gear set. For example, if Bluegill were caught in at least one gear set but not in the third gear set, then a zero must be added as the catch of Bluegill in the third gear set. The addZeroCatch() function in the FSA package was an attempt to efficiently add these zeroes. This function has proven useful over the years, but I have become dissatisfied with its clunkiness. Additionally, I recently became aware of the complete() function in the tidyr package which holds promise for handling the same task. In this post, I explore the use of complete() for handling this issue.\n \n\n\nSimple Data\nIn this first example, the data consists of species and length recorded for each captured fish organized by the gear set identification number (ID) and held in the fishdat data.frame.\n\n\n#R|    ID species  tl\n#R|  1  1     BLG 148\n#R|  2  1     BLG 153\n#R|  3  1     BLG 147\n#R|  4  1     BLG 149\n#R|  5  1     BLG 144\n#R|  6  1     BLG 145\n\n\nThe catch of each species in each gear set may be found using group_by() and summarize() with n().1\n1  I find the tibble structure returned by group_by() to be annoying with simple data frames like this. Thus, I usually use as.data.frame() to remove it.\ncatch &lt;- fishdat |&gt;\n  group_by(ID,species) |&gt;\n  summarize(num=n()) |&gt;\n  as.data.frame()\ncatch\n\n#R|     ID species num\n#R|  1   1     BLG  10\n#R|  2   1     LMB   5\n#R|  3   1     YEP   5\n#R|  4   2     LMB   9\n#R|  5   2     YEP   7\n#R|  6   3     BLG  12\n#R|  7   3     YEP   7\n#R|  8   4     BLG   1\n#R|  9   4     LMB  11\n#R|  10  4     YEP  11\n#R|  11  5     BLG   9\n\n\nFrom this it is seen that three species (“BLG”, “LMB”, and “YEP”) were captured across all nets, but that “BLG” were not captured in “ID=2”, “LMB” were not captured in “ID=3”, and “LMB” and “YEP” were not captured in “ID=5”. The sample size, mean, and SD of catches per species from these data may be found by again using group_by() and summarize(). However, these calculations are INCORRECT because they do not include the zero catches of “BLG” in “ID=2”, “LMB” in “ID=3”, and “LMB” and “YEP” in “ID=5”. The problem is most evident in the sample sizes, which should be five (gear sets) for each species.\n\n## Example of INCORRECT summaries because not using zeroes\ncatch |&gt; \n  group_by(species) |&gt;\n  summarize(n=n(),mn=mean(num),sd=sd(num)) |&gt;\n  as.data.frame()\n\n#R|    species n       mn       sd\n#R|  1     BLG 4 8.000000 4.830459\n#R|  2     LMB 3 8.333333 3.055050\n#R|  3     YEP 4 7.500000 2.516611\n\n\nThe complete() function can be used to add rows to a data frame for variables (or combinations of variables) that should be present in the data frame (relative to other values that are present) but are not. The complete() function takes a data frame as its first argument (but will be “piped” in below with |&gt;) and the variable or variables that will be used to identify which items are missing. For example, with these data, a zero should be added to num for missing combinations defined by ID and species.\n\n## Example of default complete ... see below to add zeroes, not NAs\ncatch |&gt;\n  complete(ID,species) |&gt;\n  as.data.frame()\n\n#R|     ID species num\n#R|  1   1     BLG  10\n#R|  2   1     LMB   5\n#R|  3   1     YEP   5\n#R|  4   2     BLG  NA\n#R|  5   2     LMB   9\n#R|  6   2     YEP   7\n#R|  7   3     BLG  12\n#R|  8   3     LMB  NA\n#R|  9   3     YEP   7\n#R|  10  4     BLG   1\n#R|  11  4     LMB  11\n#R|  12  4     YEP  11\n#R|  13  5     BLG   9\n#R|  14  5     LMB  NA\n#R|  15  5     YEP  NA\n\n\nFrom this result, it is seen that complete() added a row for “BLG” in “ID=2”, “LMB” in “ID=3”, and “LMB” and “YEP” in “ID=5” as we had hoped. However, complete() adds NAs by default. The value to add can be changed with fill=, which takes a list that includes the name of the variable to which the NAs were added (num in this case) set equal to the value to be added (0 in this case).2\n2 Here the result is saved into the catch data frame, thus modifying the original data frame with the addition of the zeroes.\ncatch &lt;- catch |&gt;\n  complete(ID,species,fill=list(num=0)) |&gt;\n  as.data.frame()\ncatch\n\n#R|     ID species num\n#R|  1   1     BLG  10\n#R|  2   1     LMB   5\n#R|  3   1     YEP   5\n#R|  4   2     BLG   0\n#R|  5   2     LMB   9\n#R|  6   2     YEP   7\n#R|  7   3     BLG  12\n#R|  8   3     LMB   0\n#R|  9   3     YEP   7\n#R|  10  4     BLG   1\n#R|  11  4     LMB  11\n#R|  12  4     YEP  11\n#R|  13  5     BLG   9\n#R|  14  5     LMB   0\n#R|  15  5     YEP   0\n\n\nThese correct catch data can then be summarized as above to show the correct sample size, mean, and SD of catches per species.\n\ncatch |&gt;\n  group_by(species) |&gt;\n  summarize(n=n(),mn=mean(num),sd=sd(num)) |&gt;\n  as.data.frame()\n\n#R|    species n  mn       sd\n#R|  1     BLG 5 6.4 5.504544\n#R|  2     LMB 5 5.0 5.049752\n#R|  3     YEP 5 6.0 4.000000\n\n\n \n\n\nMultiple Values to Receive Zeroes\nSuppose that the fish data included a column that indicates whether the fish was marked and returned to the waterbody or not.\n\n\n#R|    ID species  tl marked\n#R|  1  1     BLG 148    YES\n#R|  2  1     BLG 153    YES\n#R|  3  1     BLG 147    YES\n#R|  4  1     BLG 149    YES\n#R|  5  1     BLG 144    YES\n#R|  6  1     BLG 145    YES\n\n\nThe catch and number of fish marked and returned per gear set ID and species may again be computed with group_by() and summarize(). Note, however, the use of ifelse() to use a 1 if the fish was marked and a 0 if it was not. Summing these values returns the number of fish that were marked. Giving this data frame to complete() as before will add zeroes for both the num and nmarked variables as long as both are included in the list given to fill=.\n\ncatch2 &lt;- fishdat2 |&gt;\n  group_by(ID,species) |&gt;\n  summarize(num=n(),\n            nmarked=sum(ifelse(marked==\"YES\",1,0)))\ncatch2\n\n#R|  # A tibble: 11 × 4\n#R|  # Groups:   ID [5]\n#R|        ID species   num nmarked\n#R|     &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt;\n#R|   1     1 BLG        10       8\n#R|   2     1 LMB         5       2\n#R|   3     1 YEP         5       2\n#R|   4     2 LMB         9       5\n#R|   5     2 YEP         7       2\n#R|   6     3 BLG        12       3\n#R|   7     3 YEP         7       4\n#R|   8     4 BLG         1       0\n#R|   9     4 LMB        11       4\n#R|  10     4 YEP        11       7\n#R|  11     5 BLG         9       6\n\n\nThere are two things to note in this output. First, that there are no zeroes for num and nmarked for the same species and gear sets as before. Second, the summarization was across two groups but summarize() only removes one of the group_by() variables. Thus, this result is still grouped by ID as shown above, which will interfere with using complete() to add the zeroes. This grouping can be removed with ungroup() as shown below before using complete().\n\ncatch2 &lt;- catch2 |&gt;\n  ungroup() |&gt;\n  complete(ID,species,fill=list(num=0,nmarked=0)) |&gt;\n  as.data.frame()\ncatch2\n\n#R|     ID species num nmarked\n#R|  1   1     BLG  10       8\n#R|  2   1     LMB   5       2\n#R|  3   1     YEP   5       2\n#R|  4   2     BLG   0       0\n#R|  5   2     LMB   9       5\n#R|  6   2     YEP   7       2\n#R|  7   3     BLG  12       3\n#R|  8   3     LMB   0       0\n#R|  9   3     YEP   7       4\n#R|  10  4     BLG   1       0\n#R|  11  4     LMB  11       4\n#R|  12  4     YEP  11       7\n#R|  13  5     BLG   9       6\n#R|  14  5     LMB   0       0\n#R|  15  5     YEP   0       0\n\n\n \n\n\nMore Information that Does Not Get Zeroes\nSuppose there is a data frame called geardat that contains information specific to each gear set.\n\n\n\n\ngeardat\n\n#R|    ID mon year  lake run effort\n#R|  1  1 May 2018 round   1   1.34\n#R|  2  2 May 2018 round   2   1.87\n#R|  3  3 May 2018 round   3   1.56\n#R|  4  4 May 2018  twin   1   0.92\n#R|  5  5 May 2018  twin   2   0.67\n\n\nAnd, for the purposes of this example, suppose that we have summarized catch data WITHOUT the zeroes having been added.\n\ncatch3 &lt;- fishdat2 |&gt;\n  group_by(ID,species) |&gt;\n  summarize(num=n(),\n            nmarked=sum(ifelse(marked==\"YES\",1,0))) |&gt;\n  as.data.frame()\ncatch3\n\n#R|     ID species num nmarked\n#R|  1   1     BLG  10       8\n#R|  2   1     LMB   5       2\n#R|  3   1     YEP   5       2\n#R|  4   2     LMB   9       5\n#R|  5   2     YEP   7       2\n#R|  6   3     BLG  12       3\n#R|  7   3     YEP   7       4\n#R|  8   4     BLG   1       0\n#R|  9   4     LMB  11       4\n#R|  10  4     YEP  11       7\n#R|  11  5     BLG   9       6\n\n\nFinally, suppose that these summarized catch data are joined with the gear data such that the gear set specific information is shown with each catch.\n\ncatch3 &lt;- right_join(geardat,catch3,by=\"ID\")\ncatch3\n\n#R|     ID mon year  lake run effort species num nmarked\n#R|  1   1 May 2018 round   1   1.34     BLG  10       8\n#R|  2   1 May 2018 round   1   1.34     LMB   5       2\n#R|  3   1 May 2018 round   1   1.34     YEP   5       2\n#R|  4   2 May 2018 round   2   1.87     LMB   9       5\n#R|  5   2 May 2018 round   2   1.87     YEP   7       2\n#R|  6   3 May 2018 round   3   1.56     BLG  12       3\n#R|  7   3 May 2018 round   3   1.56     YEP   7       4\n#R|  8   4 May 2018  twin   1   0.92     BLG   1       0\n#R|  9   4 May 2018  twin   1   0.92     LMB  11       4\n#R|  10  4 May 2018  twin   1   0.92     YEP  11       7\n#R|  11  5 May 2018  twin   2   0.67     BLG   9       6\n\n\nThese data simulate what might be seen from a flat database.\nWith these data, zeroes still need to be added as defined by missing combinations of ID and species. However, if only these two variables are included in complete() then zeroes will be added for mon, year, lake, run, and effort, which is not desired. These five variables are connected to or “nested” with the ID variable (i.e., if you know ID then you know the values of these other variables) and should be treated as a group. Nesting of variables can be handled in complete() by including the names of all the connected variables in nesting().\n\ncatch3 |&gt; complete(nesting(ID,mon,year,lake,run,effort),species,\n                    fill=list(num=0,nmarked=0)) |&gt;\n  as.data.frame()\n\n#R|     ID mon year  lake run effort species num nmarked\n#R|  1   1 May 2018 round   1   1.34     BLG  10       8\n#R|  2   1 May 2018 round   1   1.34     LMB   5       2\n#R|  3   1 May 2018 round   1   1.34     YEP   5       2\n#R|  4   2 May 2018 round   2   1.87     BLG   0       0\n#R|  5   2 May 2018 round   2   1.87     LMB   9       5\n#R|  6   2 May 2018 round   2   1.87     YEP   7       2\n#R|  7   3 May 2018 round   3   1.56     BLG  12       3\n#R|  8   3 May 2018 round   3   1.56     LMB   0       0\n#R|  9   3 May 2018 round   3   1.56     YEP   7       4\n#R|  10  4 May 2018  twin   1   0.92     BLG   1       0\n#R|  11  4 May 2018  twin   1   0.92     LMB  11       4\n#R|  12  4 May 2018  twin   1   0.92     YEP  11       7\n#R|  13  5 May 2018  twin   2   0.67     BLG   9       6\n#R|  14  5 May 2018  twin   2   0.67     LMB   0       0\n#R|  15  5 May 2018  twin   2   0.67     YEP   0       0\n\n\nIt is possible to have nesting with species as well. Suppose, for example, that the scientific name for the species was included in the original fishdata2 that was summarized (using a combination of the examples from above, but not shown here) to catch4.\n\n\n\n\ncatch4\n\n#R|     ID species                spsci num nmarked mon year  lake run effort\n#R|  1   1     BLG  Lepomis macrochirus  10       8 May 2018 round   1   1.34\n#R|  2   1     LMB Micropterus dolomieu   5       2 May 2018 round   1   1.34\n#R|  3   1     YEP     Perca flavescens   5       2 May 2018 round   1   1.34\n#R|  4   2     LMB Micropterus dolomieu   9       5 May 2018 round   2   1.87\n#R|  5   2     YEP     Perca flavescens   7       2 May 2018 round   2   1.87\n#R|  6   3     BLG  Lepomis macrochirus  12       3 May 2018 round   3   1.56\n#R|  7   3     YEP     Perca flavescens   7       4 May 2018 round   3   1.56\n#R|  8   4     BLG  Lepomis macrochirus   1       0 May 2018  twin   1   0.92\n#R|  9   4     LMB Micropterus dolomieu  11       4 May 2018  twin   1   0.92\n#R|  10  4     YEP     Perca flavescens  11       7 May 2018  twin   1   0.92\n#R|  11  5     BLG  Lepomis macrochirus   9       6 May 2018  twin   2   0.67\n\n\nThe zeroes are then added to this data.frame making sure to note the nesting of species and spsci.\n\ncatch4 |&gt; \n  complete(nesting(ID,mon,year,lake,run,effort),\n           nesting(species,spsci),\n           fill=list(num=0,nmarked=0)) |&gt;\n  as.data.frame()\n\n#R|     ID mon year  lake run effort species                spsci num nmarked\n#R|  1   1 May 2018 round   1   1.34     BLG  Lepomis macrochirus  10       8\n#R|  2   1 May 2018 round   1   1.34     LMB Micropterus dolomieu   5       2\n#R|  3   1 May 2018 round   1   1.34     YEP     Perca flavescens   5       2\n#R|  4   2 May 2018 round   2   1.87     BLG  Lepomis macrochirus   0       0\n#R|  5   2 May 2018 round   2   1.87     LMB Micropterus dolomieu   9       5\n#R|  6   2 May 2018 round   2   1.87     YEP     Perca flavescens   7       2\n#R|  7   3 May 2018 round   3   1.56     BLG  Lepomis macrochirus  12       3\n#R|  8   3 May 2018 round   3   1.56     LMB Micropterus dolomieu   0       0\n#R|  9   3 May 2018 round   3   1.56     YEP     Perca flavescens   7       4\n#R|  10  4 May 2018  twin   1   0.92     BLG  Lepomis macrochirus   1       0\n#R|  11  4 May 2018  twin   1   0.92     LMB Micropterus dolomieu  11       4\n#R|  12  4 May 2018  twin   1   0.92     YEP     Perca flavescens  11       7\n#R|  13  5 May 2018  twin   2   0.67     BLG  Lepomis macrochirus   9       6\n#R|  14  5 May 2018  twin   2   0.67     LMB Micropterus dolomieu   0       0\n#R|  15  5 May 2018  twin   2   0.67     YEP     Perca flavescens   0       0\n\n\n \n\n\nFinal Thoughts\nThis is my first exploration with complete() and it looks promising for this task of adding zeroes to data frames of catch by gear set for gear sets in which a species was not caught. I will be curious to hear what others think of this function and how it might fit in their workflow.\n \n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching Resources",
    "section": "",
    "text": "Order By\n       Default\n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n         \n          Published - Oldest\n        \n         \n          Published - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nWeight-Length Relationship\n\n\nFit a weight-length relationsip for Trout Lake Yellow Perch.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeight-Length Relationship\n\n\nFit a weight-length relationsip for Inch Lake Largemouth Bass.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Depletion Data\n\n\nEstimate abundance of Largemouth Bass in Lake Guntersville (AL) using electrofishing depletion data.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge Comparisons\n\n\nCompare age estimates from two structures taken from Walleye sampled from Pymatuning Sanctuary (PA).\n\n\n\nDerek H. Ogle\n\n\nDec 19, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality\n\n\nUse catch curve methods to estimate mortality of a population of Lake Superior Lake Trout.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Mark-Recapture Data\n\n\nUse single census mark-recapture techniques to estimate abundance of Brown Trout in a stretch of the Ugly River (New Zealand).\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual Growth\n\n\nFit a von Bertalanffy growth function to age-length data from Lake Ontario Rock Bass.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge-Length Key\n\n\nConstruct and apply an age-length key for Lake Ontario Rock Bass.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Mark-Recapture Data\n\n\nUse single census mark-recapture techniques to estimate abundance of Rainbow Trout in Upper Niagara Springs Pond (ID).\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Mark-Recapture Data\n\n\nUse multiple census mark-recapture techniques to estimate abundance of Tan Riffleshell mussels in Indian Creek (VA).\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Depletion Data\n\n\nEstimate abundance of European Bullhead in a headwater stream in England.\n\n\n\nDerek H. Ogle\n\n\nDec 23, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Mark-Recapture Data\n\n\nUse multiple census mark-recapture techniques to estimate abundance of YOY Walleye in Pine Lake (WI).\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual Growth\n\n\nCompare a von Bertalanffy growth function among sexes of Lake Tjeukemeer European Perch.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge-Length Key\n\n\nConstruct and apply an age-length key for Striped Bass from Virginia waters of the Atlantic Ocean.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge-Length Key\n\n\nConstruct and apply an age-length key for Lake Erie Freshwater Drum.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge Comparisons\n\n\nCompare age estimates between two readers or from two structures taken from Lake Champlain (VT) Lake Whitefish.\n\n\n\nDerek H. Ogle\n\n\nDec 19, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nStock-Recruitment\n\n\nFit stock-recruitment models to Escanaba Lake (WI) Walleye data.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSize Structure\n\n\nEstimate size structure statistics and compare them among groups of Yellow Perch from Saginaw Bay, Lake Huron (MI).\n\n\n\nDerek H. Ogle\n\n\nDec 20, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Depletion Data\n\n\nEstimate abundance of Ruffe in Chequamegon Bay, Lake Superior (WI) from experimental trawling data.\n\n\n\nDerek H. Ogle\n\n\nDec 23, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality\n\n\nUse catch curve methods to estimate mortality of a population of Lake Superior Lake Trout.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nBag Limits in Minnesota\n\n\nUse creel data to assess the impact of bag limits on reducing harvest of freshwater fish.\n\n\n\nDerek H. Ogle\n\n\nDec 21, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Removal Data\n\n\nEstimate abundance of Signal Crayfish in Bookill Gill Beck captured with an experimental method.\n\n\n\nDerek H. Ogle\n\n\nDec 23, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbundance from Mark-Recapture Data\n\n\nUse single census mark-recapture techniques to estimate abundance of Sea Lamprey in a Lake Huron tributary.\n\n\n\nDerek H. Ogle\n\n\nDec 23, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual Growth\n\n\nFit a von Bertalanffy growth function to age-length data of Slimy Sculpins from Alaska.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\nPractice data wrangling skills with the Trouit Lake Yellow Perch data set.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSize Structure\n\n\nEstimate size structure statistics and compare them among groups of Yellow Perch from Trout Lake (WI).\n\n\n\nDerek H. Ogle\n\n\nMar 19, 2022\n\n\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraphing\n\n\nPractice graphing skills for the Trout Lake Yellow Perch data set.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeight-Length Relationship\n\n\nFit a weight-length relationsip for Inch Lake Bluegill.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCondition\n\n\nExamine relative weight as a measure of condition for Inch Lake Bluegill.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\nPractice data wrangling skills for the Inch Lake data set.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraphing\n\n\nPractice graphing skills for the Inch Lake data set.\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2019\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSize Structure of Bluegills Collected with Different Gears\n\n\nDevelop size structure summaries to infer gear type used.\n\n\n\nDerek H. Ogle\n\n\nDec 21, 2022\n\n\n\n\n\nDec 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSize Structure of Species in a Cold-Water Wisconsin Stream\n\n\nDevelop size structure summaries to infer species identification.\n\n\n\nDerek H. Ogle\n\n\nDec 20, 2022\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding Zero Catches\n\n\nAdd zeroes for species not caught so that CPE calculations are correct\n\n\n\nDerek H. Ogle\n\n\nApr 19, 2018\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nHalf-Life Property of K\n\n\nDemonstrate that K is the time it takes for a mean fish to grow halfway to the aymptotic length.\n\n\n\nDerek H. Ogle\n\n\nJun 12, 2016\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing fishR Data with Students\n\n\nThoughts on how best to use R package data with students.\n\n\n\nDerek H. Ogle\n\n\nDec 19, 2022\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/data_fishR_byTopic.html",
    "href": "pages/data_fishR_byTopic.html",
    "title": "Datasets from FSA and FSAdata by Topic",
    "section": "",
    "text": "Length Expansion\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBluntnoseIL1\nSubampled lengths of Bluntnose Minnows from Inch Lake, WI.\n\n\nTroutperchLM3\nSubsampled lengths of Troutperch from Lake Michigan, 1977.\n\n\n\n \n\n\nLength Conversion\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBluegillLM\nLengths and weights for Bluegill from Lake Mary, MN.\n\n\nLakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nPallid\nLengths and weights for Pallid Sturgeon from four locations in the Missouri River.\n\n\n\n \n\n\nAge Comparison\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nAlewifeLH\nAges of Lake Huron Alewife assigned from otoliths and scales.\n\n\nBluefishAge\nAges of Bluefish assigned from otoliths by two readers.\n\n\nCroaker1\nAges of Atlantic Croaker assigned from otoliths by two readers.\n\n\nMorwong1\nAges of Morwong assigned from otoliths by Reader A at two times.\n\n\nMorwong2\nAges of Morwong assigned from otoliths by Reader B at two times.\n\n\nMorwong3\nAges of Morwong assigned from otoliths by two readers.\n\n\nMulletBS\nAges of Red Mullet assigned from whole and broken-burnt otoliths.\n\n\nMuskieSLR\nAges of Muskellunge assigned from scales and cleithra.\n\n\nPygmyWFBC\nBiological data for Pygmy Whitefish from Dina Lake #1 (British Columbia), 2000 and 2001.\n\n\nShadCR\nAges of American Shad assigned from scales by three readers at two times.\n\n\nStripedBass1\nAges of Striped Bass assigned from scales and otoliths.\n\n\nStripedBass4\nAges of Striped Bass assigned from scales by two readers.\n\n\nStripedBass5\nAges of Striped Bass assigned from otoliths by two readers.\n\n\nStripedBass6\nAges of Striped Bass assigned from scales and otoliths.\n\n\nWalleyePS\nAges of Walleye assigned from otoliths, scales, and spines.\n\n\nWhitefishMB\nAges of Lake Whitefish from four lakes assigned from scales and fin-rays.\n\n\nYTFlounder\nAges of Yellowtail Flounder assigned from scales and otoliths.\n\n\n\n \n\n\nAge-Length Key\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nCreekChub\nAges (subsample) and lengths (all fish) for Creek Chub.\n\n\nFWDrumLE2\nAges (subsample) and lengths (all fish) for Freshwater Drum from Lake Erie.\n\n\nJonubi2\nAges (subsample) and lengths (all fish) of Jonubi.\n\n\nMorwong4a\nAges (subsample) and lengths (all fish) for Morwong from Morwong4.\n\n\nRockBassLO2\nAges (subsample) and lengths (all fish) for Rock Bass from Lake Ontario.\n\n\nSiscowetMI2004\nAges (subsample) and lengths (all fish) for male and female Siscowet Lake Trout captured at four locations in Michigan waters of Lake Superior.\n\n\nSnapperHG1\nAge (subsample) and length (all fish) of Snapper from two survey locations.\n\n\nSnapperHG2\nAges (subsample) and lengths (all fish) for Snapper.\n\n\nSpotVA2\nAges (subsample) and lengths (all fish) for Spot.\n\n\nStripedBass3\nAges (subsample) and lengths (all fish) for Striped Bass.\n\n\nWR79\nAges and lengths for a hypothetical sample from Westerheim and Ricker (1979).\n\n\n\n \n\n\nBack-Calculation\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nSMBassWB\nGrowth increment data for West Bearskin Lake, MN, Smallmouth Bass.\n\n\n\n \n\n\nLength Frequency\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBluegillLM\nLengths and weights for Bluegill from Lake Mary, MN.\n\n\nBluntnoseIL1\nSubampled lengths of Bluntnose Minnows from Inch Lake, WI.\n\n\nBullTroutRML1\nLengths and weights for Bull Trout from two Rocky Mountain lakes and two eras.\n\n\nCiscoTL\nLengths, weights, and sex of Cisco from Trout Lake, WI.\n\n\nHerman\nLengths for Walleye, Yellow Perch, Black Crappie, and Black Bullheads from Lake Herman, SD.\n\n\nInchLake1\nLengths for all fish captured in Inch Lake, WI, in two years\n\n\nInchLake2\nLengths and weights for fish captured in Inch Lake\n\n\nLMBassBL\nLengths for Largemouth Bass from Boomer Lake, OK.\n\n\nLMBassLCB\nLengths for Largemouth Bass from Lake Carl Blackwell, OK.\n\n\nLakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nPygmyWFBC\nBiological data for Pygmy Whitefish from Dina Lake #1 (British Columbia), 2000 and 2001.\n\n\nRBSmeltLM\nLengths for Rainbow Smelt from Lake Michigan, 1977.\n\n\nRuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nRuffeTL89\nLengths of Ruffe captured from the St. Louis River in July, 1989.\n\n\nSculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nSnapper\nLengths for Snapper from Australia.\n\n\nTroutperchLM2\nLengths for Troutperch from Lake Michigan, 1977.\n\n\nTroutperchLM3\nSubsampled lengths of Troutperch from Lake Michigan, 1977.\n\n\nWalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nYPerchGL\nLengths and weights of Yellow Perch from Grafton Lake (ME) by year.\n\n\nYPerchSB1\nLengths for Yellow Perch from two locations in Saginaw Bay, Lake Michigan.\n\n\nYPerchTL\nLengths and weights for Yellow Perch from Trout Lake, WI.\n\n\n\n \n\n\nSize Structure\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBluntnoseIL1\nSubampled lengths of Bluntnose Minnows from Inch Lake, WI.\n\n\nHerman\nLengths for Walleye, Yellow Perch, Black Crappie, and Black Bullheads from Lake Herman, SD.\n\n\nInchLake1\nLengths for all fish captured in Inch Lake, WI, in two years\n\n\nLMBassBL\nLengths for Largemouth Bass from Boomer Lake, OK.\n\n\nLMBassLCB\nLengths for Largemouth Bass from Lake Carl Blackwell, OK.\n\n\nLakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nPSDlit\nGabelhouse five-cell length categories for various species.\n\n\nRBSmeltLM\nLengths for Rainbow Smelt from Lake Michigan, 1977.\n\n\nRuffeTL89\nLengths of Ruffe captured from the St. Louis River in July, 1989.\n\n\nSculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nSnapper\nLengths for Snapper from Australia.\n\n\nTroutperchLM2\nLengths for Troutperch from Lake Michigan, 1977.\n\n\nTroutperchLM3\nSubsampled lengths of Troutperch from Lake Michigan, 1977.\n\n\nWalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nYPerchSB1\nLengths for Yellow Perch from two locations in Saginaw Bay, Lake Michigan.\n\n\nYPerchTL\nLengths and weights for Yellow Perch from Trout Lake, WI.\n\n\n\n \n\n\nWeight-Length\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBlackDrum2001\nBiological data for Black Drum from Virginia waters of the Atlantic Ocean, 2001.\n\n\nBluegillLM\nLengths and weights for Bluegill from Lake Mary, MN.\n\n\nBullTroutRML1\nLengths and weights for Bull Trout from two Rocky Mountain lakes and two eras.\n\n\nChinookArg\nLengths and weights for Chinook Salmon from three locations in Argentina.\n\n\nCiscoTL\nLengths, weights, and sex of Cisco from Trout Lake, WI.\n\n\nInchLake2\nLengths and weights for fish captured in Inch Lake\n\n\nLakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nPallid\nLengths and weights for Pallid Sturgeon from four locations in the Missouri River.\n\n\nPygmyWFBC\nBiological data for Pygmy Whitefish from Dina Lake #1 (British Columbia), 2000 and 2001.\n\n\nRuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nSculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nWalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nYPerchGL\nLengths and weights of Yellow Perch from Grafton Lake (ME) by year.\n\n\nYPerchTL\nLengths and weights for Yellow Perch from Trout Lake, WI.\n\n\n\n \n\n\nDepletion\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBlueCrab\nCatch and effort data for male Blue Crabs.\n\n\nDarterMahon\nCatch and effort data for Fantail Darter.\n\n\nJobfishSIO\nCatch and effort data for South Indian Ocean Jobfish.\n\n\nLobsterHI\nCatch and effort data for Hawaiian Islands Slipper Lobster.\n\n\nLobsterPEI\nCatch and effort data for Prince Edward Island Lobster.\n\n\nPathfinder\nCatch and effort for three Snapper species in a depletion experiment.\n\n\nPikeIL\nCatch and effort data for Northern Pike from Island Lake, NB.\n\n\nSMBassLS\nCatch-effort data for Little Silver Lake (Ont) Smallmouth Bass.\n\n\nShrimpGuam\nCatch and effort data for Deepwater Caridean Shrimp.\n\n\n\n \n\n\nRemoval\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBrookTroutNEWP\nCatches in removal events for Brook Trout in the Nashwaak Experimental Watersheds Project.\n\n\nBrookTroutNEWP1\nCatches in removal events for Brook Trout in the Nashwaak Experimental Watersheds Project.\n\n\nDeckeretal1999\nCatches in removal events of Cutthroat Trout and Coho Salmon in Little Stawamus Creek (British Columbia, Canada) in 1997.\n\n\nGreensCreekMine\nCatches in removal events of Coho Salmon and Dolly Varden Char at various locations near the Greens Creek (AK) Mine site.\n\n\nJonesStockwell\nCatches in removal events of Brown and Rainbow Trout at various locations.\n\n\nSalmonADP\nCatches in removal events of salmon parr.\n\n\nSalmonidsMCCA\nCatches in removal events of Cutthroate Trout and Steelhead of various sizes in two reaches of McGarvey Creek (CA).\n\n\nSimonsonLyons\nCatches in removal events of trout at various locations.\n\n\nTroutADP\nCatches in removal events of trout.\n\n\n\n \n\n\nCapture-Recapture\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBluegillJL\nCapture histories (2 samples) of Bluegill from Jewett Lake, MI.\n\n\nBrookTroutOnt\nSummarized single mark-recapture data for Brook Trout across many years.\n\n\nBrownTroutVC1\nSingle census mark-recapture data with lengths for Brown Trout from Valley Creek, MN.\n\n\nCutthroatAL\nCapture histories (9 samples) of Cutthroat Trout from Auke Lake.\n\n\nCutthroatALf\nCapture histories (9 samples) of Cutthroat Trout from Auke Lake.\n\n\nHumpbackWFCR\nCapture histories (2 sample) of Humpback Whitefish.\n\n\nMuskieWI06MR\nSummarized mark-recapture data for Muskellunge from many Wisconsin Lakes, 2006.\n\n\nPikeHL\nCapture histories (2 samples) of Northern Pike from Harding Lake.\n\n\nPikeNY\nSummarized multiple mark-recapture data for all Northern Pike from Buckhorn Marsh, NY.\n\n\nPikeNYPartial1\nCapture histories (4 samples), in capture history format, of a subset of Northern Pike from Buckhorn Marsh, NY.\n\n\nPikeNYPartial2\nCapture histories (4 samples) of a subset of Northern Pike from Buckthorn Marsh.\n\n\nRBTroutUNSP\nCapture histories (2 sample) of Rainbow Trout.\n\n\nRiffleshell\nSummarized multiple mark-recapture data for Tan Riffleshell.\n\n\nSturgeonBL\nSummarized multiple mark-recapture data for Lake Sturgeon.\n\n\nSunfishIN\nSummarized multiple mark-recapture data for Redear Sunfish.\n\n\nWalleyeMN06b\nSummarized multiple mark-recapture data for Walleye from four lakes in Northern Minnesota.\n\n\nWalleyePL\nSummarized multiple mark-recapture data for YOY walleye.\n\n\n\n \n\n\nMortality\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBassFL\nCatch-at-age for Suwanee and Largemouth Bass.\n\n\nBrookTroutTH\nCatch-at-age for Tobin Harbor, Isle Royale Brook Trout.\n\n\nBullTroutTC\nCatch-at-age for Bull Trout in Trestle Creek, ID.\n\n\nCCatfishNB\nCatch-at-age of Channel Catfish from two sections of the Platte River, NB.\n\n\nFHCatfish\nCatch-at-age of Flathead Catfish from three southeastern rivers.\n\n\nFHCatfishATL\nCatch-at-age of Flathead Catfish from three Atlantic rivers.\n\n\nMenhaden1\nCatch-at-age for Gulf Menhaden, 1964-2004.\n\n\nRockBassCL\nCatch-at-age of Cayuga Lake Rock Bass.\n\n\nSunfishLP\nCatch-at-age for Bluegill and Redear Sunfish in Florida.\n\n\nWalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nWalleyeKS\nCatch-at-age for Walleye from eight Kansas reservoirs.\n\n\nWalleyeMN06a\nCatch-at-age for Walleye.\n\n\nWalleyeWad\nCatches-at-age for male and female Walleye from Lake Winnebago, WI, 2010.\n\n\nWhiteGrunt1\nCatch-at-age for White Grunt.\n\n\nWhitefishGSL\nCatch-at-age of Great Slave Lake Whitefish (commercial) by area.\n\n\nYPerchCB1\nCatch-at-age for Yellow Perch from Chequamegon Bay, Lake Superior.\n\n\n\n \n\n\nGrowth\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBlackDrum2001\nBiological data for Black Drum from Virginia waters of the Atlantic Ocean, 2001.\n\n\nBlueCatfish\nAges and lengths of Blue Catfish.\n\n\nBluegillIL\nLength-at-marking and recapture and time-at-large of Bluegill.\n\n\nBullTroutRML2\nAges and lengths of Bull Trout from two Rocky Mountain lakes at two times.\n\n\nCabezon\nAges, lengths, and maturity for female Cabezon from Oregon.\n\n\nCreekChub\nAges (subsample) and lengths (all fish) for Creek Chub.\n\n\nCroaker2\nAges, lengths, and sexes of Atlantic Croaker by sex.\n\n\nDarterOnt\nAges and lengths of Channel Darters from two locations.\n\n\nEuroPerchTJ\nAges, lengths, and sexes of European Perch.\n\n\nFWDrumLE1\nAges and lengths of Lake Erie Freshwater Drum.\n\n\nJonubi1\nAges and lengths of male Jonubi.\n\n\nLJCisco\nAges and lengths of Longjaw Cisco from two locations in Lake Michigan.\n\n\nLakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nMorwong4\nAges and lengths of Morwong.\n\n\nRBTroutKenai\nLength-at-marking and recapture and time-at-large of Rainbow Trout.\n\n\nRWhitefishAI\nAges and lengths of Round Whitefish.\n\n\nRWhitefishIR\nAges and lengths of Round Whitefish.\n\n\nRedDrum\nAges and lengths for Red Drum from the Atlantic Coast.\n\n\nRockBassLO1\nAges and lengths of Lake Ontario Rock Bass.\n\n\nRuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nSMBassWB\nGrowth increment data for West Bearskin Lake, MN, Smallmouth Bass.\n\n\nSardineChile\nAges and lengths of two year-classes of Sardine from Chilean waters.\n\n\nSardineLK\nAges and lengths of larval Lake Tanganyika Sardine.\n\n\nSculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nSiscowetMI2004\nAges (subsample) and lengths (all fish) for male and female Siscowet Lake Trout captured at four locations in Michigan waters of Lake Superior.\n\n\nSpotVA1\nAge and length of spot.\n\n\nSpottedSucker1\nAges and lengths of Spotted Sucker.\n\n\nStripedBass2\nAges and lengths of Atlantic Ocean Striped Bass.\n\n\nTroutBR\nAges and lengths of migratory Brown and Rainbow Trout.\n\n\nTroutperchLM1\nAges, lengths, and sexes of Troutperch.\n\n\nWalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nWalleyeML\nBack-calculated lengths-at-age for Walleye from Lake Mille Lacs, 2000-2011.\n\n\nWalleyeRL\nGrowth increment data for Red Lakes Walleye.\n\n\nWhiteGrunt2\nAges, lengths, and sexes of White Grunt.\n\n\nYERockfish\nAges, lengths, and maturity for Yelloweye Rockfish.\n\n\n\n \n\n\nRecruitment\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBSkateGB\nStock and recruitment data for Barndoor Skate from Georges Bank, 1966-2007.\n\n\nBloaterLH\nStock and recruitment data for Lake Huron Bloaters, 1981-1996.\n\n\nBrookTroutNC\nStock and recruitment data for Brook Trout from Ball Creek, NC, 1991-2004.\n\n\nChinookKR\nStock and recruitment data for Klamath River Chinook Salmon, 1979-2000.\n\n\nCodNorwegian\nStock and recruitment data for Norwegian cod, 1937-1960.\n\n\nCrappieARMS\nStock and recruitment data for Crappies from four reservoirs in Arkansas and Mississippi, USA.\n\n\nHake\nStock and recruitment data for Hake, 1982-1996.\n\n\nHalibutPAC\nStock and recruitment data for Pacific Halibut, 1929-1991.\n\n\nHerringBWE\nStock and recruitment data for Blackwater Estuary Herring, 1962-1997.\n\n\nHerringISS\nStock and recruitment data for Icelandic summer spawning Herring, 1946-1996.\n\n\nKingCrabAK\nStock and recruitment data for Red King Crab in Alaska, 1960-2004.\n\n\nLakeTroutGIS\nStock and recruitment data for Lake Trout from Gull Island Shoal, Lake Superior, 1964-1991.\n\n\nLakeTroutMI\nStock and recruitment data for Lake Trout in Lake Superior, 1971-1991.\n\n\nLizardfish\nStock and recruitment data for Greater Lizardfish, 1955-1964.\n\n\nPSalmonAK\nStock and recruitment data for Alaskan Pink Salmon, 1960-1990.\n\n\nPikeWindermere\nStock and recruitment data for Northern Pike from Lake Windermere, 1944-1981.\n\n\nRBSmeltErie\nRecruitment time-series for Rainbow Smelt in Lake Erie, 1977-1996.\n\n\nSLampreyGL\nStock and recruitment data for Sea Lamprey in the Great Lakes, 1997-2007.\n\n\nSardinesPacific\nStock and recruitment data for Pacific Sardines, 1935-1990.\n\n\nSockeyeKL\nStock and recruitment data for Sockeye Salmon from Karluk Lake, AK, 1921-1948.\n\n\nSockeyeSR\nStock and recruitment data for Skeena River Sockeye Salmon, 1940-1967.\n\n\nTPrawnsEG\nStock and recruitment data for Exmouth Gulf Tiger Prawn, 1970-83.\n\n\nVendaceLP\nStock and recruitment data for Vendace from Lake Puulavesi, 1982-1996.\n\n\nVendaceLP2\nStock and recruitment data for Vendace from Lake Pyhajarvi.\n\n\nWShrimpGA\nStock and recruitment data for White Shrimp off the coast of Georgia (USA), 1979-2000.\n\n\nWalleyeEL\nStock and recruitment data for Walleye from Escanaba Lake, WI, 1958-1992.\n\n\nWalleyeErie\nRecruitment time-series for Walleye in Lake Erie, 1959-1972.\n\n\nWalleyeWyrlng\nAnnual catches of yearling Walleye in bottom trawls from Lake Winnebago, WI, 1986-2010.\n\n\nWhitefishTB\nStock and recruitment data for Lake Whitefish in Thunder Bay, Lake Superior, 1975-1988.\n\n\nYPerchCB2\nStock and recruitment data for Yellow Perch in Chequamegon Bay, 1975-1986.\n\n\nYPerchGB\nRecruitment time-series for Yellow Perch in Green Bay, 1978-1992.\n\n\nYPerchRL\nRecruitment time-series for Yellow Perch in Red Lakes, MN, 1942-1960.\n\n\nYPerchSB\nStock and recruitment data for Yellow Perch from South Bay, Lake Huron, 1950-1983.\n\n\n\n \n\n\nMaturity\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nCabezon\nAges, lengths, and maturity for female Cabezon from Oregon.\n\n\nRuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nWalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nYERockfish\nAges, lengths, and maturity for Yelloweye Rockfish.\n\n\n\n \n\n\nData Manipulation\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBGHRfish\nFish information from samples collected from Big Hill Reservoir, KS, 2014.\n\n\nBGHRsample\nInformation for each electrofishing sample from Big Hill Reservoir, KS, 2014.\n\n\n\n \n\n\nOther\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nAfricanRivers\nCharacteristics of a sample of West African rivers.\n\n\nCasselman1990\nInstantaneous growth rates for two calcified ageing structures.\n\n\nCreelMN\nResults of a large number of creel surveys in Minnestoa lakes.\n\n\nEcoli\nPopulation growth of Escherichia coli.\n\n\nGhats\nSpecies accumulation data for fish of the Western Ghats of India.\n\n\nLakeTroutEggs\nLength and egg deposition of Lake Superior Lake Trout.\n\n\nMirex\nMirex concentration, weight, capture year, and species of Lake Ontario salmon.\n\n\nWhitefishLS\nLandings and value of Lake Superior Lake Whitefish."
  },
  {
    "objectID": "pages/data_Comp_byTopic.html",
    "href": "pages/data_Comp_byTopic.html",
    "title": "Fisheries-related datasets in CRAN packages by Topic",
    "section": "",
    "text": "Datasets from packages not controlled by the fishR Core Team (i.e., FSA and FSAdata) are entered manually, so please submit a GitHub Issue (see link at bottom of right sidebar menu) with any corrections or additions.\n\n\n\n \n\nLength Expansion\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BluntnoseIL1\nSubampled lengths of Bluntnose Minnows from Inch Lake, WI.\n\n\nFSAdata::TroutperchLM3\nSubsampled lengths of Troutperch from Lake Michigan, 1977.\n\n\n\n \n\n\nLength Conversion\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BluegillLM\nLengths and weights for Bluegill from Lake Mary, MN.\n\n\nFSAdata::LakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nFSAdata::Pallid\nLengths and weights for Pallid Sturgeon from four locations in the Missouri River.\n\n\n\n \n\n\nAge Comparison\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::AlewifeLH\nAges of Lake Huron Alewife assigned from otoliths and scales.\n\n\nFSAdata::BluefishAge\nAges of Bluefish assigned from otoliths by two readers.\n\n\nFSAdata::Croaker1\nAges of Atlantic Croaker assigned from otoliths by two readers.\n\n\nFSAdata::Morwong1\nAges of Morwong assigned from otoliths by Reader A at two times.\n\n\nFSAdata::Morwong2\nAges of Morwong assigned from otoliths by Reader B at two times.\n\n\nFSAdata::Morwong3\nAges of Morwong assigned from otoliths by two readers.\n\n\nFSAdata::MulletBS\nAges of Red Mullet assigned from whole and broken-burnt otoliths.\n\n\nFSAdata::MuskieSLR\nAges of Muskellunge assigned from scales and cleithra.\n\n\nFSAdata::PygmyWFBC\nBiological data for Pygmy Whitefish from Dina Lake #1 (British Columbia), 2000 and 2001.\n\n\nFSAdata::ShadCR\nAges of American Shad assigned from scales by three readers at two times.\n\n\nFSAdata::StripedBass1\nAges of Striped Bass assigned from scales and otoliths.\n\n\nFSAdata::StripedBass4\nAges of Striped Bass assigned from scales by two readers.\n\n\nFSAdata::StripedBass5\nAges of Striped Bass assigned from otoliths by two readers.\n\n\nFSAdata::StripedBass6\nAges of Striped Bass assigned from scales and otoliths.\n\n\nFSAdata::WalleyePS\nAges of Walleye assigned from otoliths, scales, and spines.\n\n\nFSAdata::WhitefishMB\nAges of Lake Whitefish from four lakes assigned from scales and fin-rays.\n\n\nFSAdata::YTFlounder\nAges of Yellowtail Flounder assigned from scales and otoliths.\n\n\n\n \n\n\nAge-Length Key\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::CreekChub\nAges (subsample) and lengths (all fish) for Creek Chub.\n\n\nFSAdata::FWDrumLE2\nAges (subsample) and lengths (all fish) for Freshwater Drum from Lake Erie.\n\n\nFSAdata::Jonubi2\nAges (subsample) and lengths (all fish) of Jonubi.\n\n\nFSAdata::Morwong4a\nAges (subsample) and lengths (all fish) for Morwong from Morwong4.\n\n\nFSAdata::RockBassLO2\nAges (subsample) and lengths (all fish) for Rock Bass from Lake Ontario.\n\n\nFSAdata::SiscowetMI2004\nAges (subsample) and lengths (all fish) for male and female Siscowet Lake Trout captured at four locations in Michigan waters of Lake Superior.\n\n\nFSAdata::SnapperHG1\nAge (subsample) and length (all fish) of Snapper from two survey locations.\n\n\nFSAdata::SnapperHG2\nAges (subsample) and lengths (all fish) for Snapper.\n\n\nFSAdata::SpotVA2\nAges (subsample) and lengths (all fish) for Spot.\n\n\nFSAdata::StripedBass3\nAges (subsample) and lengths (all fish) for Striped Bass.\n\n\nFSA::WR79\nAges and lengths for a hypothetical sample from Westerheim and Ricker (1979).\n\n\nfishmethods::alkdata\nAge-length key (summarized form) for Gulf of Hauraki snapper shown in Table 8.3 of Quinn and Deriso (1999)\n\n\n\n \n\n\nBack-Calculation\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSA::SMBassWB\nGrowth increment data for West Bearskin Lake, MN, Smallmouth Bass.\n\n\n\n \n\n\nLength Frequency\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BluegillLM\nLengths and weights for Bluegill from Lake Mary, MN.\n\n\nFSAdata::BluntnoseIL1\nSubampled lengths of Bluntnose Minnows from Inch Lake, WI.\n\n\nFSAdata::BullTroutRML1\nLengths and weights for Bull Trout from two Rocky Mountain lakes and two eras.\n\n\nFSAdata::CiscoTL\nLengths, weights, and sex of Cisco from Trout Lake, WI.\n\n\nFSAdata::Herman\nLengths for Walleye, Yellow Perch, Black Crappie, and Black Bullheads from Lake Herman, SD.\n\n\nFSAdata::InchLake1\nLengths for all fish captured in Inch Lake, WI, in two years\n\n\nFSAdata::InchLake2\nLengths and weights for fish captured in Inch Lake\n\n\nFSAdata::LMBassBL\nLengths for Largemouth Bass from Boomer Lake, OK.\n\n\nFSAdata::LMBassLCB\nLengths for Largemouth Bass from Lake Carl Blackwell, OK.\n\n\nFSAdata::LakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nFSAdata::PygmyWFBC\nBiological data for Pygmy Whitefish from Dina Lake #1 (British Columbia), 2000 and 2001.\n\n\nFSAdata::RBSmeltLM\nLengths for Rainbow Smelt from Lake Michigan, 1977.\n\n\nFSAdata::RuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nFSAdata::RuffeTL89\nLengths of Ruffe captured from the St. Louis River in July, 1989.\n\n\nFSAdata::SculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nfishmethods::Shepherd\nSeasonal length frequency data of Raja clavata\n\n\nFSAdata::Snapper\nLengths for Snapper from Australia.\n\n\nFSAdata::TroutperchLM2\nLengths for Troutperch from Lake Michigan, 1977.\n\n\nFSAdata::TroutperchLM3\nSubsampled lengths of Troutperch from Lake Michigan, 1977.\n\n\nFSAdata::WalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nFSAdata::YPerchGL\nLengths and weights of Yellow Perch from Grafton Lake (ME) by year.\n\n\nFSAdata::YPerchSB1\nLengths for Yellow Perch from two locations in Saginaw Bay, Lake Michigan.\n\n\nFSAdata::YPerchTL\nLengths and weights for Yellow Perch from Trout Lake, WI.\n\n\nTropFishR::alba\nLength-frequency data of the clam Abra alba as presented by Brey et al. (1988)\n\n\nCatDyn::gayhakelm\nLength and Month data of Chilean hake from Artisanal Fishery\n\n\nfishmethods::sblen\nTotal length (inches) of striped bass collected by Massachusetts volunteer anglers in 2014\n\n\n\n \n\n\nSize Structure\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BluntnoseIL1\nSubampled lengths of Bluntnose Minnows from Inch Lake, WI.\n\n\nFSAdata::Herman\nLengths for Walleye, Yellow Perch, Black Crappie, and Black Bullheads from Lake Herman, SD.\n\n\nFSAdata::InchLake1\nLengths for all fish captured in Inch Lake, WI, in two years\n\n\nFSAdata::LMBassBL\nLengths for Largemouth Bass from Boomer Lake, OK.\n\n\nFSAdata::LMBassLCB\nLengths for Largemouth Bass from Lake Carl Blackwell, OK.\n\n\nFSAdata::LakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nFSA::PSDlit\nGabelhouse five-cell length categories for various species.\n\n\nFSAdata::RBSmeltLM\nLengths for Rainbow Smelt from Lake Michigan, 1977.\n\n\nFSAdata::RuffeTL89\nLengths of Ruffe captured from the St. Louis River in July, 1989.\n\n\nFSAdata::SculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nFSAdata::Snapper\nLengths for Snapper from Australia.\n\n\nFSAdata::TroutperchLM2\nLengths for Troutperch from Lake Michigan, 1977.\n\n\nFSAdata::TroutperchLM3\nSubsampled lengths of Troutperch from Lake Michigan, 1977.\n\n\nFSAdata::WalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nFSAdata::YPerchSB1\nLengths for Yellow Perch from two locations in Saginaw Bay, Lake Michigan.\n\n\nFSAdata::YPerchTL\nLengths and weights for Yellow Perch from Trout Lake, WI.\n\n\n\n \n\n\nWeight-Length\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BlackDrum2001\nBiological data for Black Drum from Virginia waters of the Atlantic Ocean, 2001.\n\n\nFSAdata::BluegillLM\nLengths and weights for Bluegill from Lake Mary, MN.\n\n\nFSAdata::BullTroutRML1\nLengths and weights for Bull Trout from two Rocky Mountain lakes and two eras.\n\n\nFSA::ChinookArg\nLengths and weights for Chinook Salmon from three locations in Argentina.\n\n\nFSAdata::CiscoTL\nLengths, weights, and sex of Cisco from Trout Lake, WI.\n\n\nFSAdata::InchLake2\nLengths and weights for fish captured in Inch Lake\n\n\nFSAdata::LakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nFSAdata::Pallid\nLengths and weights for Pallid Sturgeon from four locations in the Missouri River.\n\n\nFSAdata::PygmyWFBC\nBiological data for Pygmy Whitefish from Dina Lake #1 (British Columbia), 2000 and 2001.\n\n\nFSAdata::RuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nFSAdata::SculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nFSAdata::WalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nFSAdata::YPerchGL\nLengths and weights of Yellow Perch from Grafton Lake (ME) by year.\n\n\nFSAdata::YPerchTL\nLengths and weights for Yellow Perch from Trout Lake, WI.\n\n\nfishkirkko2015::fishkirkkojarvi2015\nWeight-length data for six freshwater species from Lake Kirkkojarvi, Finland\n\n\nCatDyn::gayhakelw\nLength and Weight data of Chilean hake from Artisanal Fishery\n\n\n\n \n\n\nDepletion\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BlueCrab\nCatch and effort data for male Blue Crabs.\n\n\nFSAdata::DarterMahon\nCatch and effort data for Fantail Darter.\n\n\nFSAdata::JobfishSIO\nCatch and effort data for South Indian Ocean Jobfish.\n\n\nFSAdata::LobsterHI\nCatch and effort data for Hawaiian Islands Slipper Lobster.\n\n\nFSAdata::LobsterPEI\nCatch and effort data for Prince Edward Island Lobster.\n\n\nFSAdata::Pathfinder\nCatch and effort for three Snapper species in a depletion experiment.\n\n\nFSAdata::PikeIL\nCatch and effort data for Northern Pike from Island Lake, NB.\n\n\nFSA::SMBassLS\nCatch-effort data for Little Silver Lake (Ont) Smallmouth Bass.\n\n\nFSAdata::ShrimpGuam\nCatch and effort data for Deepwater Caridean Shrimp.\n\n\nfishmethods::darter\nSequence of catch data for the faintail darter from removal experiments by Mahon\n\n\n\n \n\n\nRemoval\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BrookTroutNEWP\nCatches in removal events for Brook Trout in the Nashwaak Experimental Watersheds Project.\n\n\nFSAdata::BrookTroutNEWP1\nCatches in removal events for Brook Trout in the Nashwaak Experimental Watersheds Project.\n\n\nFSAdata::Deckeretal1999\nCatches in removal events of Cutthroat Trout and Coho Salmon in Little Stawamus Creek (British Columbia, Canada) in 1997.\n\n\nFSAdata::GreensCreekMine\nCatches in removal events of Coho Salmon and Dolly Varden Char at various locations near the Greens Creek (AK) Mine site.\n\n\nFSAdata::JonesStockwell\nCatches in removal events of Brown and Rainbow Trout at various locations.\n\n\nFSAdata::SalmonADP\nCatches in removal events of salmon parr.\n\n\nFSAdata::SalmonidsMCCA\nCatches in removal events of Cutthroate Trout and Steelhead of various sizes in two reaches of McGarvey Creek (CA).\n\n\nFSAdata::SimonsonLyons\nCatches in removal events of trout at various locations.\n\n\nFSAdata::TroutADP\nCatches in removal events of trout.\n\n\n\n \n\n\nCapture-Recapture\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSA::BluegillJL\nCapture histories (2 samples) of Bluegill from Jewett Lake, MI.\n\n\nFSAdata::BrookTroutOnt\nSummarized single mark-recapture data for Brook Trout across many years.\n\n\nFSAdata::BrownTroutVC1\nSingle census mark-recapture data with lengths for Brown Trout from Valley Creek, MN.\n\n\nFSA::CutthroatAL\nCapture histories (9 samples) of Cutthroat Trout from Auke Lake.\n\n\nFSAdata::CutthroatALf\nCapture histories (9 samples) of Cutthroat Trout from Auke Lake.\n\n\nfishmethods::Hightower\nThe complete capture histories of striped bass for Lake Gaston, North Carolina\n\n\nFSAdata::HumpbackWFCR\nCapture histories (2 sample) of Humpback Whitefish.\n\n\nFSAdata::MuskieWI06MR\nSummarized mark-recapture data for Muskellunge from many Wisconsin Lakes, 2006.\n\n\nFSAdata::PikeHL\nCapture histories (2 samples) of Northern Pike from Harding Lake.\n\n\nFSA::PikeNY\nSummarized multiple mark-recapture data for all Northern Pike from Buckhorn Marsh, NY.\n\n\nFSA::PikeNYPartial1\nCapture histories (4 samples), in capture history format, of a subset of Northern Pike from Buckhorn Marsh, NY.\n\n\nFSAdata::PikeNYPartial2\nCapture histories (4 samples) of a subset of Northern Pike from Buckthorn Marsh.\n\n\nFSAdata::RBTroutUNSP\nCapture histories (2 sample) of Rainbow Trout.\n\n\nFSAdata::Riffleshell\nSummarized multiple mark-recapture data for Tan Riffleshell.\n\n\nFSAdata::SturgeonBL\nSummarized multiple mark-recapture data for Lake Sturgeon.\n\n\nFSAdata::SunfishIN\nSummarized multiple mark-recapture data for Redear Sunfish.\n\n\nFSAdata::WalleyeMN06b\nSummarized multiple mark-recapture data for Walleye from four lakes in Northern Minnesota.\n\n\nFSAdata::WalleyePL\nSummarized multiple mark-recapture data for YOY walleye.\n\n\n\n \n\n\nMortality\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BassFL\nCatch-at-age for Suwanee and Largemouth Bass.\n\n\nFSA::BrookTroutTH\nCatch-at-age for Tobin Harbor, Isle Royale Brook Trout.\n\n\nFSAdata::BullTroutTC\nCatch-at-age for Bull Trout in Trestle Creek, ID.\n\n\nFSAdata::CCatfishNB\nCatch-at-age of Channel Catfish from two sections of the Platte River, NB.\n\n\nFSAdata::FHCatfish\nCatch-at-age of Flathead Catfish from three southeastern rivers.\n\n\nFSAdata::FHCatfishATL\nCatch-at-age of Flathead Catfish from three Atlantic rivers.\n\n\nfishmethods::Jensen\nThe age data are from reconstructed catches of lake whitefish reported by Jensen (1996) in Table 1 and were expanded to individual observations from the age frequency table\n\n\nFSAdata::Menhaden1\nCatch-at-age for Gulf Menhaden, 1964-2004.\n\n\nFSAdata::RockBassCL\nCatch-at-age of Cayuga Lake Rock Bass.\n\n\nFSAdata::SunfishLP\nCatch-at-age for Bluegill and Redear Sunfish in Florida.\n\n\nFSAdata::WalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nFSAdata::WalleyeKS\nCatch-at-age for Walleye from eight Kansas reservoirs.\n\n\nFSAdata::WalleyeMN06a\nCatch-at-age for Walleye.\n\n\nFSAdata::WalleyeWad\nCatches-at-age for male and female Walleye from Lake Winnebago, WI, 2010.\n\n\nFSAdata::WhiteGrunt1\nCatch-at-age for White Grunt.\n\n\nFSAdata::WhitefishGSL\nCatch-at-age of Great Slave Lake Whitefish (commercial) by area.\n\n\nFSAdata::YPerchCB1\nCatch-at-age for Yellow Perch from Chequamegon Bay, Lake Superior.\n\n\nTropFishR::goatfish\nData of Yellowstriped goatfish (Upeneus vittatus) from Manila Bay, Philippines. Can be used for the estimation of the instantaneous mortality rate (Z)\n\n\nfishmethods::goosefish\nThe mean lengths by year and number of observations for length&gt;=smallest length at first capture (Lc) for northern goosefish used in Gedamke and Hoenig (2006)\n\n\nfishmethods::rockbass\nAge data from a sample of rock bass trap-netted from Cayuga Lake, New York\n\n\nTropFishR::whiting\nDataset of North Sea whiting Merlangius merlangus caught during the period 1974-1980\n\n\n\n \n\n\nGrowth\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BlackDrum2001\nBiological data for Black Drum from Virginia waters of the Atlantic Ocean, 2001.\n\n\nFSAdata::BlueCatfish\nAges and lengths of Blue Catfish.\n\n\nFSAdata::BluegillIL\nLength-at-marking and recapture and time-at-large of Bluegill.\n\n\nFSAdata::BullTroutRML2\nAges and lengths of Bull Trout from two Rocky Mountain lakes at two times.\n\n\nFSAdata::Cabezon\nAges, lengths, and maturity for female Cabezon from Oregon.\n\n\nFSAdata::CreekChub\nAges (subsample) and lengths (all fish) for Creek Chub.\n\n\nFSAdata::Croaker2\nAges, lengths, and sexes of Atlantic Croaker by sex.\n\n\nFSAdata::DarterOnt\nAges and lengths of Channel Darters from two locations.\n\n\nFSAdata::EuroPerchTJ\nAges, lengths, and sexes of European Perch.\n\n\nFSAdata::FWDrumLE1\nAges and lengths of Lake Erie Freshwater Drum.\n\n\nFSAdata::Jonubi1\nAges and lengths of male Jonubi.\n\n\nfishmethods::Kimura\nLength and age data for male and female Pacific Hake\n\n\nFSAdata::LJCisco\nAges and lengths of Longjaw Cisco from two locations in Lake Michigan.\n\n\nFSAdata::LakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nMQMF::LatA\nSimulated age data for redfish from eastern Australia in the 1990s\n\n\nFSAdata::Morwong4\nAges and lengths of Morwong.\n\n\nfishmethods::P.donacina\nGrowth increment data derived from a tagging experiment on Paphis donacina\n\n\nFSAdata::RBTroutKenai\nLength-at-marking and recapture and time-at-large of Rainbow Trout.\n\n\nFSAdata::RWhitefishAI\nAges and lengths of Round Whitefish.\n\n\nFSAdata::RWhitefishIR\nAges and lengths of Round Whitefish.\n\n\nFSAdata::RedDrum\nAges and lengths for Red Drum from the Atlantic Coast.\n\n\nFSAdata::RockBassLO1\nAges and lengths of Lake Ontario Rock Bass.\n\n\nFSAdata::RuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nFSA::SMBassWB\nGrowth increment data for West Bearskin Lake, MN, Smallmouth Bass.\n\n\nFSAdata::SardineChile\nAges and lengths of two year-classes of Sardine from Chilean waters.\n\n\nFSAdata::SardineLK\nAges and lengths of larval Lake Tanganyika Sardine.\n\n\nFSAdata::SculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nfishmethods::Shepherd\nSeasonal length frequency data of Raja clavata\n\n\nFSAdata::SiscowetMI2004\nAges (subsample) and lengths (all fish) for male and female Siscowet Lake Trout captured at four locations in Michigan waters of Lake Superior.\n\n\nFSA::SpotVA1\nAge and length of spot.\n\n\nFSAdata::SpottedSucker1\nAges and lengths of Spotted Sucker.\n\n\nFSAdata::StripedBass2\nAges and lengths of Atlantic Ocean Striped Bass.\n\n\nFSAdata::TroutBR\nAges and lengths of migratory Brown and Rainbow Trout.\n\n\nFSAdata::TroutperchLM1\nAges, lengths, and sexes of Troutperch.\n\n\nFSAdata::WalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nFSAdata::WalleyeML\nBack-calculated lengths-at-age for Walleye from Lake Mille Lacs, 2000-2011.\n\n\nFSAdata::WalleyeRL\nGrowth increment data for Red Lakes Walleye.\n\n\nFSAdata::WhiteGrunt2\nAges, lengths, and sexes of White Grunt.\n\n\nFSAdata::YERockfish\nAges, lengths, and maturity for Yelloweye Rockfish.\n\n\nTropFishR::alba\nLength-frequency data of the clam Abra alba as presented by Brey et al. (1988)\n\n\nMQMF::blackisland\nLength from tagging data for blacklip abalone at the Black Island site, Tasmania\n\n\nfishmethods::bonito\nGrowth increment data derived from tagging experiments on Pacific bonito (Sarda chiliensis)\n\n\nAquaticLifeHistory::growth_data\nLength-at-age data for common blacktip sharks (Carcharhinus limbatus) from Indonesia\n\n\nalr3::lakemary\nAge and length of Bluegill from Lake Mary, MN\n\n\nMQMF::minnow\nWeekly length measurements of a minnow for use with seasonal growth curves\n\n\nfishmethods::pinfish\nLength, age and sex data for pinfish (Lagodon rhomboides) from Tampa Bay, Florida\n\n\nfishmethods::rig\nTagging growth increment data for New Zealand rig (Mustelus lenticulatus), after removal of outliers\n\n\nfishmethods::trout\nRelease lengths, recapture lengths and times-at-large for trout in the Kenai River\n\n\nalr3::walleye\nAge and length of Walleye from Butternut Lake, WI in 3 different management periods\n\n\nalr3::wblake\nAge and length of Smallmouth Bass from West Bearsking Lake, MN (exclude age-9 and older)\n\n\nalr3::wblake2\nAge and length of Smallmouth Bass from West Bearsking Lake, MN (all fish)\n\n\n\n \n\n\nRecruitment\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BSkateGB\nStock and recruitment data for Barndoor Skate from Georges Bank, 1966-2007.\n\n\nFSAdata::BloaterLH\nStock and recruitment data for Lake Huron Bloaters, 1981-1996.\n\n\nFSAdata::BrookTroutNC\nStock and recruitment data for Brook Trout from Ball Creek, NC, 1991-2004.\n\n\nFSAdata::ChinookKR\nStock and recruitment data for Klamath River Chinook Salmon, 1979-2000.\n\n\nFSA::CodNorwegian\nStock and recruitment data for Norwegian cod, 1937-1960.\n\n\nFSAdata::CrappieARMS\nStock and recruitment data for Crappies from four reservoirs in Arkansas and Mississippi, USA.\n\n\nFSAdata::Hake\nStock and recruitment data for Hake, 1982-1996.\n\n\nFSAdata::HalibutPAC\nStock and recruitment data for Pacific Halibut, 1929-1991.\n\n\nFSAdata::HerringBWE\nStock and recruitment data for Blackwater Estuary Herring, 1962-1997.\n\n\nFSAdata::HerringISS\nStock and recruitment data for Icelandic summer spawning Herring, 1946-1996.\n\n\nFSAdata::KingCrabAK\nStock and recruitment data for Red King Crab in Alaska, 1960-2004.\n\n\nFSAdata::LakeTroutGIS\nStock and recruitment data for Lake Trout from Gull Island Shoal, Lake Superior, 1964-1991.\n\n\nFSAdata::LakeTroutMI\nStock and recruitment data for Lake Trout in Lake Superior, 1971-1991.\n\n\nFSAdata::Lizardfish\nStock and recruitment data for Greater Lizardfish, 1955-1964.\n\n\nFSAdata::PSalmonAK\nStock and recruitment data for Alaskan Pink Salmon, 1960-1990.\n\n\nFSAdata::PikeWindermere\nStock and recruitment data for Northern Pike from Lake Windermere, 1944-1981.\n\n\nFSAdata::RBSmeltErie\nRecruitment time-series for Rainbow Smelt in Lake Erie, 1977-1996.\n\n\nFSAdata::SLampreyGL\nStock and recruitment data for Sea Lamprey in the Great Lakes, 1997-2007.\n\n\nFSAdata::SardinesPacific\nStock and recruitment data for Pacific Sardines, 1935-1990.\n\n\nFSAdata::SockeyeKL\nStock and recruitment data for Sockeye Salmon from Karluk Lake, AK, 1921-1948.\n\n\nFSAdata::SockeyeSR\nStock and recruitment data for Skeena River Sockeye Salmon, 1940-1967.\n\n\nFSAdata::TPrawnsEG\nStock and recruitment data for Exmouth Gulf Tiger Prawn, 1970-83.\n\n\nFSAdata::VendaceLP\nStock and recruitment data for Vendace from Lake Puulavesi, 1982-1996.\n\n\nFSAdata::VendaceLP2\nStock and recruitment data for Vendace from Lake Pyhajarvi.\n\n\nFSAdata::WShrimpGA\nStock and recruitment data for White Shrimp off the coast of Georgia (USA), 1979-2000.\n\n\nFSAdata::WalleyeEL\nStock and recruitment data for Walleye from Escanaba Lake, WI, 1958-1992.\n\n\nFSAdata::WalleyeErie\nRecruitment time-series for Walleye in Lake Erie, 1959-1972.\n\n\nFSAdata::WalleyeWyrlng\nAnnual catches of yearling Walleye in bottom trawls from Lake Winnebago, WI, 1986-2010.\n\n\nFSAdata::WhitefishTB\nStock and recruitment data for Lake Whitefish in Thunder Bay, Lake Superior, 1975-1988.\n\n\nFSAdata::YPerchCB2\nStock and recruitment data for Yellow Perch in Chequamegon Bay, 1975-1986.\n\n\nFSAdata::YPerchGB\nRecruitment time-series for Yellow Perch in Green Bay, 1978-1992.\n\n\nFSAdata::YPerchRL\nRecruitment time-series for Yellow Perch in Red Lakes, MN, 1942-1960.\n\n\nFSAdata::YPerchSB\nStock and recruitment data for Yellow Perch from South Bay, Lake Huron, 1950-1983.\n\n\nMQMF::tigers\nPrawn recruitment data from Penn and Caputi (1986)\n\n\n\n \n\n\nMaturity\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::Cabezon\nAges, lengths, and maturity for female Cabezon from Oregon.\n\n\nStat2Data::FishEggs\nPercent dry mass of eggs, age, and month for Lake Ontario Lake Trout\n\n\nFSAdata::RuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nFSAdata::WalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nFSAdata::YERockfish\nAges, lengths, and maturity for Yelloweye Rockfish.\n\n\nfishmethods::maki\nFrom Table 1 of Maki et al. (2001) for 3 years combined\n\n\nAquaticLifeHistory::maturity_data\nLength-at-maturity and age-at-maturity data for female silky sharks (Carcharhinus falciformis) from Papua New Guinea\n\n\nMQMF::tasab\nMaturity and length data for Blacklip Abalone from Tasmania\n\n\n\n \n\n\nData Manipulation\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::BGHRfish\nFish information from samples collected from Big Hill Reservoir, KS, 2014.\n\n\nFSAdata::BGHRsample\nInformation for each electrofishing sample from Big Hill Reservoir, KS, 2014.\n\n\n\n \n\n\nOther\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFSAdata::AfricanRivers\nCharacteristics of a sample of West African rivers.\n\n\nFSAdata::Casselman1990\nInstantaneous growth rates for two calcified ageing structures.\n\n\nFSAdata::CreelMN\nResults of a large number of creel surveys in Minnestoa lakes.\n\n\nFSA::Ecoli\nPopulation growth of Escherichia coli.\n\n\nFSAdata::Ghats\nSpecies accumulation data for fish of the Western Ghats of India.\n\n\nFSAdata::LakeTroutEggs\nLength and egg deposition of Lake Superior Lake Trout.\n\n\nFSA::Mirex\nMirex concentration, weight, capture year, and species of Lake Ontario salmon.\n\n\nfishMod::TigerFlathead\nData set arising from a south-east Australia fish survey, see Bax and Williams (1999)\n\n\nFSAdata::WhitefishLS\nLandings and value of Lake Superior Lake Whitefish.\n\n\nMQMF::abdat\nCatch data for the Tasmanian abalone fishery\n\n\nTropFishR::bream\nData of a covered codend experimental catch of the species Threadfin bream (Nemipterus japonicus) in South China Sea\n\n\nfishmethods::catch\nNumber of cod captured in 10 standardized bottom trawl hauls from Massachusetts, 1985\n\n\nfishmethods::codcluslen\nLengths of Atlantic cod caught during Massachusetts Division of Marine Fisheries bottom trawl survey, spring 1985\n\n\nfishmethods::codstrcluslen\nLengths of Atlantic cod caught during Massachusetts Division of Marine Fisheries stratified random bottom trawl survey, spring 1985\n\n\nfishmethods::counts\nRun size data of alewife (Alosa pseudoharengus) in Herring River, MA from 1980-2010\n\n\nfishmethods::cowcod\nCatch data (metric tons) for cowcod Sebastes levis 1900 to 2008\n\n\nMQMF::dataspm\nCatch data for Pink Ling\n\n\nTropFishR::emperor\nInformation about sky emperor (Lethrinus mahsena) and its fisheries of offshore Mauritius banks (Nazareth banks). It can be used for production models\n\n\nCOUNT::fishing\nSite abundance and characteristics from commercial fishing\n\n\nCatDyn::gayhake\nIndustrial and Artisanal Catch and Effort Data from the Chilean Hake Fishery\n\n\nTropFishR::gillnet\nData of an experiment with several gillnets with different mesh sizes\n\n\nfishmethods::haddock\nAge, weight at spawning, partial recruitment, and fraction mature data for haddock (Melanogrammus aeglefinus) used by Gabriel et al. (1989) to calculate spawning stock biomass-per-recruit\n\n\nTropFishR::hake\nLength-frequency data and biological characteristics about hake (Merluccius merluccius) and its fisheries off Senegal\n\n\nfishmethods::kappenman\nPacific cod catch per effort from Table 1 in Kappenman (1999)\n\n\nfishmethods::lingcod\nCatch data (metric tons) for lingcod 1889 to 2001\n\n\nCatDyn::lolgahi\nIndustrial Trawling Data from the Squid Fishery of the Falkland Islands\n\n\nfishmethods::menhaden\nAge, fecundity-at-age, partial recruitment, fraction mature, and nautral mortality data for menhaden to calculate eggs-per-recruit\n\n\nMQMF::npf\nCatch data (year, biomass, effort) for several species in the Australian Northern Prawn Fishery\n\n\nfishmethods::nshrimp\nRecruit and postrecruit survey indices and catch data for Gulf of Maine northern shrimp (Pandulus borealis), 1985-2007\n\n\nMQMF::pttuna\nYellowfin tuna fishery data (catch effort by year) from Pella-Tomlinson 1969\n\n\nMQMF::schaef\nYellowfin tuna fishery data (catch effort by year) from Schaefer 1957\n\n\nfishmethods::sole\nFlathead sole CPUEs for a side-by-side trawl calibration study of National Marine Fisheries Service (NMFS) and Alaska Department of Fish and Game (ADFG) vessels\n\n\nfishmethods::striper\nEstimates of recruits and female spawning stock biomass for striped bass from the Atlantic State Marine Fisheries 2016 stock assessment\n\n\nTropFishR::trammelnet\nData of an experiment with several trammel nets with different mesh sizes\n\n\nTropFishR::trawl_fishery_java\nTimes series of catch and effort data from the trawl fishery off the North coast of Java\n\n\nCatDyn::twelver\nExhaustive recorded operational activity of the Anguilla japonica sport fishery in Taiwan during the 1983-1984 season\n\n\nTropFishR::whiting\nDataset of North Sea whiting Merlangius merlangus caught during the period 1974-1980\n\n\nfishmethods::wolffish\nSpring untransformed mean catch per tow for wolffish (Anarhichas lupus)\n\n\nfishmethods::yellowtail\nFall average catch per tow for southern New England yellowtail flounder"
  },
  {
    "objectID": "pages/contribute.html",
    "href": "pages/contribute.html",
    "title": "Get Involved with fishR",
    "section": "",
    "text": "It is our vision that fishR will be a website where others contribute content or resources. Specific opportunities to get involved or to contribute to this community are:\n\nSubmit edits or corrections to any page (by “Edit this page” and submitting a pull request, by “Report an issue”, or simply sending us an e-mail).\nSubmit a data set for inclusion in FSAdata and thus the fishR webpage.\nSubmit (for here) a post of using R for a particular fisheries-related analysis (we can help you with the formatting).\nSubmit (for here) a teaching-related concept explanation, class or homework exercise, or case study (again, we can help with formatting).\nSuggest additions to the website.\nDiscuss a post or teaching resource (see discussion box at bottom of all posts and teaching resources).\nTell others about the fishR website!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fishR",
    "section": "",
    "text": "fishR provides resources for fisheries-related analyses performed in R, an environment for statistical analyses and graphics that has gained wide popularity with fisheries scientists. We hope the resources in the navigation bar above will help you use or expand your knowledge of R for fisheries-related analyses."
  },
  {
    "objectID": "blog/posts/2024-2-5_LVB_brms/index.html",
    "href": "blog/posts/2024-2-5_LVB_brms/index.html",
    "title": "Bayesian LVB I - brms",
    "section": "",
    "text": "Introduction\nThe use of Bayesian inference in fisheries biology has been increasing. For good reason, as there are many benefits to taking a Bayesian approach. I won’t go into those reasons here but you can read about them in Dorazio (2016) and Doll and Jacquemin (2018). This post assumes you have already decided to use Bayesian methods and will present how to estimate parameters of the von Bertalanffy growth model. Previous posts describe frequentist methods here and here\nThere are many programming languages that can be used to fit models with Bayesian inference. In this post, I will use Stan with the brms package. The brms packages is a wrapper for Stan that makes fitting simple models easier than writing the full model code. In a second post, I will show how the same model is fit using rstan and writing the full Stan model code. Although the brms package will write the full Stan model for you.\nBoth methods will fit the typical three parameter von Bertalanffy growth model\n\\[\nTL_i=L_\\infty * (1-e^{(-\\kappa * (t_i-t_0))} )\n\\] where \\(TL_i\\) is total length of individual i, \\(L_\\infty\\) is the average maximum length obtained, \\(\\kappa\\) is the Brody growth coefficient, \\(t_i\\) is the age of individual i, and \\(t_0\\) is the theoretical age at zero length. To finish the model, an error term is added:\n\\[\nTL_i=L_\\infty * (1-e^{(-\\kappa * (t_i-t_0))} + \\epsilon_i)\n\\] \\[\n\\epsilon_i \\sim normal(0,\\sigma)\n\\] Where \\(\\epsilon_i\\) is a random error term for individual i with a mean of 0 and standard deviation \\(\\sigma\\).\n\n\nPrior probabilities\nAt the heart of Bayesian analysis is the prior probability distribution. This post will use non-informative prior probability distributions. When and how to use informative priors when fitting a von Bertalanffy growth model will be discussed in a future post. However, you can read about it in Doll and Jacquemin (2018). The prior probability distributions used in this post are:\n\n\n\nParameter\nPrior Probability Distribution\n\n\n\n\n\\(L_\\infty\\)\nnormal(0,1000)\n\n\n\\(\\kappa\\)\nnormal(0,10)\n\n\n\\(t_0\\)\nnormal(0,10)\n\n\n\\(\\sigma\\)\nstudent-t(3,0,30)\n\n\n\nStan parameterizes the normal distribution with the mean and standard deviation and the student-t distribution with the degrees of freedom, mean, and standard deviation\n\n\nPreliminaries\nFirst step is to load the necessary packages\n\nlibrary(FSA)\nlibrary(FSAdata)   # for data\nlibrary(dplyr)     # for filter(), select()\nlibrary(ggplot2)   # for plotting\nlibrary(brms)      # for fitting Stan models\nlibrary(tidybayes) # for plotting posterior results\nlibrary(bayesplot) # for plotting posterior predictive checks\n\n\n\nData\nThe WalleyeErie2 data available in the FSAdata package was used in previous posts demonstrating von Bertalanffy growth models and will once again be used here. These data are Lake Erie Walleye (Sander vitreus) captured during October-November, 2003-2014. As before, the primary interest here is in the tl (total length in mm) and age variables. The data will also be filtered to focus only on female Walleye from location “1” captured in 2014.\n\ndata(WalleyeErie2,package=\"FSAdata\")\nwf14T &lt;- WalleyeErie2 %&gt;%\n  filter(year==2014,sex==\"female\",loc==1) %&gt;%\n  select(-year,-sex,-setID,-loc,-grid)\nheadtail(wf14T)\n\n#R|       tl    w      mat age\n#R|  1   445  737 immature   2\n#R|  2   528 1571   mature   4\n#R|  3   380  506 immature   1\n#R|  323 488 1089 immature   2\n#R|  324 521 1408   mature   3\n#R|  325 565 1745   mature   3\n\n\n\n\nbrms\nThe first step is to specify the formula using brmsformula(). The first argument below is the von Bertalanffy growth model, the second argument is to specify any hierarchical model of parameters (for example, estimate parameters by group or random effects), and the third is to tell brms that I want to fit a nonlinear model. Note in this example I do not specify any random effects for parameters. Also note that tl and age MUST be exactly how they are shown in the column headers of the data.\n\n#Set formula\nformula &lt;- brmsformula(tl ~ Linf * (1 - exp(-K * (age - t0))),\n                       Linf ~ 1, K ~ 1, t0 ~ 1, nl=TRUE)\n\nThe next step is to specify the prior probability distribution. Several parameters can’t be negative values so they are truncated using the lb=0 argument.\n\npriors &lt;- prior(normal(0,1000), nlpar=\"Linf\", lb=0) +\n          prior(normal(0,10), nlpar=\"K\", lb=0) +\n          prior(normal( 0,10), nlpar=\"t0\") +\n          prior(student_t(3,0,40), class=sigma)\n\nAn optional step is to specify initial values for the parameters. It is good practice to always specify initial values, particularly with non-linear and other complex models. I will fit the model using multiple chains and it is advisable to use different starting values for each chain. To accomplish this, we will specify a function and use a random number generator for each parameter. Adjust the range for the uniform distribution to cover a large range of values that make sense for your data. You can use other distributions as long as they match the declared range in the model code. In this example, I am using the random uniform function because \\(L_\\infty\\) and \\(\\kappa\\) are restricted to be positive in the model. Therefore, the starting value must be positive.\n\ninitsLst &lt;- function() list(\n  Linf=runif(1, 200, 800),\n  K=runif(1, 0.05, 3.00),\n  t0=rnorm(1, 0, 0.5)\n)\n\nFinally, use the brm() function to send the model, data, priors, initial values, and any specified settings to Stan and save the output in the fit1 object\n\nfit1 &lt;- brm(formula,            # calls the formula object created above\n            family=gaussian(),  # specifies the error distribution\n            data=wf14T,         # the data object\n            prior=priors,       # the prior probability object \n            init=initsLst,      # the initial values object\n            chains=3,           # number of chains, typically 3 to 4\n            cores=3,            # number of cores for multi-core processing. Typically set to\n                                # match number of chains. Adjust as needed to match the number\n                                # of cores on your computer and number of chains\n            iter=3000,          # number of iterations\n            warmup=1000,        # number of warm up steps to discard\n            control=list(adapt_delta=0.80,    # Adjustments to algorithm to \n                           max_treedepth=15)) # improve convergence.\n\n\n\nAssess convergence\nBefore diving into the output, I like to examine the chains to see if they sufficiently mixed and reached a stationary posterior distribution.\n\nplot(fit1)\n\n\n\n\n\n\n\n\nThe chains appear to have mixed well for all parameters and reached a stationary posterior. This is seen by a unimodal distribution (on the left) and caterpillar plots for each parameter appear “on top” of each other (right).\nWe can also do a posterior predictive check to assess how well the model fit the data. A posterior predictive check compares observed data to predicted values based on the fitted model. If the model predicted values are “similar” to the observed data values then you can conclude the model fits the data well.\n\npp_check(fit1,ndraws=100)  # posterior predictive checks\n\n\n\n\n\n\n\n\nThis figure shows two lines, one represents the data (y) and the other represents posterior predicted values from the model (yrep). The yrep generally follows the observed data so we can conclude the model fits the data well.\n\n\nPosterior summary\nFinally, we can examine the summary table of the output.\n\nsummary(fit1)\n\n#R|   Family: gaussian \n#R|    Links: mu = identity; sigma = identity \n#R|  Formula: tl ~ Linf * (1 - exp(-K * (age - t0))) \n#R|           Linf ~ 1\n#R|           K ~ 1\n#R|           t0 ~ 1\n#R|     Data: wf14T (Number of observations: 325) \n#R|    Draws: 3 chains, each with iter = 3000; warmup = 1000; thin = 1;\n#R|           total post-warmup draws = 6000\n#R|  \n#R|  Population-Level Effects: \n#R|                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#R|  Linf_Intercept   648.33      9.98   629.46   668.04 1.00     1364     2104\n#R|  K_Intercept        0.36      0.02     0.32     0.40 1.00     1246     1743\n#R|  t0_Intercept      -1.28      0.09    -1.45    -1.12 1.00     1349     1839\n#R|  \n#R|  Family Specific Parameters: \n#R|        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#R|  sigma    18.35      0.73    16.98    19.86 1.00     2048     2379\n#R|  \n#R|  Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#R|  and Tail_ESS are effective sample size measures, and Rhat is the potential\n#R|  scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe summary table provides the point estimates, 95% credible intervals, Rhat values, and ESS. The Rhat values are another check to assess model convergence. It is generally accepted that you want Rhat values less than 1.10. Bulk_ESS and Tail_ESS are also used to assess convergence. Bulk_ESS and Tail_ESS refer to “Effective Sample Size.” Because of the nature of MCMC methods, each successive sample from the posterior will typically be autocorrelated within a chain. Autocorrelation within the chains can increase uncertainty in the estimates. One way to assessing how much autocorrelation is present and big of an effect it might be, is with the “Effective Sample Size.” ESS represents the number of independent draws from the posterior. The ESS will be lower than the actual number of draws and you are looking for a high ESS. It has been recommended that an ESS of 1,000 for each parameter is sufficient Bürkner (2017).\n\n\nPosterior plotting\nNow that we are convinced the model fit the data well, let’s plot the data and model predictions using the ggplot2 and tidybayes packages.\n\nwf14T %&gt;%\n  add_predicted_draws(fit1) %&gt;%  # adding the posterior distribution with tidybayes\n  ggplot(aes(x=age, y=tl)) +  \n  stat_lineribbon(aes(y=.prediction), .width=c(.95, .80, .50),  # regression line and CI\n                  alpha=0.5, colour=\"black\") +\n  geom_point(data=wf14T, colour=\"darkblue\", size=3) +   # raw data\n  scale_fill_brewer(palette=\"Greys\") +\n  ylab(\"Total length (mm)\\n\") + \n  xlab(\"\\nAge (years)\") +\n  theme_bw() +\n  theme(legend.title=element_blank(),\n        legend.position=c(0.15, 0.85))\n\n\n\n\n\n\n\n\nThe figure above shows the observed data in blue circles, the prediction line as a solid black line, and the posterior prediction intervals (0.50, 0.80, and 0.95) in different shades of gray.\n\n\n\n\n\nReferences\n\nBürkner, P.-C. 2017. Brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software 80:1–28.\n\n\nDoll, J. C., and S. J. Jacquemin. 2018. Introduction to Bayesian modeling and inference for fisheries scientists. Fisheries 43(3):152–161.\n\n\nDorazio, R. M. 2016. Bayesian data analysis in population ecology: Motivations, methods, and benefits. Population Ecology 58:31–44.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{doll2024,\n  author = {Doll, Jason},\n  title = {Bayesian {LVB} {I} - Brms},\n  date = {2024-02-05},\n  url = {https://fishr-core-team.github.io/fishR/blog/posts/2024-2-5_LVB_brms/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDoll, J. 2024, February 5. Bayesian LVB I - brms. https://fishr-core-team.github.io/fishR/blog/posts/2024-2-5_LVB_brms/."
  },
  {
    "objectID": "blog/posts/2023_2_13_Quistetal2022_AgeData/index.html",
    "href": "blog/posts/2023_2_13_Quistetal2022_AgeData/index.html",
    "title": "Quist et al. (2022) Age Comparison Figures",
    "section": "",
    "text": "Introduction\nQuist et al. (2022) examined three structures (scales, sectioned otoliths, and whole otoliths) to estimate age of Yellowstone Cutthroat Trout (Oncorhynchus clarkii bouvieri). Their published paper included a supplement with the data they used to estimate age precision and bias between readers and between structures.\nA previous post demonstrated my preferred methods for displaying such data. Here, as I continue to hone my ggplot2 skills, I did not follow my preferences but rather tried to recreate Figures 1 and 2 in Quist et al. (2022). My process is described below.\n \n\n\nGetting Setup\nThe following packages are loaded for use below.\n\nlibrary(tidyverse)  # for dplyr, ggplot2, stringr packages\nlibrary(patchwork)  # for positioning multiple plots\n\nThe following ggplot2 theme was used below.1\n1 See this post for more information on creating and using ggplot2 themes.\ntheme_q &lt;- function(base_size=14) {\n  theme_bw(base_size=base_size) +\n    theme(\n      # margin for the plot\n      plot.margin=unit(c(0.5,0.5,0.5,0.5),\"cm\"),\n      # set axis label (i.e., title) colors and margins\n      axis.title.y=element_text(colour=\"black\",margin=margin(t=0,r=10,b=0,l=0)),\n      axis.title.x=element_text(colour=\"black\",margin=margin(t=10,r=0,b=0,l=0)),\n      # set tick label color, margin, and position and orientation\n      axis.text.y=element_text(colour=\"black\",margin=margin(t=0,r=5,b=0,l=0),\n                               vjust=0.5,hjust=1),\n      axis.text.x=element_text(colour=\"black\",margin=margin(t=5,r=0,b=0,l=0),\n                               vjust=0,hjust=0.5,),\n      # set size of the tick marks for y- and x-axis\n      axis.ticks=element_line(linewidth=0.5),\n      # adjust length of the tick marks\n      axis.ticks.length=unit(0.2,\"cm\"),\n      # set the axis size,color,and end shape\n      axis.line=element_line(colour=\"black\",linewidth=0.5,lineend=\"square\"),\n      # adjust size of text for legend\n      legend.text=element_text(size=12),\n      # remove grid\n      panel.grid=element_blank()\n    )\n}\n\n \n\n\nData\nI downloaded the Excel file provided as a supplement to the published paper to my local drive and named it “quistetal_2022.xlsx”. The authors used “.” for missing data which I addressed with na= in read_excel() below. The authors also recorded “confidence” values for each age assignment but those were not used in any analysis here so I removed them. Names for the remaining variables were long and somewhat inconsistent so I renamed those to be shorter and consistent.2\n2 Note that as.data.frame() is used to remove the tibble class returned from read_excel() that I don’t prefer.\ndf &lt;- readxl::read_excel(\"quist_etal_2022.xlsx\",na=c(\"\",\".\")) |&gt;\n  select(-contains(\"Confidence\")) |&gt;\n  rename(scale_r1=`Reader_1_Scale_age`,\n         scale_r2=`Reader_2_Scale_age`,\n         scale_age=`Consensus_scale_age`,\n         soto_r1=`Reader_1_Sectioned_Otolith_age`,\n         soto_r2=`Reader_2_Sectioned_Otolith_age`,\n         soto_age=`Consensus_Sectioned_Otolith_Age`,\n         woto_r1=`Reader_1_Whole_Otolith_age`,\n         woto_r2=`Reader_2_Whole_Otolith_age`,\n         woto_age=`Consensus_Whole_Otolith`) |&gt;\n  as.data.frame()\nhead(df)\n\n#R|    Length Year scale_r1 scale_r2 scale_age soto_r1 soto_r2 soto_age woto_r1\n#R|  1    168 2019        1        1         1       1       1        1      NA\n#R|  2    169 2019        1        1         1       1       1        1       1\n#R|  3    172 2019        1        2         1       2       1        1      NA\n#R|  4    175 2020        1        1         1       2       2        2       2\n#R|  5    178 2019        1        1         1       1       1        1       1\n#R|  6    216 2020        2        2         2       2       2        2      NA\n#R|    woto_r2 woto_age\n#R|  1      NA       NA\n#R|  2       1        1\n#R|  3      NA       NA\n#R|  4       1        1\n#R|  5       1        1\n#R|  6      NA       NA\n\n\nFigures 1 and 2 in Quist et al. (2022) each used axis labels that ranged from 0 to 12 and were only shown for even values. Those limits and labels are set here for ease and consistency below.\n\naxlbls &lt;- seq(0,12,2)\naxlmts &lt;- range(axlbls)\n\n \n\n\nPlot One Structure\nI begin by showing how to create one panel (i.e., for estimate age between readers for one structure) of Figure 1.\nI first created a summary data frame that has the number of observations for each combination of age estimates by each reader. For example, there were three instances where each reader assigned an age of 1 and two instances where reader 1 assigned an age of 1 but reader 2 assigned and age of 2.\n\ndf_wo &lt;- df |&gt;\n  group_by(woto_r1,woto_r2) |&gt;\n  summarize(freq=n()) |&gt;\n  as.data.frame()\nhead(df_wo)\n\n#R|    woto_r1 woto_r2 freq\n#R|  1       1       1    3\n#R|  2       1       2    2\n#R|  3       2       1    2\n#R|  4       2       2  114\n#R|  5       2       3   28\n#R|  6       2       4    1\n\n\nagePrecision() from FSA was then used to compute a variety of age precision statistics between the two readers. For these purposes, note that exact percent agreement is returned in $PercAgree and the average coefficient of variation is returned in $ACV of the object saved from agePrecision().\n\nap_wo &lt;- FSA::agePrecision(woto_r2~woto_r1,data=df)\nstr(ap_wo)\n\n#R|  List of 14\n#R|   $ detail   :'data.frame': 416 obs. of  12 variables:\n#R|    ..$ woto_r2: num [1:416] NA 1 NA 1 1 NA 2 2 1 2 ...\n#R|    ..$ woto_r1: num [1:416] NA 1 NA 2 1 NA 2 2 1 2 ...\n#R|    ..$ mean   : num [1:416] NA 1 NA 1.5 1 NA 2 2 1 2 ...\n#R|    ..$ median : num [1:416] NA 1 NA 1.5 1 NA 2 2 1 2 ...\n#R|    ..$ mode   : num [1:416] NA 1 NA NA 1 NA 2 2 1 2 ...\n#R|    ..$ SD     : num [1:416] NA 0 NA 0.707 0 ...\n#R|    ..$ CV     : num [1:416] NA 0 NA 47.1 0 ...\n#R|    ..$ CV2    : num [1:416] NA 0 NA 47.1 0 ...\n#R|    ..$ AD     : num [1:416] NA 0 NA 0.5 0 NA 0 0 0 0 ...\n#R|    ..$ PE     : num [1:416] NA 0 NA 33.3 0 ...\n#R|    ..$ PE2    : num [1:416] NA 0 NA 33.3 0 ...\n#R|    ..$ D      : num [1:416] NA 0 NA 33.3 0 ...\n#R|   $ rawdiff  : 'table' int [1:7(1d)] 2 36 248 82 6 0 1\n#R|    ..- attr(*, \"dimnames\")=List of 1\n#R|    .. ..$ : chr [1:7] \"-2\" \"-1\" \"0\" \"1\" ...\n#R|   $ absdiff  : 'table' int [1:5(1d)] 248 118 8 0 1\n#R|    ..- attr(*, \"dimnames\")=List of 1\n#R|    .. ..$ : chr [1:5] \"0\" \"1\" \"2\" \"3\" ...\n#R|   $ ASD      : num 0.26\n#R|   $ ACV      : num 7.81\n#R|   $ ACV2     : num 7.81\n#R|   $ AAD      : num 0.184\n#R|   $ APE      : num 5.52\n#R|   $ APE2     : num 5.52\n#R|   $ AD       : num 5.52\n#R|   $ PercAgree: num 66.1\n#R|   $ R        : int 2\n#R|   $ n        : int 416\n#R|   $ validn   : int 375\n#R|   - attr(*, \"class\")= chr \"agePrec\"\n\n\nQuist et al. (2022) also reported “PA-1” (i.e., the percent agreement within one age), which is not returned by agePrecision(). This value can, however, be calculated by summing the first two values in $absDiff (these are the counts of perfect agreement and agreement within one age) and dividing by the total number of paired assignments in $validn.\n\nsum(ap_wo$absdiff[1:2])/ap_wo$validn*100\n\n#R|  [1] 97.6\n\n\nQuist et al. (2022) annotated their plots with these three summary values. Below I use str_glue() to concatenate together these values into a single string that will be added to the plots below. In the use of str_glue() below the three summary measures are assigned to objects named PA, PA1, and CV (last three lines) and those values are then inserted into the string (first four lines) within the {} delimiters. I also used formatC() is used to format the string to one decimal place and that \\n is a “carriage return” that starts a new line in the string.\n\nlbl_wo &lt;- str_glue(\"Whole Otoliths\\n\",\n                   \"PA = {PA}%\\n\",\n                   \"PA-1 = {PA1}%\\n\",\n                   \"CV = {CV}\",\n                   PA=formatC(ap_wo$PercAgree,format='f',digits=1),\n                   PA1=formatC(sum(ap_wo$absdiff[1:2])/ap_wo$validn*100,format='f',digits=1),\n                   CV=formatC(ap_wo$ACV,format='f',digits=1))\nlbl_wo\n\n#R|  Whole Otoliths\n#R|  PA = 66.1%\n#R|  PA-1 = 97.6%\n#R|  CV = 7.8\n\n\nFinally, the plot is made by first3 adding the 1:1 reference line with geom_a line(), the frequency of age estimates with geom_label(), and the summary results as text with annotate(). The arguments in geom_label() remove the rounded edges of the boxes (i.e., label.r) and attempt to make the boxes larger to more closely abut as in Quist et al. (2022). The arguments in annotate() put the label at the minimum x and maximum y values, then vertically and horizontally adjust that text box so that the upper-left corner is just inside the upper-left corner of the plot. Finally, the expansion factors are removed from both axes and both axes are limited to and labeled with the limits and labels set above.\n3 So that it appears behind the numbers.\nrs_wo &lt;- ggplot(data=df_wo,mapping=aes(x=woto_r1,y=woto_r2,label=freq)) +\n  geom_abline(slope=1,intercept=0) +\n  geom_label(label.r=unit(0,\"lines\"),label.padding=unit(0.3,\"lines\")) +\n  annotate(geom=\"text\",label=lbl_wo,\n           x=0,y=12,hjust=-0.1,vjust=1.1,size=4) +\n  scale_x_continuous(name=\"Reader 1\",expand=expansion(mult=0),\n                     limits=axlmts,breaks=axlbls) +\n  scale_y_continuous(name=\"Reader 2\",expand=expansion(mult=0),\n                     limits=axlmts,breaks=axlbls) +\n  theme_q()\nrs_wo\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNote\n\n\n\n\nI did not find a good way to make the boxes around each value the same size as in Quist et al. (2022).\nAlso, I think that graphs with the same rnge of values on both axes should be presented as a square, rather than as the rectangle used by Quist et al. (2022).\n\n\n\n \n\n\nRecreating Figure 1\nThe code described above can largely be repeated for scales and sectioned otoliths, which I did without showing the code but saving the results in ggplot2 objects called rs_sc and rs_so.\n\n\n\n\n\n\nHowever, when these two objects are combined together with rs_wo to form Figure 1, they should not have a title or labels for the x-axis. The x-axis title and labels are removed by setting axis.title.x= and axis.text.x= in theme() to element_blank() and appending this to each object.4\n4 Note that each object is modified and then saved to the same name here.\nrs_so &lt;- rs_so +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank())\nrs_sc &lt;- rs_sc +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank())\n\nFinally, the three plots can be stacked on top of each other using patchwork as shown below.5\n5 Use smaller values in plot.margin= in theme_q to more closely resemble how close the panels are in Quist et al. (2022).\nrs_sc + rs_so + rs_wo +\n  plot_layout(ncol=1)\n\n\n\n\n\n\n\n\n \n\n\nRecreating Figure 2\nFigure 2 would follow the same general process as Figure 1 except that the summary results annotation would only have the results and not a “title” and each panel in the figure would have the x-axis title and labels.\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nQuist, M. C., D. K. McCarrick, and L. M. Harris. 2022. Comparison of structures used to estimate age and growth of Yellowstone Cutthroat Trout. Journal of Fish and Wildlife Management 13(2):544–551.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Quist Et Al. (2022) {Age} {Comparison} {Figures}},\n  date = {2023-02-13},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023_2_13_Quistetal2022_AgeData},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, February 13. Quist et al. (2022) Age Comparison\nFigures. https://fishr-core-team.github.io/fishR//blog/posts/2023_2_13_Quistetal2022_AgeData."
  },
  {
    "objectID": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html",
    "href": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html",
    "title": "Vasquez et al. (2022) Kuskal-Wallis Results",
    "section": "",
    "text": "Vasquez et al. (2022) examined the nesting materials of Black-Crested Titmice (Baeolophus atricristatus) near San Marcos, Texas. In one part of their analysis they used a Kruskal-Wallis test to identify differences in total nest weight for nests from four different area types (based on a rural to urban gradient). They summarized their results in Panel A of their Figure 2. Here I recreate their statistics, but summarize the results differently.\nThe following packages are loaded for use below. A few functions from each of FSA, car, DescTools, and scales are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages"
  },
  {
    "objectID": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#assumption-checking",
    "href": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#assumption-checking",
    "title": "Vasquez et al. (2022) Kuskal-Wallis Results",
    "section": "Assumption Checking",
    "text": "Assumption Checking\nVasquez et al. (2022) chose to use a Kruskal-Wallis test1 to determine if nest weights differed among area types because of concerns over normality and equal variances. They tested normality with a Shapiro-Wilk test, but this is difficult to do given the small sample sizes, especially in the “Parks” and “Urban” areas. Instead of the test I examined a density plot of total nest weight for each area type.\n1 I assume the reader is familiar with a Kruskal-Wallis Test. If not, this is one resource that may be useful.\nggplot(data=dat2,mapping=aes(x=Nest.Weight)) +\n  geom_density(linewidth=0.75) +\n  geom_rug(linewidth=0.75) +\n  scale_x_continuous(name=\"Nest Weight (g)\") +\n  scale_y_continuous(expand=expansion(mult=c(0,0.05))) +\n  facet_wrap(vars(Location3)) +\n  theme_bw() +\n  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank())\n\n\n\n\n\n\n\n\nAlternatively, Q-Q plots2 may be used to examine normality.\n2 I assume the reader is familiar with a Q_Q plot. If not, this is one resource that may be useful.\nggplot(data=dat2,mapping=aes(sample=Nest.Weight)) +\n  geom_qq() +\n  geom_qq_line(linewidth=0.75) +\n  scale_x_continuous(name=\"Nest Weight (g)\") +\n  scale_y_continuous(expand=expansion(mult=c(0,0.05))) +\n  facet_wrap(vars(Location3)) +\n  theme_bw() +\n  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank())\n\n\n\n\n\n\n\n\nWith either plot, the data do not appear to follow a normal distribution but, given the sample size, they do not look terribly non-normal either.\nVasquez et al. (2022) examined homogeneity of variances with the Bartlett test. I prefer to use Levene’s test for this purpose as it does not assume a normal distribution. Both bartlett.test() and leveneTest() (from car) require a formula of the form response~explanatory3 as the first argument and the corresponding data frame in data=.\n3 Or dependent~independent.\nbartlett.test(Nest.Weight~Location3,data=dat2)\n\n#R|  \n#R|     Bartlett test of homogeneity of variances\n#R|  \n#R|  data:  Nest.Weight by Location3\n#R|  Bartlett's K-squared = 10.009, df = 3, p-value = 0.01849\n\ncar::leveneTest(Nest.Weight~Location3,data=dat2)\n\n#R|  Levene's Test for Homogeneity of Variance (center = median)\n#R|        Df F value Pr(&gt;F)\n#R|  group  3  2.2121 0.1012\n#R|        41\n\n\nUnfortunately, the two results give different answers, with Bartlett’s test suggesting unequal variances and Levene’s test suggesting the opposite.\nGiven these results it may have been appropriate to analyze these data with a one-way ANOVA. I suspect, however, that other variables in the data set more egregiously violated these assumptions and the authors wanted to use consistent methods for all variables. And, of course, using the Kruskal-Wallis test here a priori would not be incorrect, though there is some loss of power."
  },
  {
    "objectID": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#kruskal-wallis-test-1",
    "href": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#kruskal-wallis-test-1",
    "title": "Vasquez et al. (2022) Kuskal-Wallis Results",
    "section": "Kruskal-Wallis Test",
    "text": "Kruskal-Wallis Test\nThe Kruskal-Wallis test may be performed with kruskal.test() using the same arguments as bartlett.test() or leveneTest() above.\n\nkruskal.test(Nest.Weight~Location3,data=dat2)\n\n#R|  \n#R|     Kruskal-Wallis rank sum test\n#R|  \n#R|  data:  Nest.Weight by Location3\n#R|  Kruskal-Wallis chi-squared = 11.571, df = 3, p-value = 0.009006\n\n\nThis result provides strong evidence that the median total nest weight is different for at least one of the area types."
  },
  {
    "objectID": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#post-hoc-multiple-comparisons",
    "href": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#post-hoc-multiple-comparisons",
    "title": "Vasquez et al. (2022) Kuskal-Wallis Results",
    "section": "Post-hoc Multiple Comparisons",
    "text": "Post-hoc Multiple Comparisons\nDunn’s test4 may be used, after a significant Kruskal-Wallis test, to determine which pairs of medians differ after adjusting for multiple comparisons. Dunn’s test is performed with dunnTest() from FSA using the same arguments as given to kruskal.test() above. As noted in the results below, dunnTest() defaults to using the “Holm” method to adjust for multiple comparisons. Other methods may be used with method=.5\n4 I assume that the reader is familiar with Dunn’s Test. If not this is one resources that may be helpful.5 See the possible methods with ?dunn.test::dunn.test.\nFSA::dunnTest(Nest.Weight~Location3,data=dat2)\n\n#R|             Comparison        Z     P.unadj      P.adj\n#R|  1 Parks - Residential 0.233670 0.815241172 0.81524117\n#R|  2       Parks - Rural 1.007831 0.313535451 0.62707090\n#R|  3 Residential - Rural 1.042160 0.297337581 0.89201274\n#R|  4       Parks - Urban 2.751550 0.005931397 0.02965699\n#R|  5 Residential - Urban 3.126111 0.001771347 0.01062808\n#R|  6       Rural - Urban 2.542487 0.011006664 0.04402666\n\n\nFrom these results it is apparent that the median nest weight in the “Urban” areas differs from the other three area types, which all have statistically equal medians."
  },
  {
    "objectID": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#create-summary-data-frame",
    "href": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#create-summary-data-frame",
    "title": "Vasquez et al. (2022) Kuskal-Wallis Results",
    "section": "Create Summary Data Frame",
    "text": "Create Summary Data Frame\nThe Kruskal-Wallis test is a test of the equality of medians across groups. Thus, in my opinion, a summary graphic for this test should plot the medians with their respective confidence intervals. Computing the confidence intervals for medians across groups takes a bit of work in R. This process is described below.\nThe original data frame is converted to a list with separate vectors of nest weights for each area type.\n\ntmp &lt;- split(dat2$Nest.Weight,dat2$Location3)\ntmp\n\n#R|  $Rural\n#R|   [1] 45.7 52.6 32.2 61.5 93.9 22.2 54.2 40.9 63.1 47.1 19.2 52.2 39.1 82.8 50.8\n#R|  [16] 25.6 58.5 38.8 23.8 85.4 23.8\n#R|  \n#R|  $Parks\n#R|  [1] 38.7 54.2 87.9 55.1 45.7\n#R|  \n#R|  $Residential\n#R|   [1] 44.1 69.1 37.1 47.0 70.4 51.7 79.2 53.8 44.4 65.8 44.7 43.7\n#R|  \n#R|  $Urban\n#R|  [1] 27.5 31.8 35.4 35.6 18.5 32.2 27.5\n\n\nMedianCI() from DescTools may be used to compute the median and corresponding confidence interval for a vector of values. Below is an example for the “Urban” area. The default is to use a so-called “exact” method, but here I prefer to use “bootstrapping” instead so I include method=\"boot\".\n\nDescTools::MedianCI(tmp$Urban,method=\"boot\")\n\n#R|  median lwr.ci upr.ci \n#R|    31.8   28.2   36.1\n\n\nsapply() can be used to “apply” MedianCI() to each item in the tmp list (so each area type) and return the result as a matrix.\n\ntmp &lt;- sapply(tmp,FUN=DescTools::MedianCI,method=\"boot\")\ntmp\n\n#R|         Rural Parks Residential Urban\n#R|  median  47.1  54.2       49.35  31.8\n#R|  lwr.ci  40.0  20.5       31.25  28.2\n#R|  upr.ci  55.4  69.7       54.45  36.1\n\n\nHowever, the “median”, “lwr.ci”, and “upper.ci” should be the columns rather than the rows of this matrix. Thus, t() is used below to “transpose” the matrix which is then given to data.frame() to convert the matrix to a data frame.\n\nsumNW &lt;- t(tmp) |&gt;\n  data.frame()\nsumNW\n\n#R|              median lwr.ci upr.ci\n#R|  Rural        47.10  40.00  55.40\n#R|  Parks        54.20  20.50  69.70\n#R|  Residential  49.35  31.25  54.45\n#R|  Urban        31.80  28.20  36.10\n\n\nNow, unfortunately, the area type names are row names and not a variable. Below, these row names are used to create a new Location3 variable and then converted to a factor with the levels controlled to match the authors’ order along a rural to urban gradient.\n\nsumNW &lt;- sumNW |&gt;\n  mutate(Location3=rownames(sumNW),\n         Location3=factor(Location3,levels=c(\"Rural\",\"Parks\",\"Residential\",\"Urban\")))\nsumNW\n\n#R|              median lwr.ci upr.ci   Location3\n#R|  Rural        47.10  40.00  55.40       Rural\n#R|  Parks        54.20  20.50  69.70       Parks\n#R|  Residential  49.35  31.25  54.45 Residential\n#R|  Urban        31.80  28.20  36.10       Urban\n\n\nFinally, a new variable is added that contains “significance letters” that communicate the results from Dunn’s Test above. Groups that have the same letter are statistically equal and those with different letters are statistically not equal. In this case, “Urban” was different than the other three which were all the same. Thus, “Urban” will have a unique letter that is different then the same letter used for the other three. I usually start the letters on the left, so “Rural” gets an “a” which it will share with “Parks” and “Residential” before “Urban” gets a “b.”\n\nsumNW &lt;- sumNW |&gt;\n  mutate(letsH=c(\"a\",\"a\",\"a\",\"b\"))\nsumNW\n\n#R|              median lwr.ci upr.ci   Location3 letsH\n#R|  Rural        47.10  40.00  55.40       Rural     a\n#R|  Parks        54.20  20.50  69.70       Parks     a\n#R|  Residential  49.35  31.25  54.45 Residential     a\n#R|  Urban        31.80  28.20  36.10       Urban     b\n\n\nThis data frame is the used to construct the summary graphic below."
  },
  {
    "objectID": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#summary-graphic",
    "href": "blog/posts/2023-4-5_Vasquez2022_Fig2/index.html#summary-graphic",
    "title": "Vasquez et al. (2022) Kuskal-Wallis Results",
    "section": "Summary Graphic",
    "text": "Summary Graphic\nI created the summary graphic below by …\n\nMapping Location3 to the x-axis for all geoms.\nMapping lwr.ci and upr.ci to ymin= and ymax=, respectively, in geom_errobar() to create the “intervals.” Here linewidth= was increased to make the interval lines “heavier”6 and width= was reduced to make the “caps” at the end of the interval lines narrower.\nMapping median to y= in geom_point() to place a point at the median. geom_point() came afer geom_errorbar() so that the point would be “on top” of the interval line. Here an open circle was used for the point with an inner portion a light gray and a black outline. The point was made larger than the default, as was the “stroke” used to draw the outline.7\nMapping letsH to labels= and upr.ci= to y in geom_text() to place the significance letters at the upper CI value. The vjust= was used to move the text up slightly8 and size= was used to show the letters in a 12 pt font.9\nThe y-axis was labeled, the breaks were controlled at 10 g intervals, the lower limit was constrained to be at 0, and the lower expansion was removed so that the x-axis was shown at a y of 0.\nThe x-axis was labeled.\nThe black-and-white theme was applied.\nThe axis titles were set a 14 pt font, and the axis tick mark labels were set to be black and in a 12 pt font.\n\n6 The use of linewidth= was discussed in more detail in this post.7 The use of size= and stroke= was discussed in more detail in this post.8 The use of vjust= was discussed in more detail in this post.9 The use of size= was discussed in more detail in this post.\nggplot(data=sumNW,mapping=aes(x=Location3)) +\n  geom_errorbar(mapping=aes(ymin=lwr.ci,ymax=upr.ci),\n                linewidth=0.75,width=0.1) +\n  geom_point(mapping=aes(y=median),\n             shape=21,size=2.5,stroke=1,\n             fill=\"gray70\",color=\"black\") +\n  geom_text(mapping=aes(label=letsH,y=upr.ci),vjust=-0.5,size=12/.pt) +\n  scale_y_continuous(name=\"Median Nest Weight (g)\",\n                     breaks=scales::breaks_width(10),\n                     limits=c(0,NA),expand=expansion(mult=c(0,0.05))) +\n  scale_x_discrete(name=\"Area Type\") +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=14),\n        axis.text=element_text(size=12,color=\"black\"))"
  },
  {
    "objectID": "blog/posts/2023-4-24_Multiple_CatchCurves/index.html",
    "href": "blog/posts/2023-4-24_Multiple_CatchCurves/index.html",
    "title": "Working with Multiple Catch Curves",
    "section": "",
    "text": "Introduction\nIn a previous post a helper function was created and some of the map family of functions from purr were used to efficiently create and apply an age-length key (ALK) to assign estimated ages to unaged fish in a sample. That post will be extended here by using functions from purr and FSA to efficiently compute mortality estimates from catch curves for multiple groups of fish.\nThe following packages are loaded for use below.\n\nlibrary(tidyverse)  # for dplyr, tidyr, purr packages\nlibrary(FSA)        # for catchCurve functionality\n\nThe random number seed was set to ensure repeatability for the random components of alkIndivAge() below.\n\nset.seed(14354454)\n\n \n\n\nInitial Wrangling & Catch-at-Age Summary\nThis post begins with the final data frame from the previous post of lengths and ages for all sampled Channel Catfish (Ictalurus punctatus) and Walleye (Sander vitreus) data. A portion of the resultant data frame is shown below, but the code is folded to save space.1 Note here that the final data frame is called dat.2\n1 Please see the previous post for a thorough description of this process.2 Rather than dat3 as in the previous post.\n\nCode\n## Computes and applies an ALK\n##   data: The data frame with, at least, the age & length variables\n##   avar: The name (without quotes) of the age variable in data\n##   lvar: The name (without quotes) of the length variable in data\n##   w: The width of length categories/bins for use in the ALK\n## Returns the data data frame with ages in avar assigned from the ALK for\n##   unaged fish and a new length category (lcat) variable derived from w\n\napplyALK &lt;- function(data,avar,lvar,w) {\n  ## Get avar variable name as character for non-tidyverse functions below\n  avarn &lt;- deparse(substitute(avar))\n  ## Add length category variable\n  data &lt;- data |&gt; dplyr::mutate(lcat=FSA::lencat({{lvar}},w=w))\n  ## Separate into aged and unaged dataframes\n  aged &lt;- data |&gt; dplyr::filter(!is.na({{avar}}))\n  unaged &lt;- data |&gt; dplyr::filter(is.na({{avar}}))\n  ## Make ALK (find frequencies, convert to row proportions)\n  ALK &lt;- prop.table(xtabs(as.formula(paste0(\"~lcat+\",avarn)),data=aged),margin=1)\n  ## Apply ALK according to Isermann-Knight method\n  tmp &lt;- FSA::alkIndivAge(ALK,as.formula(paste0(avarn,\"~lcat\")),data=unaged)\n  ## Put aged and newly assigned age data frames together to return\n  dplyr::bind_rows(aged,tmp)\n}\n\n## Wrangle the data\ndat &lt;- read.csv(\"../2023-4-23_Multiple_ALKs/JFWM-20-027.S1.csv\") |&gt;\n  select(-Weight,-Sex,-BCAge,-BCLength,-Year) |&gt;\n  mutate(Season=case_when(\n    Month==\"May\" ~ \"Spring\",\n    Month==\"September\" ~ \"Fall\"\n  )) |&gt;\n  filter(!is.na(Length)) |&gt;\n  filter(!(Spp==\"CCF\" & Season==\"Spring\" & Length&lt;279)) |&gt;\n  filter(!(Spp==\"CCF\" & Season==\"Fall\" & Length&lt;200)) |&gt;\n  split(~Spp+Season) |&gt;\n  map_df(applyALK,avar=Age,lvar=Length,w=10) |&gt;\n  select(Spp,Season,Age,Length,lcat)\n\n\n\nheadtail(dat)\n\n#R|       Spp Season Age Length lcat\n#R|  1    CCF   Fall   1    232  230\n#R|  2    CCF   Fall   1    216  210\n#R|  3    CCF   Fall   1    238  230\n#R|  2355 WAE Spring  10    632  630\n#R|  2356 WAE Spring  10    653  650\n#R|  2357 WAE Spring  13    720  720\n\n\nThese data are summarized to provide total catches at each OBSERVED age for each species and season combination.\n\nCatAge &lt;- dat |&gt;\n  group_by(Spp,Season,Age) |&gt;\n  summarize(Catch=n()) |&gt;\n  ungroup()\nCatAge\n\n#R|  # A tibble: 53 × 4\n#R|     Spp   Season   Age Catch\n#R|     &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;\n#R|   1 CCF   Fall       1    15\n#R|   2 CCF   Fall       2    22\n#R|   3 CCF   Fall       3     3\n#R|   4 CCF   Fall       4    16\n#R|   5 CCF   Fall       5    29\n#R|   6 CCF   Fall       6    23\n#R|   7 CCF   Fall       7    20\n#R|   8 CCF   Fall       8    29\n#R|   9 CCF   Fall       9    22\n#R|  10 CCF   Fall      10    12\n#R|  # … with 43 more rows\n\n\nA quick summary of the age range and number of observed ages for each species and season combination indicates that some ages within the age range were not observed.\n\nCatAge |&gt;\n  group_by(Spp,Season) |&gt;\n  summarize(agerng=max(Age)-min(Age)+1,\n            agesobs=n())\n\n#R|  # A tibble: 4 × 4\n#R|  # Groups:   Spp [2]\n#R|    Spp   Season agerng agesobs\n#R|    &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;int&gt;\n#R|  1 CCF   Fall       17      14\n#R|  2 CCF   Spring     16      14\n#R|  3 WAE   Fall       11      10\n#R|  4 WAE   Spring     18      15\n\n\nFor example, the age range for Channel Catfish in the Fall was from 1 to 17, but no age 12, 13, or 16 fish were caught.\n\nCatAge |&gt; filter(Spp==\"CCF\",Season==\"Fall\")\n\n#R|  # A tibble: 14 × 4\n#R|     Spp   Season   Age Catch\n#R|     &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;\n#R|   1 CCF   Fall       1    15\n#R|   2 CCF   Fall       2    22\n#R|   3 CCF   Fall       3     3\n#R|   4 CCF   Fall       4    16\n#R|   5 CCF   Fall       5    29\n#R|   6 CCF   Fall       6    23\n#R|   7 CCF   Fall       7    20\n#R|   8 CCF   Fall       8    29\n#R|   9 CCF   Fall       9    22\n#R|  10 CCF   Fall      10    12\n#R|  11 CCF   Fall      11     2\n#R|  12 CCF   Fall      14     1\n#R|  13 CCF   Fall      15     2\n#R|  14 CCF   Fall      17     3\n\n\nSchall et al. (2020) assumed a catch of 0 for ages that were not observed within the range of observed ages for each species and season. Thus, the first “issue” encountered with these data was how to insert 0s into the data frames for these “unobserved” ages.\n\n\nHandling Zero Catches\n\nA Helper Function\nA helper function, add0CatchByAge(), was created to facilitate the adding of zero catches for the appropriate ages. This function looks complicated because I chose to allow the age and catch variables to be given without quotes. However, the general algorithm in this function is to use full_seq() from tidyr to create a sequence of sequential ages over the range of observed ages, then use left_join() from dplyr to join the original data frame to the sequence of ages, and use remove_na() from tidyr to replace missing “catch” data with zero and missing non-age and non-catch data with values from the first row of the original data frame. It is important to note that this function will only work if the non-age and non-catch variables are constant across rows.\n\n## Adds a row with catch=0 for unobserved ages within the range of observed ages\n##   data: The data frame with, at least, the age & catch variables\n##   avar: The name (without quotes) of the age variable in data\n##   cvar: The name (without quotes) of the catch variable in data\n##   fill: The value to use for the catch of unobserved ages (defaults to 0)\n## Returns the data data frame with previoulsy unobserved ages in avar, the fill\n##   value for those ages in cvar, and values in other variables repeated for\n##   those ages\n\nadd0CatchByAge &lt;- function(data,avar,cvar,fill=0L) {\n  ## Get variable names as characters for non-tidyverse functions\n  avarn &lt;- deparse(substitute(avar))\n  cvarn &lt;- deparse(substitute(cvar))\n  ## Get names of variables in data\n  dnms &lt;- names(data) # all\n  dnms2 &lt;- dnms[!dnms %in% c(avarn,cvarn)]\n  ## Create sequence of ages that cover full range of ages\n  tmpages &lt;- data.frame(tidyr::full_seq(as.data.frame(data)[,avarn],1))\n  names(tmpages) &lt;- avarn\n  ## Create fill list for unobserved ages\n  fills &lt;- c(as.list(data[1,dnms2]),fill)\n  names(fills) &lt;- c(dnms2,cvarn)  \n  ## Add zeroes to ages not observed\n  tmp &lt;- dplyr::left_join(tmpages,data,by=avarn) |&gt;\n    tidyr::replace_na(fills)\n  ## Return data frame with columns ordered as in original data\n  tmp[,dnms]\n}\n\nadd0CatchByAge() takes a data frame (with at least age and catch variables) as the first argument, unquoted names for “age” and “catch” variables in avar= and cvar=, and the value to fill in for the catch of unobserved ages in fill= (defaults to 0). The example below shows how zeroes are filled in for the unobserved ages of Channel Catfish captured in the Fall.\n\ntmpex &lt;- CatAge |&gt; filter(Spp==\"CCF\",Season==\"Fall\") ## temporary data frame for example\nadd0CatchByAge(tmpex,avar=Age,cvar=Catch)            ## example result\n\n#R|     Spp Season Age Catch\n#R|  1  CCF   Fall   1    15\n#R|  2  CCF   Fall   2    22\n#R|  3  CCF   Fall   3     3\n#R|  4  CCF   Fall   4    16\n#R|  5  CCF   Fall   5    29\n#R|  6  CCF   Fall   6    23\n#R|  7  CCF   Fall   7    20\n#R|  8  CCF   Fall   8    29\n#R|  9  CCF   Fall   9    22\n#R|  10 CCF   Fall  10    12\n#R|  11 CCF   Fall  11     2\n#R|  12 CCF   Fall  12     0\n#R|  13 CCF   Fall  13     0\n#R|  14 CCF   Fall  14     1\n#R|  15 CCF   Fall  15     2\n#R|  16 CCF   Fall  16     0\n#R|  17 CCF   Fall  17     3\n\n\n\n\nAdding the Zeroes\nAdding the zeroes must be done on a per-group basis, which is by species-season combination here. Thus, the CatAge summary data frame is split() by Spp and Season into a list of four data frames.3 This list of data frames is then given to map_df() which applies add0CatchByAge() to each of the four data frames using Age for avar= and Catch for cvar=. The four data frames are then row-bound together to return one overall data frame.\n3 The process of using split() was defined more thoroughly in this post.\nCatAge &lt;- CatAge |&gt;\n  split(~Spp+Season) |&gt;\n  map_df(add0CatchByAge,avar=Age,cvar=Catch)\nheadtail(CatAge)\n\n#R|     Spp Season Age Catch\n#R|  1  CCF   Fall   1    15\n#R|  2  CCF   Fall   2    22\n#R|  3  CCF   Fall   3     3\n#R|  60 WAE Spring  16     2\n#R|  61 WAE Spring  17     0\n#R|  62 WAE Spring  18     2\n\n\nThe result is a single data frame with the same variables as the original CatAge, but with new rows of Catch=0 for unobserved Ages. This is best seen by examining the portion of the new CatAge summary data frame for Channel Catfish captured in the Fall.\n\nCatAge |&gt; filter(Spp==\"CCF\",Season==\"Fall\")\n\n#R|     Spp Season Age Catch\n#R|  1  CCF   Fall   1    15\n#R|  2  CCF   Fall   2    22\n#R|  3  CCF   Fall   3     3\n#R|  4  CCF   Fall   4    16\n#R|  5  CCF   Fall   5    29\n#R|  6  CCF   Fall   6    23\n#R|  7  CCF   Fall   7    20\n#R|  8  CCF   Fall   8    29\n#R|  9  CCF   Fall   9    22\n#R|  10 CCF   Fall  10    12\n#R|  11 CCF   Fall  11     2\n#R|  12 CCF   Fall  12     0\n#R|  13 CCF   Fall  13     0\n#R|  14 CCF   Fall  14     1\n#R|  15 CCF   Fall  15     2\n#R|  16 CCF   Fall  16     0\n#R|  17 CCF   Fall  17     3\n\n\nThis data frame is now ready for catch curve analysis.\n \n\n\n\nCatch Curve Analysis for Multiple Groups\n\nData Preparation\nA catch curve plots log catch versus age, though in this case, because of the presence of zero catches for some ages, log catch plus 1 was used.4 Below a new variable called Catch1 is added to CatAge that contains the catch plus 1 value and the log of this is taken to create logCatch1. Catch1 is needed because catchCurve() from FSA requires the raw “catch” data. logCatch1 is needed for plotting.\n4 In this previous post I question whether it is appropriate to add 1 to the catch data for this purpose.\nCatAge &lt;- CatAge |&gt;\n  mutate(Catch1=Catch+1,\n         logCatch1=log(Catch1))\nheadtail(CatAge)\n\n#R|     Spp Season Age Catch Catch1 logCatch1\n#R|  1  CCF   Fall   1    15     16  2.772589\n#R|  2  CCF   Fall   2    22     23  3.135494\n#R|  3  CCF   Fall   3     3      4  1.386294\n#R|  60 WAE Spring  16     2      3  1.098612\n#R|  61 WAE Spring  17     0      1  0.000000\n#R|  62 WAE Spring  18     2      3  1.098612\n\n\nIn general, instantaneous mortality (\\(Z\\)) is only estimated from the slope of the “descending limb” of the catch curve. The plot below is used to identify the “descending limb”, though I ultimately used the ages defined by Schall et al. (2020):\n\nChannel Catfish, Fall – ages 5 and older\nChannel Catfish, Spring – ages 6 and older\nWalleye, Fall – ages 1 and older\nWalleye, Spring – ages 2 and older\n\n\nggplot(dat=CatAge,mapping=aes(x=Age,y=logCatch1,color=Season)) +\n  geom_point(size=2) +\n  geom_line(alpha=0.1,linewidth=1.5) +\n  facet_grid(rows=vars(Spp),cols=vars(Season)) +\n  theme_bw() + theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nFrom this, a new summary data frame (i.e., CatAgeReduced) is created that is the same as CatAge but with all ages not on the descending limb removed.\n\nCatAgeReduced &lt;- CatAge |&gt;\n  filter(!(Spp==\"CCF\" & Season==\"Fall\" & Age&lt;5)) |&gt;\n  filter(!(Spp==\"CCF\" & Season==\"Spring\" & Age&lt;6)) |&gt;\n  filter(!(Spp==\"WAE\" & Season==\"Fall\" & Age&lt;1)) |&gt;\n  filter(!(Spp==\"WAE\" & Season==\"Spring\" & Age&lt;2))\n\nThe data were plotted with this new data frame to ensure that the descending limbs were retained as defined in Schall et al. (2020).\n\n\nCode\nggplot(dat=CatAgeReduced,mapping=aes(x=Age,y=logCatch1,color=Season)) +\n  geom_point(size=2) +\n  geom_line(alpha=0.1,linewidth=1.5) +\n  facet_grid(rows=vars(Spp),cols=vars(Season)) +\n  theme_bw() + theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nFinally, the overall number of fish for each species and season was computed to compare to results in Schall et al. (2020). The number of Channel Catfish retained here was the same for Fall, but one fewer for Spring than in Schall et al. (2020). The number of Walleye retained here was one more for Fall and eight more for Spring than in Schall et al. (2020). Thus, my mortality calculations in the next section will likely differ slightly from those published in Schall et al. (2020).\n\nCatAgeReduced |&gt;\n  group_by(Spp,Season) |&gt;\n  summarize(n=sum(Catch)) |&gt;\n  ungroup() |&gt;\n  mutate(Schall_n=c(143,423,1217,293))\n\n#R|  # A tibble: 4 × 4\n#R|    Spp   Season     n Schall_n\n#R|    &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;    &lt;dbl&gt;\n#R|  1 CCF   Fall     143      143\n#R|  2 CCF   Spring   422      423\n#R|  3 WAE   Fall    1218     1217\n#R|  4 WAE   Spring   301      293\n\n\n\n\nCatch Curve Results\nThe catch curve calculations for one data frame can be performed efficiently with catchCurve() from FSA. Thus, to use this function for multiple groups of data, the reduced catch-at-age data frame must be split() into a list of separate data frames by Spp and Season.\n\nCatAgeRedSplit &lt;- CatAgeReduced |&gt;\n  split(~Spp+Season)\n\nA helper function was then created that performs the catch curve analysis with catchCurve() and extracts the instantaneous and total (\\(A\\)) mortality rate estimates, along with their 95% confidence intervals. This function takes the data frame (with at least the catch and age data) as its first argument and a formula of the form cvar~avar. Additional named arguments to catchCurve(), for example weighted=TRUE, can also be included.5\n5 See ?catchCurve for more information on catchCurve().\n## Performs catch curve analysis, extracts A & Z estimates and CIs\n##   data: The data frame with, at least, the age & catch variables\n##   formula: A formula of the form `cvar~avar` where cvar is the catch variable\n##            in data and avar is the age variable in data\n##   ...: Additional arguments for FSA::catchCurve()\n## Returns a named vector with the A & Z estimates and CIs\n\ngetCCresults &lt;- function(data,formula,...) {\n  tmp &lt;- FSA::catchCurve(formula,data,...)\n  res &lt;- c(coef(tmp),confint(tmp,parm=\"Z\"),confint(tmp,parm=\"A\"))\n  names(res) &lt;- c(\"Z\",\"A\",\"Z.LCI\",\"Z.UCI\",\"A.LCI\",\"A.UCI\")\n  res[c(1,3,4,2,5,6)]\n}\n\nAs an example, this function is used below to fit a weighted catch curve to Channel Catfish captured in the Spring. Note the use of Catch1.\n\ngetCCresults(CatAgeRedSplit$CCF.Spring,formula=Catch1~Age,weighted=TRUE)  ## example\n\n#R|           Z      Z.LCI      Z.UCI          A      A.LCI      A.UCI \n#R|   0.3466275  0.1303087  0.5629463 29.2931353 12.2175614 43.0471425\n\n\nSimilar to before, getCCresults() is applied to each data frame in CatAgeRedSplit with map_df() to return a data frame of catch curve analysis results by group. .id is used here in map_df() to create a variable in the returned data frame that contains the names of “items” from CatAgeRedSplit. This is required because getCCresults() does not return any identifier for which group the result is for.\n\nccresults &lt;- CatAgeRedSplit |&gt;\n  map_df(getCCresults,formula=Catch1~Age,weighted=TRUE,.id=\"Group\")\nccresults\n\n#R|  # A tibble: 4 × 7\n#R|    Group          Z  Z.LCI Z.UCI     A A.LCI A.UCI\n#R|    &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#R|  1 CCF.Fall   0.359 0.221  0.496  30.2 19.9   39.1\n#R|  2 WAE.Fall   0.741 0.509  0.973  52.3 39.9   62.2\n#R|  3 CCF.Spring 0.347 0.130  0.563  29.3 12.2   43.0\n#R|  4 WAE.Spring 0.229 0.0906 0.367  20.5  8.67  30.7\n\n\nThe point and interval estimates for \\(A\\) are reasonably close to those published in Schall et al. (2020) given the inherent randomization in applying the ALK and the minor differences in sample sizes here versus what they reported.\nWhile it is not needed in this post, it will be common to need to “split” the Group variable in ccresults back into its constituent parts (i.e., Spp and Season). As the constituent parts in Group are consistently separated with a ., this can be easily accomplished with separate_wider_delim() from tidyr as shown below.\n\nccresults &lt;- ccresults |&gt;\n  tidyr::separate_wider_delim(Group,\".\",names=c(\"Spp\",\"Season\"))\nccresults\n\n#R|  # A tibble: 4 × 8\n#R|    Spp   Season     Z  Z.LCI Z.UCI     A A.LCI A.UCI\n#R|    &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#R|  1 CCF   Fall   0.359 0.221  0.496  30.2 19.9   39.1\n#R|  2 WAE   Fall   0.741 0.509  0.973  52.3 39.9   62.2\n#R|  3 CCF   Spring 0.347 0.130  0.563  29.3 12.2   43.0\n#R|  4 WAE   Spring 0.229 0.0906 0.367  20.5  8.67  30.7\n\n\n\n\nCatch Curve Figure\nSchall et al. (2020) displayed their catch curve results in their Figure 3. The biggest challenge to recreating this figure is that their regression lines are from the weighted regression on the descending limb. The weights used are stored in the result from catchCurve(), but again this functions needs to be applied on a per-group basis. As such, a helper function was created below that performs the catch curve analysis as above, but instead of extracting results it extracts the weights used, appends them to the data frame given to the function, and returns the modified data frame. Thus, this function serves to append the weights used to the original data used.\n\n## Performs catch curve analysis and adds estimated weights to data\n##   data: The data frame with, at least, the age & catch variables\n##   formula: A formula of the form `cvar~avar` where cvar is the catch variable\n##            in data and avar is the age variable in data\n##   ...: Additional arguments for FSA::catchCurve()\n## Returns the data data frame with the catch curve weights in wts variable\n\naddCCweights &lt;- function(data,formula,...) {\n  tmp &lt;- FSA::catchCurve(formula,data,...)\n  data &lt;- data |&gt; dplyr::mutate(wts=tmp$w)\n  data\n}\n\nThis function is then applied to each data frame in CatAgeRedSplit with map_df() to produce a modified data frame with the weights used.\n\nCatAgeRedWts&lt;- CatAgeRedSplit |&gt;\n  map_df(addCCweights,formula=Catch1~Age,weighted=TRUE)\nheadtail(CatAgeRedWts)\n\n#R|     Spp Season Age Catch Catch1 logCatch1       wts\n#R|  1  CCF   Fall   5    29     30  3.401197 3.5075254\n#R|  2  CCF   Fall   6    23     24  3.178054 3.2178363\n#R|  3  CCF   Fall   7    20     21  3.044522 2.9281472\n#R|  51 WAE Spring  16     2      3  1.098612 0.6867394\n#R|  52 WAE Spring  17     0      1  0.000000 0.4800235\n#R|  53 WAE Spring  18     2      3  1.098612 0.2733075\n\n\nThe main part of reproducing Figure 3 consists of creating three “layers” of data:\n\nPlot logCatch1 versus Age from CatAge with an open circle (i.e., pch=21) with different outline colors based on Season. This will produce an open circle for the log catch at all ages across the range of observed ages.\nOver plot logCatch versus Age from CatAgeRedWts with a circle (i.e., pch=21) with different outline and fill colors based on Season. This will “fill” the open circles for ages on the descending limb of the catch curve.\nOver plot the weighted regression line with colors based on Season and using weight=s given in wts, but which will be separated by season.6\nCreate separate facets by Spp.\n\n6 This use of the weight= aes()thetic was described in this post.\nggplot() +\n  geom_point(dat=CatAge,mapping=aes(x=Age,y=logCatch1,color=Season),\n             pch=21,size=2) +\n  geom_point(dat=CatAgeRedWts,mapping=aes(x=Age,y=logCatch1,color=Season,fill=Season),\n             pch=21,size=2) +\n  geom_smooth(dat=CatAgeRedWts,mapping=aes(x=Age,y=logCatch1,\n                                           color=Season,fill=Season,weight=wts),\n              method=lm) +\n  scale_x_continuous(name=\"Age (yrs)\",expand=expansion(mult=0.02)) +\n  scale_y_continuous(name=\"log (Catch+1)\",expand=expansion(mult=0.02)) +\n  scale_color_manual(values=c(\"Fall\"=\"black\",\"Spring\"=\"gray60\"),\n                     aesthetics=c(\"color\",\"fill\")) +\n  facet_wrap(vars(Spp),labeller=labeller(Spp=c(\"CCF\"=\"Channel Catfish\",\n                                               \"WAE\"=\"Walleye\"))) +\n  theme_bw() +\n  theme(panel.grid.major=element_blank(),\n        legend.position=c(1,1),\n        legend.justification=c(1.1,1.1),\n        legend.title=element_blank(),\n        legend.background=element_blank(),\n        strip.text=element_text(face=\"bold\"))\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nReferences\n\nSchall, B. J., C. W. Schoenebeck, and K. D. Koupal. 2020. Seasonal sampling influence on population dynamics and yield of Channel Catfish and Walleye in a large Great Plains reservoir. Journal of Fish and Wildlife Management 12(1):223–233.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Working with {Multiple} {Catch} {Curves}},\n  date = {2023-04-24},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-4-24_Multiple_CatchCurves},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, April 24. Working with Multiple Catch Curves. https://fishr-core-team.github.io/fishR//blog/posts/2023-4-24_Multiple_CatchCurves."
  },
  {
    "objectID": "blog/posts/2023-4-1_Milleretal2022_Fig4/index.html",
    "href": "blog/posts/2023-4-1_Milleretal2022_Fig4/index.html",
    "title": "Miller et al. (2022) Catch Curve Plot",
    "section": "",
    "text": "Series Note\n\n\n\nThis is the third and last post related to Miller et al. (2022). I do not have plans to recreate their Figure 5 as it looks like they largely followed this post.\n\n\n\nIntroduction\nMiller et al. (2022) examined life history characteristics of Goldeye (Hiodon alosoides) in two Kansas reservoirs. Their Figure 4 represents a catch curve (log catch at age) for Goldeye captured in Milford Reservoir in 2020. I use FSA and ggplot2 here to recreate this figure.\nThe following packages are loaded for use below. A few functions from each of readxl and scales are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\nlibrary(ggtext)     # for use of markdown in text/labels\nlibrary(FSA)        # for catchCurve et al.\n\n \n\n\nData Wrangling\nMiller et al. (2022) provided the raw data for producing Figure 4 in their Data Supplement S2. These are the same data used to recreate Figures 2 and 3 in this post. Thus, I do not explain the data wrangling used to obtain dat2 below.\n\ndat2 &lt;- read.csv(\"../2023-3-31_Milleretal2022_Fig23/JFWM-21-090.S2.csv\") |&gt;\n  filter(ann==1)\nhead(dat2)\n\n#R|    netid  tl   w agecap ann bclen\n#R|  1     6 385 521      6   1   210\n#R|  2     6 357 466      4   1   213\n#R|  3     6 397 725      5   1   209\n#R|  4     8 393 610      8   1   202\n#R|  5     8 373 571      4   1   193\n#R|  6     8 389 656      6   1   163\n\n\nThe total catch of individuals at each age-at-capture is required to construct Figure 4. One issue that occurs with these data is that no age-2 fish were captured and Miller et al. (2022) treat this as an observed zero. If the data are simply grouped by agecap and summarized then no age-2 data will be present in the result. There are several ways to deal with this but I am going to handle it by first creating a new variable fagecap that is a factored version of agecap. The key here, though, is to set the levels of this new variable to cover the entire range of observed ages.\n\ndat2 &lt;- dat2 |&gt;\n  mutate(fagecap=factor(agecap,levels=1:8))\n\nThe “sample size” (i.e., catch) at each fagecap is then found. It is important to use .drop=FALSE so that the age-2 “level” is not removed from the resultant data frame because it did not exist in the original data frame.\n\nsum_a &lt;- dat2 |&gt;\n  group_by(fagecap,.drop=FALSE) |&gt;\n  summarize(catch=n())\nsum_a\n\n#R|  # A tibble: 8 × 2\n#R|    fagecap catch\n#R|    &lt;fct&gt;   &lt;int&gt;\n#R|  1 1         122\n#R|  2 2           0\n#R|  3 3           5\n#R|  4 4           7\n#R|  5 5          10\n#R|  6 6           3\n#R|  7 7           2\n#R|  8 8           3\n\n\nThe issue now with using these data is that fcapage is a factor rather than a numeric. Thus, a new agecap variable is created below that treats age as numeric.1\n1 This conversion code comes from ?factor.\nsum_a &lt;- sum_a |&gt;\n  mutate(agecap=as.numeric(levels(fagecap)))\nsum_a\n\n#R|  # A tibble: 8 × 3\n#R|    fagecap catch agecap\n#R|    &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;\n#R|  1 1         122      1\n#R|  2 2           0      2\n#R|  3 3           5      3\n#R|  4 4           7      4\n#R|  5 5          10      5\n#R|  6 6           3      6\n#R|  7 7           2      7\n#R|  8 8           3      8\n\n\nFinally, the catch curve analysis ultimately requires log-transforming the catch variable. The zero for age-2 will cause an error when log-transforming. The authors addressed this by adding 1 to all of the catches. This is common practice, but I comment on it further below. Also, for use when recreating Figure 4, the log of these modified catches are added to the data frame.\n\nsum_a &lt;- sum_a |&gt;\n  mutate(catch1=catch+1,\n         logcatch1=log(catch1))\nsum_a\n\n#R|  # A tibble: 8 × 5\n#R|    fagecap catch agecap catch1 logcatch1\n#R|    &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n#R|  1 1         122      1    123      4.81\n#R|  2 2           0      2      1      0   \n#R|  3 3           5      3      6      1.79\n#R|  4 4           7      4      8      2.08\n#R|  5 5          10      5     11      2.40\n#R|  6 6           3      6      4      1.39\n#R|  7 7           2      7      3      1.10\n#R|  8 8           3      8      4      1.39\n\n\n\n\nCatch Curve Analysis\nCatch curves analysis2 may be conducted with catchCurve() from FSA. catchCurve require a formula of the form catch~age as the first argument and the corresponding data frame in data=. The “weighted catch curve” that Miller et al. (2022) used requires weighted=TRUE in catchCurve(). The results should be saved to an object.\n2 I assume the reader is familiar with catch curves. If not see Chapter 11 in Ogle (2016).\ncc1 &lt;- catchCurve(catch1~agecap,data=sum_a,weighted=TRUE)\n\nThe point estimates of \\(Z\\) and \\(A\\) may be extracted with coef() or seen in the Estimate column of the summary() results.\n\ncoef(cc1)\n\n#R|           Z          A \n#R|   0.2709483 23.7344113\n\nsummary(cc1)\n\n#R|      Estimate Std. Error  t value  Pr(&gt;|t|)\n#R|  Z  0.2709483  0.2544583 1.064805 0.3279271\n#R|  A 23.7344113         NA       NA        NA\n\n\nThe so-called “recruitment coefficient of determination” (RCD) in Miller et al. (2022) is just the usual r2, which can be extracted from the summary() of the lm object in cc1.\n\n( r2 &lt;- summary(cc1$lm)$r.squared )\n\n#R|  [1] 0.1589346\n\n\nThese results were put into a label that will ultimately be placed on the plot.3\n3 This process is discussed in several previous posts, including this one.\nlbl &lt;- paste0(\"*Z* = \",round(coef(cc1)[[\"Z\"]],2),\n              \"&lt;br&gt;*A* = \",round(coef(cc1)[[\"A\"]],2),\"%\",\n              \"&lt;br&gt;RCD = \",round(r2,2))\nlbl\n\n#R|  [1] \"*Z* = 0.27&lt;br&gt;*A* = 23.73%&lt;br&gt;RCD = 0.16\"\n\n\nThe cc1 object also contains the data used in the catch curve analysis, including the weights in an object called weights.e. These weights are added to the sum_a data frame as they are needed to produce Figure 4.4\n4 The weights can be produced “manually” as described in Ogle (2016), but this process using catchCurve() is more efficient and less prone to error.\nsum_a &lt;- sum_a |&gt;\n  mutate(wts=cc1$weights.e)\nsum_a\n\n#R|  # A tibble: 8 × 6\n#R|    fagecap catch agecap catch1 logcatch1   wts\n#R|    &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#R|  1 1         122      1    123      4.81  2.68\n#R|  2 2           0      2      1      0     2.45\n#R|  3 3           5      3      6      1.79  2.22\n#R|  4 4           7      4      8      2.08  1.98\n#R|  5 5          10      5     11      2.40  1.75\n#R|  6 6           3      6      4      1.39  1.52\n#R|  7 7           2      7      3      1.10  1.29\n#R|  8 8           3      8      4      1.39  1.06\n\n\nThe data in sum_a are now ready to recreate Figure 4.\n \n\n\nRecreating Figure 4\nFigure 4 is simply a scatterplot with a regression line overlaid. Constructing these kinds of plots was discussed in detail in this post. Thus, most of the code below are not discussed in depth. A very important detail, though, is that mapping weight=wts will provide the wts to lm() in geom_smooth() as weights, so that the weighted regression is used in the same way as was done in catch_curve().5\n5 In my opinion, the y-axis title should clearly note that 1 was added to the catches. Thus, my y-axis title is different than that in Miller et al. (2022).\nggplot(data=sum_a,mapping=aes(x=agecap,y=logcatch1,weight=wts)) +\n  geom_smooth(method=\"lm\",se=FALSE,color=\"black\") +\n  geom_point() +\n  scale_x_continuous(name=\"Estimated age\",\n                     limits=c(0,8),breaks=scales::breaks_width(1),\n                     expand=expansion(mult=c(0,0.02))) +\n  scale_y_continuous(name=\"log(catch+1)\",\n                     limits=c(0,5),breaks=scales::breaks_width(1),\n                     expand=expansion(mult=c(0.01,0))) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\")) +\n  annotate(geom=\"richtext\",x=Inf,y=Inf,vjust=1,hjust=1,label=lbl,\n           label.color=NA,fontface=\"bold\")\n\n\n\n\n\n\n\n\n\nPossible Modifications\nAs with other posts related to Miller et al. (2022), a confidence band can be added to this regression.6\n6 Note use of coord_cartesian() as the confidence band extends outside the desired range of the y-axis.\nggplot(data=sum_a,mapping=aes(x=agecap,y=logcatch1,weight=wts)) +\n  geom_smooth(method=\"lm\",color=\"black\") +\n  geom_point() +\n  scale_x_continuous(name=\"Estimated age\",\n                     limits=c(0,8),breaks=scales::breaks_width(1),\n                     expand=expansion(mult=c(0,0.02))) +\n  scale_y_continuous(name=\"log(catch+1)\",\n                     breaks=scales::breaks_width(1),\n                     expand=expansion(mult=c(0.01,0))) +\n  coord_cartesian(ylim=c(0,5)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\")) +\n  annotate(geom=\"richtext\",x=Inf,y=Inf,vjust=1,hjust=1,label=lbl,\n           label.color=NA,fontface=\"bold\")\n\n\n\n\n\n\n\n\nHere, I think the confidence band is critical to include because it demonstrates that the regression line is NOT statistically declining.7 Obviously, there is mortality in this population, so this indicates data issues.\n7 This was also apparent by the “large” p-value for the slope in the summary(cc1) results above.A major assumption of catch curve analysis is that recruitment is constant, which it clearly is not here. The most glaring evidence of this is the complete lack of age-2 fish. I don’t know if the age-2 fish should be considered as an observed zero (i.e., that age-class is actually very low) or if there is a sampling issue (i.e., was the gear highly selective; here is some evidence for that in the length frequency histogram in their Figure 2 and in this post.)\nIt is common to add 1 to “integer” data that has zeroes prior to log-transformation. However, this can be problematic. For example, adding 1 to the catch of age-1 fish is relatively minor (&lt;1% change from 122 to 123), but the same modification is relatively important for the catch of age-7 fish (50% increase from 2 to 3). This can influence mortality estimates. For example, below one was added only to the age-2 catch.8\n8 I am not suggesting that this is the correct thing to do. However, the authors would not have added a 1 if one age-2 fish had been captured. Thus, this will demonstrate some level of sensitivity to the data collection and analysis choice.\nsum_a &lt;- sum_a |&gt;\n  mutate(catch1a=ifelse(agecap==2,1,catch))\nsum_a\n\n#R|  # A tibble: 8 × 7\n#R|    fagecap catch agecap catch1 logcatch1   wts catch1a\n#R|    &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#R|  1 1         122      1    123      4.81  2.68     122\n#R|  2 2           0      2      1      0     2.45       1\n#R|  3 3           5      3      6      1.79  2.22       5\n#R|  4 4           7      4      8      2.08  1.98       7\n#R|  5 5          10      5     11      2.40  1.75      10\n#R|  6 6           3      6      4      1.39  1.52       3\n#R|  7 7           2      7      3      1.10  1.29       2\n#R|  8 8           3      8      4      1.39  1.06       3\n\n\nAnd the catch curve analysis was conducted with these data instead.\n\ncc1a &lt;- FSA::catchCurve(catch1a~agecap,data=sum_a,weighted=TRUE)\nsummary(cc1a)\n\n#R|      Estimate Std. Error  t value  Pr(&gt;|t|)\n#R|  Z  0.3400406  0.2737122 1.242329 0.2604674\n#R|  A 28.8258572         NA       NA        NA\n\n\nThe issue of no statistical decline is still evident, but the point estimates of A, for example, has increased substantially.\nIgnoring the age-2 “fish” completely has an even larger effect.\n\ncc1b &lt;- FSA::catchCurve(catch~agecap,data=sum_a,ages2use=-2,weighted=TRUE)\nsummary(cc1b)\n\n#R|      Estimate Std. Error t value  Pr(&gt;|t|)\n#R|  Z  0.6105809  0.1688334 3.61647 0.0152761\n#R|  A 45.6964676         NA      NA        NA\n\n\nI am not sure what is the best way to handle this problem, but this exercise suggests to me that the published mortality estimate should be considered cautiously.\n \n\n\n\n\n\n\n\n\n\n\nReferences\n\nMiller, B. T., E. Flores, D. S. Waters, and B. C. Neely. 2022. An evaluation of Goldeye life history characteristics in two Kansas reservoirs. Journal of Fish and Wildlife Management 13(1):243–249.\n\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Miller Et Al. (2022) {Catch} {Curve} {Plot}},\n  date = {2023-04-01},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-4-1_Milleretal2022_Fig4},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, April 1. Miller et al. (2022) Catch Curve Plot. https://fishr-core-team.github.io/fishR//blog/posts/2023-4-1_Milleretal2022_Fig4."
  },
  {
    "objectID": "blog/posts/2023-3-9-Clemens2022/index.html",
    "href": "blog/posts/2023-3-9-Clemens2022/index.html",
    "title": "Clemens (2022) Temperature Figure",
    "section": "",
    "text": "Clemens (2022) examined temperature as a mortality threat to Pacific Lamprey (Entosphenus tridentatus). While their Figure 3 is quite simple, it appeared to me that some aspects of the figure were manually drawn rather than being drawn from the data.1 I don’t mean this as a critique because (a) I have done this before and (b) what I illustrate here will not affect the narrative that can be derived from the figure. However, I do want to demonstrate how easy it is with ggplot2 to tie what was manually drawn directly to the data.\n1 This is my interpretation from the vertical dashed lines not being directly aligned with the points or the x-axis."
  },
  {
    "objectID": "blog/posts/2023-3-9-Clemens2022/index.html#shaded-region",
    "href": "blog/posts/2023-3-9-Clemens2022/index.html#shaded-region",
    "title": "Clemens (2022) Temperature Figure",
    "section": "Shaded Region",
    "text": "Shaded Region\nThe two July mortality dates are a range in which the mortality was thought to have happened. I thought it might be instructive to highlight the area between those two dates with geom_polygon().\ngeom_polyon() requires a data frame that contains the points (in order) for each “node” of the polygon. In this case, the data frame needs each observed point in the “5-Jul-2021” to “8-Jul-2021” range and the two points on the x-axis at those two dates. The filter() below extracts the rows from dat that are greater than or equal to “5-Jul-2021” and less than or equal to “8-Jul-2021”, which are stored in the second and third positions of morts created above. The bind_rows() line binds on the rows of a data frame that contains those two dates from morts, but in reverse order so that the points in the data frame are in order around the perimeter of the polygon, and -Inf for both temp values, indicating the points along the x-axis as above.\n\nmdat2 &lt;- dat |&gt;\n  filter(date&gt;=morts[2],date&lt;=morts[3]) |&gt;\n  bind_rows(data.frame(date=morts[3:2],\n                       temp=c(-Inf,-Inf)))\nmdat2\n\n#R|          date temp\n#R|  1 2021-07-05 27.6\n#R|  2 2021-07-06 27.3\n#R|  3 2021-07-07 26.4\n#R|  4 2021-07-08 26.3\n#R|  5 2021-07-08 -Inf\n#R|  6 2021-07-05 -Inf\n\n\ngeom_polygon() with this data frame and using a fairly light gray color is then added to the previous plot, but before the other geom_s so that the shaded polygon sits behind the lines and points.\n\nggplot() +\n  geom_polygon(data=mdat2,mapping=aes(x=date,y=temp),\n               fill=\"gray90\") +\n  geom_line(data=dat,mapping=aes(x=date,y=temp),\n            linewidth=1) +\n  geom_point(data=dat,mapping=aes(x=date,y=temp),\n             size=2) +\n  geom_segment(data=mdat,mapping=aes(x=date,y=temp,xend=date,yend=yend),\n               linetype=\"dashed\",linewidth=1,color=\"gray30\",\n               arrow=arrow(type=\"closed\",length=unit(0.1,\"inches\"))) +\n  scale_y_continuous(name=\"Mean daily temperature (degrees Celsius)\",\n                     expand=expansion(mult=0),\n                     limits=c(18,32),breaks=seq(18,32,1)) +\n  scale_x_date(name=\"Date\",expand=expansion(add=2),\n               breaks=breaks_width(\"2 days\",offset=\"-3 days\"),\n               labels=label_date(\"%d-%b\"))"
  },
  {
    "objectID": "blog/posts/2023-3-9-Clemens2022/index.html#degree-symbol",
    "href": "blog/posts/2023-3-9-Clemens2022/index.html#degree-symbol",
    "title": "Clemens (2022) Temperature Figure",
    "section": "Degree Symbol",
    "text": "Degree Symbol\nThe y-axis label is a bit verbose, with “degrees” and “Celsius” both written out, rather than using “oC”. For some simple symbols, like the degree symbol, you can use a special “unicode”. For example, including \\u00b0 in the name= argument to scale_y_continuous() will produce a degree symbol. With this, I also reduced “Celsius” to “C”.\n\nggplot() +\n  geom_polygon(data=mdat2,mapping=aes(x=date,y=temp),\n               fill=\"gray90\") +\n  geom_line(data=dat,mapping=aes(x=date,y=temp),\n            linewidth=1) +\n  geom_point(data=dat,mapping=aes(x=date,y=temp),\n             size=2) +\n  geom_segment(data=mdat,mapping=aes(x=date,y=temp,xend=date,yend=yend),\n               linetype=\"dashed\",linewidth=1,color=\"gray30\",\n               arrow=arrow(type=\"closed\",length=unit(0.1,\"inches\"))) +\n  scale_y_continuous(name=\"Mean daily temperature (\\u00b0C)\",\n                     expand=expansion(mult=0),\n                     limits=c(18,32),breaks=seq(18,32,1)) +\n  scale_x_date(name=\"Date\",expand=expansion(add=2),\n               breaks=breaks_width(\"2 days\",offset=\"-3 days\"),\n               labels=label_date(\"%d-%b\"))"
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "",
    "text": "Landry et al. (2022) examined the diets of Bobcats (Lynx rufus) in West Virginia. They used logistic regression analyses in two parts of their analyses and presented those findings in their Figure 2 and Figure 3. In a previous post I demonstrated how to produce similar plots using geom_smooth() from ggplot2. Here I want to show an alternative method that is more laborious, but I am starting to prefer as it (a) seems to always work, (b) generalizes more easily, and (c) is not a “black box.”"
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#fitting-the-logistic-regression",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#fitting-the-logistic-regression",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Fitting the Logistic Regression",
    "text": "Fitting the Logistic Regression\nA logistic regression is computed in R with glm() using a formula of the form response~explanatory as the first argument, the relevant data frame in data=, and family=\"binomial\" to force using the logit transformation and, thus, the fitting of a logistic regression. The response variable can either be coded as 0s and 1s, as it is here, or as a factor where the first level is the “failure.” The logistic regression examining the occurrence of Virginia Opossum in the diet of Bobcats relative to the KFIi index is fit below.\n\nglmOpo &lt;- glm(Opossum~KFI_i,data=dat,family=\"binomial\")\n\nThe results of the logistic regression are obtained from giving the object saved from glm() to summary(). These results are the same as those presented in Landry et al. (2022).1\n1 See sentence directly above their Figure 2.\nsummary(glmOpo)\n\n#R|  \n#R|  Call:\n#R|  glm(formula = Opossum ~ KFI_i, family = \"binomial\", data = dat)\n#R|  \n#R|  Deviance Residuals: \n#R|      Min       1Q   Median       3Q      Max  \n#R|  -1.2694  -0.6694  -0.5652  -0.4397   2.2644  \n#R|  \n#R|  Coefficients:\n#R|              Estimate Std. Error z value Pr(&gt;|z|)    \n#R|  (Intercept) -3.11771    0.52556  -5.932 2.99e-09 ***\n#R|  KFI_i        0.05002    0.01463   3.418  0.00063 ***\n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#R|  \n#R|  (Dispersion parameter for binomial family taken to be 1)\n#R|  \n#R|      Null deviance: 288.81  on 299  degrees of freedom\n#R|  Residual deviance: 276.67  on 298  degrees of freedom\n#R|  AIC: 280.67\n#R|  \n#R|  Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#making-a-data-frame-of-predicted-probabilities",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#making-a-data-frame-of-predicted-probabilities",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Making a Data Frame of Predicted Probabilities",
    "text": "Making a Data Frame of Predicted Probabilities\nThe first step in recreating Figure 2 is to create a data frame of predicted probabilities, with 95% confidence intervals, for the occurrence of Virginia Opossums in the diet across the range of observed KFIi values. I begin this process by creating a data frame that has a KFI_i variable2 with a sequence of 199 values3 from the minimum to maximum observed KFIi values.\n2 This variable name must be exactly as it was in the data frame used in glm() above.3 The larger this number, the smoother the resultant curve in the figure will be.\nKFI_i_df &lt;- data.frame(KFI_i=seq(min(dat$KFI_i),max(dat$KFI_i),length.out=199))\nhead(KFI_i_df)\n\n#R|        KFI_i\n#R|  1  9.560078\n#R|  2  9.848166\n#R|  3 10.136254\n#R|  4 10.424343\n#R|  5 10.712431\n#R|  6 11.000519\n\n\nPredicted values may be obtained with predict(), but for logistic regression interval=\"confidence\" is not supported, so corresponding confidence interval values are not automatically computed. However, predict() can be used to make predictions, with standard errors, on the logit-transformed scale, which can then be used to “manually” calculate confidence intervals. The predictions on the logit-transformed scale and the corresponding standard errors are returned by including type=\"link\" and se.fit=TRUE. predict() returns a list by default, but I forced it into a data frame below for easier manipulation further below.4\n4 For our purposes, ignore the residual.scale column.\npredOpo &lt;- predict(glmOpo,KFI_i_df,type=\"link\",se.fit=TRUE) |&gt;\n  as.data.frame()\nhead(predOpo)\n\n#R|         fit    se.fit residual.scale\n#R|  1 -2.63952 0.3936826              1\n#R|  2 -2.62511 0.3897950              1\n#R|  3 -2.61070 0.3859143              1\n#R|  4 -2.59629 0.3820407              1\n#R|  5 -2.58188 0.3781745              1\n#R|  6 -2.56747 0.3743157              1\n\n\nIt is important to note here that the fit values in this data frame are on the logit-transformed scale. These values can be back-transformed to predicted probabilities using the inverse_logit() function created above.\n\npredOpo &lt;- predOpo |&gt;\n  mutate(predProb=inverse_logit(fit))\nhead(predOpo)\n\n#R|         fit    se.fit residual.scale   predProb\n#R|  1 -2.63952 0.3936826              1 0.06663790\n#R|  2 -2.62511 0.3897950              1 0.06753978\n#R|  3 -2.61070 0.3859143              1 0.06845296\n#R|  4 -2.59629 0.3820407              1 0.06937757\n#R|  5 -2.58188 0.3781745              1 0.07031373\n#R|  6 -2.56747 0.3743157              1 0.07126156\n\n\nApproximate 95% confidence intervals for the logit-transformed predictions can be made by adding and subtracting 1.96 times the standard error from each predicted value.5 These values are then back-transformed to construct confidence intervals on the probability scale.\n5 1.96 comes from normal distribution theory.\npredOpo &lt;- predOpo |&gt;\n  mutate(predLCI=inverse_logit(fit-1.96*se.fit),\n         predUCI=inverse_logit(fit+1.96*se.fit))\nhead(predOpo)\n\n#R|         fit    se.fit residual.scale   predProb    predLCI   predUCI\n#R|  1 -2.63952 0.3936826              1 0.06663790 0.03194919 0.1337847\n#R|  2 -2.62511 0.3897950              1 0.06753978 0.03263760 0.1345735\n#R|  3 -2.61070 0.3859143              1 0.06845296 0.03333990 0.1353679\n#R|  4 -2.59629 0.3820407              1 0.06937757 0.03405632 0.1361679\n#R|  5 -2.58188 0.3781745              1 0.07031373 0.03478710 0.1369735\n#R|  6 -2.56747 0.3743157              1 0.07126156 0.03553248 0.1377849\n\n\nFinally, I “bind as columns” the original data frame of KFIi values and select (and slightly re-arrange) the variables needed to make the figure.\n\npredOpo &lt;- predOpo |&gt;\n  bind_cols(KFI_i_df) |&gt;\n  select(KFI_i,predProb,predLCI,predUCI)\nhead(predOpo)\n\n#R|        KFI_i   predProb    predLCI   predUCI\n#R|  1  9.560078 0.06663790 0.03194919 0.1337847\n#R|  2  9.848166 0.06753978 0.03263760 0.1345735\n#R|  3 10.136254 0.06845296 0.03333990 0.1353679\n#R|  4 10.424343 0.06937757 0.03405632 0.1361679\n#R|  5 10.712431 0.07031373 0.03478710 0.1369735\n#R|  6 11.000519 0.07126156 0.03553248 0.1377849\n\n\nThe steps above were separated to show the process. In practice, I would complete these steps in one set of code shown below.\n\npredOpo &lt;- predict(glmOpo,KFI_i_df,type=\"link\",se.fit=TRUE) |&gt;\n  as.data.frame() |&gt;\n  mutate(predProb=inverse_logit(fit),\n         predLCI=inverse_logit(fit-1.96*se.fit),\n         predUCI=inverse_logit(fit+1.96*se.fit)) |&gt;\n  bind_cols(KFI_i_df) |&gt;\n  select(KFI_i,predProb,predLCI,predUCI)\n\nJust to be clear, predOpo created here contains a large sequence of KFIi values across the observed range of this variable, the predicted probability that Virginia Opossum will occur in the diet of Bobcat for each of these KFIi values, and approximate 95% confidence intervals for each of those predicted probabilities. These are the data required to reconstruct one panel of Figure 2."
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#making-one-sub-panel",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#making-one-sub-panel",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Making one Sub-Panel",
    "text": "Making one Sub-Panel\nShowing the logistic regression results for Virginia Opossums (i.e., one panel in Figure 2) largely consists of plotting the predicted probabilities and 95% confidence bounds against the KFIi values.\n\npVO &lt;- ggplot(data=predOpo,mapping=aes(x=KFI_i)) +\n  geom_line(mapping=aes(y=predProb)) +\n  geom_line(mapping=aes(y=predLCI)) +\n  geom_line(mapping=aes(y=predUCI))\npVO\n\n\n\n\n\n\n\n\ngeom_ribbon() can be used to shade the area between the two confidence bounds. However, geom_ribbon() should appear first so that the plotted lines will be “on top” of it and, thus, visible.6 Including color=\"black\" in geom_ribbon() will also color the bounding lines of the ribbon, so that the separate geom_line()s for the confidence bounds are not needed.\n6 I filled the ribbon with “darkslategray” in an attempt to match the author’s color choice.\npVO &lt;- ggplot(data=predOpo,mapping=aes(x=KFI_i)) +\n  geom_ribbon(mapping=aes(ymin=predLCI,ymax=predUCI),\n              fill=\"darkslategray4\",color=\"black\") +\n  geom_line(mapping=aes(y=predProb))\npVO\n\n\n\n\n\n\n\n\nFinally, to follow the author’s choices, I labeled the y-axis and adjusted its limits, breaks, and expansion factor; labeled the x-axis and adjusted its expansion factor; provided an overall plot title; and placed an “(A)” label in the upper-left corner.7 Thus, the final code for this portion of Figure 2 is as follows.\n7 This seemed redundant to me given the plot title.\npVO &lt;- ggplot(data=predOpo,mapping=aes(x=KFI_i)) +\n  geom_ribbon(mapping=aes(ymin=predLCI,ymax=predUCI),\n              fill=\"darkslategray4\",color=\"black\") +\n  geom_line(mapping=aes(y=predProb)) +\n  scale_y_continuous(name=\"Proportion of Occurrence\",\n                     limits=c(0,1),breaks=seq(0,1,0.2),\n                     expand=expansion(mult=0.02)) +\n  scale_x_continuous(name=KFI_lbl,\n                     expand=expansion(mult=0.02)) +\n  labs(title=\"Virginia Opossum\") +\n  annotate(geom=\"text\",x=-Inf,y=Inf,label=\"(A)\",\n           hjust=-0.5,vjust=1.5)"
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#finishing-the-figure",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#finishing-the-figure",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Finishing the Figure",
    "text": "Finishing the Figure\nThe final Figure 2 has a second panel for “Rabbits.” Thus, the code from above was copied and adjusted slightly to make a similar plot for rabbits.\n\n# fit logistic regression\nglmRab &lt;- glm(Rabbits_Hares~KFI_i,data=dat,family=\"binomial\")\n\n# make predicted probabilities data frame\npredRab &lt;- predict(glmRab,KFI_i_df,type=\"link\",se.fit=TRUE) |&gt;\n  as.data.frame() |&gt;\n  mutate(predProb=inverse_logit(fit),\n         predLCI=inverse_logit(fit-1.96*se.fit),\n         predUCI=inverse_logit(fit+1.96*se.fit)) |&gt;\n  bind_cols(KFI_i_df) |&gt;\n  select(KFI_i,predProb,predLCI,predUCI)\n\n# make the plot\npRH &lt;- ggplot(data=predRab,mapping=aes(x=KFI_i)) +\n  geom_ribbon(mapping=aes(ymin=predLCI,ymax=predUCI),\n              fill=\"darkslategray4\",color=\"black\") +\n  geom_line(mapping=aes(y=predProb)) +\n  scale_y_continuous(name=\"Proportion of Occurrence\",\n                     limits=c(0,1),breaks=seq(0,1,0.2),\n                     expand=expansion(mult=0.02)) +\n  scale_x_continuous(name=KFI_lbl,\n                     expand=expansion(mult=0.02)) +\n  labs(title=\"Rabbit\") +\n  annotate(geom=\"text\",x=-Inf,y=Inf,label=\"(B)\",\n           hjust=-0.5,vjust=1.5)\n\nThe two plots are placed side-by-side as shown below using functionality from the patchwork package.8\n8 The author’s used longer ticks on the axes then I used here. Also, their figure has vertical “striations” that I think are a result of how they constructed the plot and not a feature to be replicated.\npVO + pRH"
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#fitting-the-logistic-regression-1",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#fitting-the-logistic-regression-1",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Fitting the Logistic Regression",
    "text": "Fitting the Logistic Regression\nFigure 3 is used by Landry et al. (2022) to demonstrate an interaction effect of season of capture on the relationship between the occurrence of squirrels in the diet of Bobcat and hard mast index. Thus, a logistic regression is fit with hard mast index, season, and the interaction between hard mast index and season as explanatory “variables.”9\n9 I coded the three explanatory terms explicitly here, however HardMast*Season would have expanded to code all three as well.\nglmSqrl &lt;- glm(Squirrels~HardMast+Season+HardMast:Season,data=dat,family=\"binomial\")\n\nThe summary results (not shown here) match those in Landry et al. (2022).10\n10 See results in the second sentence above Figure 3.\nsummary(glmSqrl)"
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#making-a-data-frame-of-predicted-probabilities-1",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#making-a-data-frame-of-predicted-probabilities-1",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Making a Data Frame of Predicted Probabilities",
    "text": "Making a Data Frame of Predicted Probabilities\nSimilar to constructing Figure 2, a data frame of predicted probabilities with 95% confidence intervals at a large number of hard mast index values is needed to reproduce Figure 3. However, the probabilities must be predicted for both seasons. The data frame used to make the predictions must have a variable for the hard mast index values and the season as the glm() fit above used both of these variables. Thus, this data frame must have the 199 hard mast index values repeated twice, corresponding to the two seasons and each season repeated 199 times to match the hard mast index values.\n\nHM_df &lt;- data.frame(\n  HardMast=rep(seq(min(dat$HardMast),max(dat$HardMast),length.out=199),2),\n  Season=rep(unique(dat$Season),each=199))\nFSA::headtail(HM_df)\n\n#R|      HardMast    Season\n#R|  1   26.92086 2014-2015\n#R|  2   27.06435 2014-2015\n#R|  3   27.20784 2014-2015\n#R|  396 55.04534 2015-2016\n#R|  397 55.18883 2015-2016\n#R|  398 55.33232 2015-2016\n\n\nWith this data frame, the data frame of predicted probabilities is constructed as demonstrated for Figure 2.\n\npredSqrl &lt;- predict(glmSqrl,HM_df,type=\"link\",se.fit=TRUE) |&gt;\n  as.data.frame() |&gt;\n  mutate(predProb=inverse_logit(fit),\n         predLCI=inverse_logit(fit-1.96*se.fit),\n         predUCI=inverse_logit(fit+1.96*se.fit)) |&gt;\n  bind_cols(HM_df) |&gt;\n  select(HardMast,Season,predProb,predLCI,predUCI)\nFSA::headtail(predSqrl)\n\n#R|      HardMast    Season  predProb   predLCI   predUCI\n#R|  1   26.92086 2014-2015 0.3040081 0.1317484 0.5570061\n#R|  2   27.06435 2014-2015 0.2962642 0.1309603 0.5404592\n#R|  3   27.20784 2014-2015 0.2886357 0.1301474 0.5238861\n#R|  396 55.04534 2015-2016 0.2846782 0.1470444 0.4788193\n#R|  397 55.18883 2015-2016 0.2853101 0.1466732 0.4811068\n#R|  398 55.33232 2015-2016 0.2859428 0.1463013 0.4833981"
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#finishing-the-figure-1",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#finishing-the-figure-1",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Finishing the Figure",
    "text": "Finishing the Figure\nFigure 3 is constructed very similarly to Figure 2 except that a fill= color must be mapped to Season in geom_ribbon() and linetype= must be mapped to Season in geom_line().11 scale_fill_manual() and scale_linetype_manual() are used to over-ride the default fill colors and line types to better match the author’s choices. Further guide=\"none\" wais used in scale_fill_manual() as the author’s did not show the fill color in their legend. Finally, I manually positioned the legend 75% of the way along the x-axis and 80% of the way up the y-axis.12\n11 In geom_ribbon() I did not include color= because the authors did not outline the confidence regions. I included a slight transparency with alplha=0.75 so the two regions were more visible where they overlapped.12 Again, the author’s Figure 3 looks striated, but I did not consider this a feature of the plot.\nggplot(data=predSqrl,mapping=aes(x=HardMast)) +\n  geom_ribbon(mapping=aes(ymin=predLCI,ymax=predUCI,fill=Season),\n              alpha=0.75) +\n  geom_line(mapping=aes(y=predProb,linetype=Season)) +\n  scale_y_continuous(name=\"Proportion of Occurrence\",\n                     limits=c(0,1),breaks=seq(0,1,0.2),\n                     expand=expansion(mult=0.02)) +\n  scale_x_continuous(name=\"Hard Mast Index\",\n                     expand=expansion(mult=0.02)) +\n  scale_fill_manual(values=c(\"2014-2015\"=\"darkslategray\",\"2015-2016\"=\"gray75\"),\n                    guide=\"none\") +\n  scale_linetype_manual(values=c(\"2014-2015\"=\"solid\",\"2015-2016\"=\"dashed\")) +\n  labs(title=\"Squirrel\") +\n  theme(legend.position=c(0.75,0.8))\n\n\n\n\n\n\n\n\n \nIt should be noted that this plot does not fully match Figure 3 in Landry et al. (2022). This is most evident for the 2014-2015 seasons at small hard mast indices where the authors predicted probability approaches 0.4 more closely and the upper level of the confidence region is above 0.6. I am not sure what explains this difference but it could be that the author’s used a slightly lower minimum hard mast index for their predictions or that their predicted values were made from separate logistic regressions for each season."
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#show-the-data",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#show-the-data",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Show the Data",
    "text": "Show the Data\nI generally don’t like plots that don’t show observed data, which is the case for both Figures 2 and 3. The observed data can be added to the plot using geom_point() as shown in a previous post. Note that the data=predOpo had to be removed from ggplot() and added to geom_ribbon() and geom_line() because, with this addition, geom_point() uses a different data frame. When geoms use different data frames, those data frames must be declared in the geom rather than in ggplot(). Also note the use of alpha= here so that the points are semi-transparent to handle overplotting.\n\npVO2 &lt;- ggplot(mapping=aes(x=KFI_i)) +\n  geom_ribbon(data=predOpo,mapping=aes(ymin=predLCI,ymax=predUCI),\n              fill=\"darkslategray4\",color=\"black\") +\n  geom_line(data=predOpo,mapping=aes(y=predProb)) +\n  geom_point(data=dat,mapping=aes(y=Opossum),alpha=0.2) +\n  scale_y_continuous(name=\"Proportion of Occurrence\",\n                     limits=c(0,1),breaks=seq(0,1,0.2),\n                     expand=expansion(mult=0.02)) +\n  scale_x_continuous(name=KFI_lbl,\n                     expand=expansion(mult=0.02)) +\n  labs(title=\"Virginia Opossum\") +\n  annotate(geom=\"text\",x=-Inf,y=Inf,label=\"(A)\",\n           hjust=-0.5,vjust=1.5)\npVO2"
  },
  {
    "objectID": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#present-only-over-the-range-of-the-groups-data",
    "href": "blog/posts/2023-3-6_Landryetal2022_LogRegress/index.html#present-only-over-the-range-of-the-groups-data",
    "title": "Landry et al. (2022) Logistic Regression Figures",
    "section": "Present Only Over the Range of the Group’s Data",
    "text": "Present Only Over the Range of the Group’s Data\nFigure 3 as created above and shown in Landry et al. (2022) implies the same range of hard mast index values in both seasons (i.e., the logistic regression model is presented over the same range of hard mast index values for both seasons). However, a summary of the range of hard mast index values for each season reveals very little overlap between the two seasons.\n\nsmry &lt;- dat |&gt;\n  group_by(Season) |&gt;\n  summarize(n=n(),\n            minHM=min(HardMast,na.rm=TRUE),\n            maxHM=max(HardMast,na.rm=TRUE))\nsmry\n\n#R|  # A tibble: 2 × 4\n#R|    Season        n minHM maxHM\n#R|    &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#R|  1 2014-2015   150  26.9  33.5\n#R|  2 2015-2016   150  29.8  55.3\n\n\nMy preference is to show the model fits across the ranges observed within each season. To do so requires modifying HM_df from above to use the range of values within each season, rather than the range of values for both seasons combined. I could not find a simple way to do this, though it is accomplished below using smry from above and a combination of apply(), with a user-defined function for sequence, and pivot_longer().\n\nseq2 &lt;- function(x) seq(x[\"minHM\"],x[\"maxHM\"],length.out=199)\n\ntmp &lt;- apply(smry,MARGIN=1,FUN=seq2) |&gt;\n  as.data.frame()\nnames(tmp) &lt;- smry$Season\n\nHM_df &lt;- pivot_longer(tmp,cols=everything(),\n                      values_to=\"HardMast\",names_to=\"Season\") |&gt;\n  arrange(Season,HardMast)\n\nFSA::headtail(HM_df)\n\n#R|         Season HardMast\n#R|  1   2014-2015 26.92086\n#R|  2   2014-2015 26.95414\n#R|  3   2014-2015 26.98742\n#R|  396 2015-2016 55.07422\n#R|  397 2015-2016 55.20327\n#R|  398 2015-2016 55.33232\n\n\nThen make a new predicted probabilities data frame from this new data frame.\n\npredSqrl &lt;- predict(glmSqrl,HM_df,type=\"link\",se.fit=TRUE) |&gt;\n  as.data.frame() |&gt;\n  mutate(predProb=inverse_logit(fit),\n         predLCI=inverse_logit(fit-1.96*se.fit),\n         predUCI=inverse_logit(fit+1.96*se.fit)) |&gt;\n  bind_cols(HM_df) |&gt;\n  select(HardMast,Season,predProb,predLCI,predUCI)\nFSA::headtail(predSqrl)\n\n#R|      HardMast    Season  predProb   predLCI   predUCI\n#R|  1   26.92086 2014-2015 0.3040081 0.1317484 0.5570061\n#R|  2   26.95414 2014-2015 0.3022018 0.1315677 0.5531727\n#R|  3   26.98742 2014-2015 0.3004017 0.1313858 0.5493363\n#R|  396 55.07422 2015-2016 0.2848053 0.1469697 0.4792794\n#R|  397 55.20327 2015-2016 0.2853737 0.1466358 0.4813372\n#R|  398 55.33232 2015-2016 0.2859428 0.1463013 0.4833981\n\n\nAnd remake the plot, also including the raw data. Note that I changed colors here as the original colors in Figure 3 could not be differentiated well when using semi-transparency for over-plotting.\n\nggplot(mapping=aes(x=HardMast)) +\n  geom_ribbon(data=predSqrl,mapping=aes(ymin=predLCI,ymax=predUCI,fill=Season),\n              alpha=0.75) +\n  geom_line(data=predSqrl,mapping=aes(y=predProb,linetype=Season)) +\n  geom_point(data=dat,mapping=aes(y=Squirrels,color=Season),\n             alpha=0.1) +\n  scale_y_continuous(name=\"Proportion of Occurrence\",\n                     limits=c(0,1),breaks=seq(0,1,0.2),\n                     expand=expansion(mult=0.02)) +\n  scale_x_continuous(name=\"Hard Mast Index\",\n                     expand=expansion(mult=0.02)) +\n  scale_fill_manual(values=c(\"2014-2015\"=\"red4\",\"2015-2016\"=\"cyan4\"),\n                    guide=\"none\") +\n  scale_color_manual(values=c(\"2014-2015\"=\"red4\",\"2015-2016\"=\"cyan4\"),\n                     guide=\"none\") +\n  scale_linetype_manual(values=c(\"2014-2015\"=\"solid\",\"2015-2016\"=\"dashed\")) +\n  labs(title=\"Squirrel\") +\n  theme(legend.position=c(0.75,0.8))"
  },
  {
    "objectID": "blog/posts/2023-3-30_Nested_xaxis/index.html",
    "href": "blog/posts/2023-3-30_Nested_xaxis/index.html",
    "title": "Nested X-Axis Labels",
    "section": "",
    "text": "Recently a Twitter follower asked …\n\nAny thought on easy ways to add a second x axis? Ex: sampling periods (sequential) on x axis and reviewer asking for the correspoding year as an additional axis under/over primary axis. It appears…hard to do.\n\nHere I explore a few different ways to do this.\nThe following packages are used here. Also note that a few functions from FSA and scales are used with :: so that the entire package is not attached here.\n\nlibrary(tidyverse)  # for ggplot2 and dplyr\nlibrary(ggh4x)      # for a variety of \"hacks\" described below"
  },
  {
    "objectID": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-1---bar-chart",
    "href": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-1---bar-chart",
    "title": "Nested X-Axis Labels",
    "section": "Example 1 - Bar Chart",
    "text": "Example 1 - Bar Chart\n\nSample Data\nSuppose you have very simple data where some numeric summary has been made for each group and that those groups are also categorized to a higher level. For example, total catch by species with species also categorized by family.1\n1 Family is made a factor() here so that the order could be controlled (rather than alphabetical by default).\n\nCode\ndat1 &lt;- data.frame(family=c(\"Centrarchid\",\"Centrarchid\",\"Percid\",\"Percid\",\"Esocid\"),\n                   species=c(\"Bluegill\",\"Pumpkinseed\",\"Walleye\",\"Sauger\",\"Muskellunge\"),\n                   catch=c(34,45,23,36,7)) |&gt;\n  mutate(family=factor(family,levels=c(\"Centrarchid\",\"Percid\",\"Esocid\")))\ndat1\n\n\n#R|         family     species catch\n#R|  1 Centrarchid    Bluegill    34\n#R|  2 Centrarchid Pumpkinseed    45\n#R|  3      Percid     Walleye    23\n#R|  4      Percid      Sauger    36\n#R|  5      Esocid Muskellunge     7\n\n\nA simple bar chart of these data is created with geom_col() with species mapped to the x-axis and catch mapped to the y-axis. Here I provided a title for the y-axis, removed the lower expansion and reduce the upper expansion of the y-axis, applied the classic theme, increased the size of the axis title and tick mark text, made the axis titles bold, made the tick mark text black, and removed the x-axis title.\n\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"))\n\n\n\n\n\n\n\n\nWhat the Twitter follower would want in this case is another layer of x-axis labels that would succinctly identify the family for each species.\n\n\nSpecifying Facets\nFaceting provides one option for applying these labels. Below I added one row of facets based on the family name with facet_grid().\n\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  facet_grid(cols=vars(family)) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"))\n\n\n\n\n\n\n\n\nThis does not look immediately helpful!! However, adding scales=\"free_x\" to facet_grid() reduces each x-axis to only the species present in that facet (i.e., family).\n\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  facet_grid(cols=vars(family),scales=\"free_x\") +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"))\n\n\n\n\n\n\n\n\nAdding space=free_x to facet_grid() ensures that the “bars” in each facet are the same width by adjusting the facet width to match the space needed to show just the data for that facet.\n\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  facet_grid(cols=vars(family),scales=\"free_x\",space=\"free_x\") +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"))\n\n\n\n\n\n\n\n\nFinally, adding switch=\"x\" to facet_grid() moves the facet strip labels to the bottom of the plot area for each facet.\n\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  facet_grid(cols=vars(family),scales=\"free_x\",space=\"free_x\",switch=\"x\") +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"))\n\n\n\n\n\n\n\n\nThe next step is to move the facet strip labels “outside”2 the axis tick mark labels with strip.placement='outside' in theme(). The facet strip “box” was also removed with strip.background.x= and the facet strip text was altered with strip.text=.\n2 Rather than the default “inside”.\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  facet_grid(cols=vars(family),scales=\"free_x\",space=\"free_x\",switch=\"x\") +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        strip.placement='outside',\n        strip.background.x=element_blank(),\n        strip.text=element_text(size=12,color=\"black\",face=\"bold\"))\n\n\n\n\n\n\n\n\nFinally, facets were “smushed” together so that the x-axis looks continuous by reducing panel.spacing.x= to 0.\n\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  facet_grid(cols=vars(family),scales=\"free_x\",space=\"free_x\",switch=\"x\") +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        strip.placement='outside',\n        strip.background.x=element_blank(),\n        strip.text=element_text(size=12,color=\"black\",face=\"bold\"),\n        panel.spacing.x=unit(0,\"pt\"))\n\n\n\n\n\n\n\n\nThe family categorization could be further highlighted with a fill= color mapped to family.\n\nggplot(data=dat1,mapping=aes(x=species,y=catch,fill=family)) +\n  geom_col(color=\"black\") +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  scale_fill_grey(start=0.1,end=0.5) +\n  facet_grid(cols=vars(family),scales=\"free_x\",space=\"free_x\",switch=\"x\") +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        strip.placement='outside',\n        strip.background.x=element_blank(),\n        strip.text=element_text(size=12,color=\"black\",face=\"bold\"),\n        panel.spacing.x=unit(0,\"pt\"),\n        legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Question\n\n\n\nI could not, using this method, figure out how to reduce the inter-facet gap between bars so that it matched the intra-facet gap between bars."
  },
  {
    "objectID": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-2---bar-chart-with-dates",
    "href": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-2---bar-chart-with-dates",
    "title": "Nested X-Axis Labels",
    "section": "Example 2 - Bar Chart with Dates",
    "text": "Example 2 - Bar Chart with Dates\nThe Twitter follower’s questions was about dates. So, imagine data that is the total catch of a species from the second(ish) week of each month over a few year period.\n\n\nCode\ndat2 &lt;- data.frame(sample_date=seq(as.Date(\"2020-8-12\"),\n                                   as.Date(\"2022-4-7\"),by=\"month\")) |&gt;\n  mutate(sample_date=sample_date-sample((-5:5),length(sample_date),replace=TRUE),\n         catch=round(runif(length(sample_date),50,150)))\n\nFSA::headtail(dat2)\n\n\n#R|     sample_date catch\n#R|  1   2020-08-14    73\n#R|  2   2020-09-15   110\n#R|  3   2020-10-15   149\n#R|  18  2022-01-14   131\n#R|  19  2022-02-07    83\n#R|  20  2022-03-10    54\n\n\nNow extract the month and year from the sample dates.\n\ndat2 &lt;- dat2 |&gt;\n  mutate(mon=month(sample_date,label=TRUE),\n         yr=year(sample_date))\nFSA::headtail(dat2)\n\n#R|     sample_date catch mon   yr\n#R|  1   2020-08-14    73 Aug 2020\n#R|  2   2020-09-15   110 Sep 2020\n#R|  3   2020-10-15   149 Oct 2020\n#R|  18  2022-01-14   131 Jan 2022\n#R|  19  2022-02-07    83 Feb 2022\n#R|  20  2022-03-10    54 Mar 2022\n\n\nA plot using the faceting method is created below.\n\nggplot(data=dat2,mapping=aes(x=mon,y=catch,fill=factor(yr))) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.25))) +\n  scale_fill_viridis_d(begin=0.1,end=0.5) +\n  facet_grid(cols=vars(yr),space='free_x',scales='free_x',switch='x') +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        strip.placement='outside',\n        strip.background.x=element_blank(),\n        strip.text=element_text(size=12,color=\"black\",face=\"bold\"),\n        panel.spacing.x=unit(0,\"pt\"),\n        legend.position=\"none\")"
  },
  {
    "objectID": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-1---bar-chart-1",
    "href": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-1---bar-chart-1",
    "title": "Nested X-Axis Labels",
    "section": "Example 1 - Bar Chart",
    "text": "Example 1 - Bar Chart\nThe ggh4x package provides guide_axis_nested() and related helpers for creating the secondary x-axis. This function, however, is predicated on plotting the interaction between the variables that defined the primary and secondary labels on the x-axis. An “interaction” is created with interaction() with, for these purposes, the nested variable listed second. For further below it is important to note that the parts of the interactions are separated by a single dot/period.\n\ndat1 &lt;- dat1 |&gt;\n  mutate(sfint=interaction(species,family))\ndat1\n\n#R|         family     species catch                   sfint\n#R|  1 Centrarchid    Bluegill    34    Bluegill.Centrarchid\n#R|  2 Centrarchid Pumpkinseed    45 Pumpkinseed.Centrarchid\n#R|  3      Percid     Walleye    23          Walleye.Percid\n#R|  4      Percid      Sauger    36           Sauger.Percid\n#R|  5      Esocid Muskellunge     7      Muskellunge.Esocid\n\n\nThe basic bar chart from above is modified by mapping this interaction variable, rather than species, to the x-axis.\n\nggplot(data=dat1,mapping=aes(x=sfint,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"))\n\n\n\n\n\n\n\n\nThe parts of the interactions can be disentangled and turned into nested axes by setting guide_axis_nested() equal to guide= in scale_x_discrete(). Note that delim=\".\" is used in guide_axis_nested() because the parts of the interaction were separated with a single dot/period.\n\nggplot(data=dat1,mapping=aes(x=sfint,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  scale_x_discrete(guide=guide_axis_nested(delim=\".\")) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"))\n\n\n\n\n\n\n\n\nThe primary tick mark labels are controlled with axis.text.x= which inherits from axis.text= if not given. Thus, the “species” labels are 10 pt black as defined above. The secondary tick mark labels inherit from axis.text.x= unless modifications are made in ggh4x.axis.nesttext.x= using element_text(). Below the “family” labels are set to 12 pt bold text.3 Finally, the line that separates the levels of labels is controlled with element_line() in ggh4x.axis.nestline.x=. The line is made slightly heavier below for illustration.\n3 And will be black because axis.text= was set to black.\nggplot(data=dat1,mapping=aes(x=sfint,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  scale_x_discrete(guide=guide_axis_nested(delim=\".\")) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        ggh4x.axis.nesttext.x=element_text(size=12,face=\"bold\"),\n        ggh4x.axis.nestline.x=element_line(linewidth=0.75))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Question\n\n\n\nI could not, using this method, figure out how to make the line between the two levels of labels longer. For example, I would prefer the “Centrarchid” line to extend further right to more completely cover “Pumpkinseed.”"
  },
  {
    "objectID": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-2-bar-chart-with-dates",
    "href": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-2-bar-chart-with-dates",
    "title": "Nested X-Axis Labels",
    "section": "Example 2 – Bar Chart with Dates",
    "text": "Example 2 – Bar Chart with Dates\nThe data frame used for Option 1 must be modified to include the interaction between the mon and yr for use with ggh4x.\n\ndat2 &lt;- dat2 |&gt;\n  mutate(myint=interaction(mon,yr))\nFSA::headtail(dat2)\n\n#R|     sample_date catch mon   yr    myint\n#R|  1   2020-08-14    73 Aug 2020 Aug.2020\n#R|  2   2020-09-15   110 Sep 2020 Sep.2020\n#R|  3   2020-10-15   149 Oct 2020 Oct.2020\n#R|  18  2022-01-14   131 Jan 2022 Jan.2022\n#R|  19  2022-02-07    83 Feb 2022 Feb.2022\n#R|  20  2022-03-10    54 Mar 2022 Mar.2022\n\n\nA plot using ggh4x is created below.\n\nggplot(data=dat2,mapping=aes(x=myint,y=catch,fill=factor(yr))) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.25))) +\n  scale_fill_viridis_d(begin=0.1,end=0.5) +\n  scale_x_discrete(guide=guide_axis_nested(delim=\".\")) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        ggh4x.axis.nesttext.x=element_text(size=12,face=\"bold\"),\n        ggh4x.axis.nestline.x=element_line(linewidth=0.75),\n        legend.position=\"none\")"
  },
  {
    "objectID": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-1---bar-chart-2",
    "href": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-1---bar-chart-2",
    "title": "Nested X-Axis Labels",
    "section": "Example 1 - Bar Chart",
    "text": "Example 1 - Bar Chart\nA third options is to create space below the x-axis and “place” the secondary axis “manually.” Before creating this space it is important to order primary axis labels so that categories to be grouped by the secondary level are next to each other. In this example, the species variable must be converted to a factor with the levels controlled so that families are grouped together.\n\ndat1 &lt;- dat1 |&gt;\n  mutate(species=factor(species,levels=c(\"Bluegill\",\"Pumpkinseed\",\n                                         \"Walleye\",\"Sauger\",\n                                         \"Muskellunge\")))\n\nSpace below the x-axis is created by increasing the bottom margin around the plot with plot.margin= in theme(). Below the bottom margin was increased to two “lines” and the other margins were set to one “line”.4\n4 The top, left, and right margins are defined by t=, l= and r= in margin().\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        plot.margin=margin(b=2,t=1,l=1,r=1,unit=\"lines\"))\n\n\n\n\n\n\n\n\nPlot “clipping” must be turned off with clip=\"off\" in coord_caresian() to be able to place “objects” in the space just created.5 In addition, the y-axis limits should be set with ylim= in coord_cartesian() so that the y-axis limits do not change as labels are added beneath the x-axis. As a demonstration, I placed my name at the x=1.5 and y=-5 coordinate.\n5 Because that space is outside of the main plot area.\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        plot.margin=margin(b=2,t=1,l=1,r=1,unit=\"lines\")) +\n  coord_cartesian(ylim=c(0,NA),clip=\"off\") +\n  annotate(geom=\"text\",x=1.5,y=-5,label=\"Derek\",color=\"red\",fontface=\"bold\")\n\n\n\n\n\n\n\n\nChoosing a value for the y-coordinate beneath the axis is largely a matter of trial-and-error after an initial first guess. The x-coordinate, though, can largely be determined from the x-axis. The levels for a factor such as species are recorded as integers “behind-the-scenes.” In this example, a “Bluegill” is defined with a “1” as it is the first level and its bar will be above an invisible “1” on the x-axis. Thus, placing my name at x=1.5 centers it between the first and second bars.\nHere I want to create a secondary axis similar to what ggh4x produced – a line segment with a label underneath it. The first segment for the secondary axis should cover “Bluegill” and “Pumpkinseed.” Thus, it should start a little below 1 and end a little above 2. Other segments are defined similarly and stored in a data frame below.\n\nx2lns &lt;- data.frame(xstart=c(0.55,2.55,4.55),\n                    xend=c(2.45,4.45,5.45))\nx2lns\n\n#R|    xstart xend\n#R|  1   0.55 2.45\n#R|  2   2.55 4.45\n#R|  3   4.55 5.45\n\n\nAt first, I roughly guess at starting and ending points and then come back to adjust these values if the lines need to be extended or shortened to look good. These segments are added to the plot with geom_segment() using this new data frame with the variables mapped appropriately. I set y= and yend= outside of aes() as the y-coordinate is constant for all these segments. Finally, the segment was made slightly heavier.\n\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        plot.margin=margin(b=2,t=1,l=1,r=1,unit=\"lines\")) +\n  coord_cartesian(ylim=c(0,NA),clip=\"off\") +\n  geom_segment(data=x2lns,mapping=aes(x=xstart,xend=xend),y=-5,yend=-5,\n               linewidth=0.75)\n\n\n\n\n\n\n\n\nOnce the segments are set as desired, another data frame is created that has the x-axis coordinate and text for the secondary axis label. Below, the coordinate was found by averaging xstart and xend for each segment in x2lns.6 The text for the labels was then entered manually.\n6 This x coordinates in this data frame could have been entered manually, but computing the average makes it easier to put the label in the center of the segment.\nx2lbls &lt;- x2lns |&gt;\n  rowwise() |&gt;\n  summarize(x=mean(c(xstart,xend))) |&gt;\n  ungroup() |&gt;\n  mutate(lbl=c(\"Centrarchid\",\"Percid\",\"Esocid\"))\nx2lbls\n\n#R|  # A tibble: 3 × 2\n#R|        x lbl        \n#R|    &lt;dbl&gt; &lt;chr&gt;      \n#R|  1   1.5 Centrarchid\n#R|  2   3.5 Percid     \n#R|  3   5   Esocid\n\n\nThese labels are then added with geom_text() using the new data frame with the variables mapped appropriately. Again, y= was set outside of aes() as this coordinate is constant for all labels. The text was adjusted to be a 12 pt bold.\n\nggplot(data=dat1,mapping=aes(x=species,y=catch)) +\n  geom_col() +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=c(0,0.025))) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        plot.margin=margin(b=2,t=1,l=1,r=1,unit=\"lines\")) +\n  coord_cartesian(ylim=c(0,NA),clip=\"off\") +\n  geom_segment(data=x2lns,mapping=aes(x=xstart,xend=xend),y=-5,yend=-5,\n               linewidth=0.75) +\n  geom_text(data=x2lbls,mapping=aes(x=x,label=lbl),y=-8,\n            size=12/.pt,fontface=\"bold\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nChoosing how much to increase the bottom margin and choosing the x- and y-coordinates for the secondary x-axis labels with this method is largely a matter of trial-and-error."
  },
  {
    "objectID": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-3---line-plot",
    "href": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-3---line-plot",
    "title": "Nested X-Axis Labels",
    "section": "Example 3 - Line Plot",
    "text": "Example 3 - Line Plot\nOptions 1 and 2 cannot handle situations where the plot is not “discrete.”7 For example, suppose that a line plot of total catch in dat2 by sample date (rather than month) is desired.\n7 At least, I could not make them work for these situations.\nggplot(data=dat2,mapping=aes(x=sample_date,y=catch)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=scales::breaks_width(\"month\"),\n               labels=scales::label_date(\"%b %y\"),expand=expansion(mult=0.01)) +\n  scale_y_continuous(name=\"Total catch\",limits=c(0,200),expand=expansion(mult=0)) +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"))\n\n\n\n\n\n\n\n\nHowever, the x-axis is cluttered and suppose that a reviewer wants the months shown on the x-axis ticks with years as a secondary axis below that. The methods from above may be tried, but faceting clearly does not work (see below) and I could not get the ggh4x method to work with scale_x_date().\n\nggplot(data=dat2,mapping=aes(x=sample_date,y=catch)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=scales::breaks_width(\"month\"),\n               labels=scales::label_date(\"%b\"),expand=expansion(mult=0.01)) +\n  scale_y_continuous(name=\"Total catch\",limits=c(0,200),expand=expansion(mult=0)) +\n  facet_grid(cols=vars(yr),space='free_x',scales='free_x',switch='x') +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        strip.placement='outside',\n        strip.background.x=element_blank(),\n        strip.text=element_text(size=12,color=\"black\",face=\"bold\"),\n        panel.spacing.x=unit(0,\"pt\"))\n\n\n\n\n\n\n\n\nOption 3, however, will work for situations like this. For this example, segments should “cover” Sep-Dec for 2021, Jan-Dec for 2022, and Jan-Mar for 2023. Starting (in xstart) and ending (in xend) dates for these segments are put in a data frame below. A data frame of labels is then constructed from it similar to before, except that the label is the year extracted from the x-coordinate date.\n\nx2lns &lt;- data.frame(xstart=as.Date(c(\"20-Aug-2020\",\"20-Dec-2020\",\"20-Dec-2021\"),\n                                   format=\"%d-%B-%Y\"),\n                    xend=as.Date(c(\"15-Dec-2020\",\"15-Dec-2021\",\"15-Mar-2022\"),\n                                 format=\"%d-%B-%Y\"))\n\nx2lbls &lt;- x2lns |&gt;\n  rowwise() |&gt;\n  summarize(x=mean(c(xstart,xend))) |&gt;\n  ungroup() |&gt;\n  mutate(lbl=year(x))\n\nSegments and labels are then added as before.8\n8 Note the use of coord_cartesian(), geom_segment(), geom_text(), and plot.margin=.\nggplot(data=dat2,mapping=aes(x=sample_date,y=catch)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=scales::breaks_width(\"month\"),\n               labels=scales::label_date(\"%b\"),expand=expansion(mult=0.01)) +\n  scale_y_continuous(name=\"Total catch\",expand=expansion(mult=0)) +\n  coord_cartesian(ylim=c(0,200),clip=\"off\") +\n  theme_classic() +\n  theme(axis.title=element_text(size=12,face=\"bold\"),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,color=\"black\"),\n        plot.margin=margin(t=1,l=1,b=2,r=1,unit=\"lines\")) +\n  geom_segment(data=x2lns,mapping=aes(x=xstart,xend=xend),y=-18,yend=-18,\n               linewidth=0.75) +\n  geom_text(data=x2lbls,mapping=aes(x=x,label=lbl),y=-26,\n            size=12/.pt,fontface=\"bold\")"
  },
  {
    "objectID": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-4---histogram",
    "href": "blog/posts/2023-3-30_Nested_xaxis/index.html#example-4---histogram",
    "title": "Nested X-Axis Labels",
    "section": "Example 4 - Histogram",
    "text": "Example 4 - Histogram\nIn this example, I use data on female Walleye captured from location “2” in Lake Erie in 2010.\n\n\nCode\ndata(WalleyeErie2,package=\"FSAdata\")\nwaedat &lt;- WalleyeErie2 |&gt;\n  filter(loc==2,year==2010,sex==\"female\")\nFSA::headtail(waedat)\n\n\n#R|        setID loc grid year  tl    w    sex    mat age\n#R|  1   2010019   2 1005 2010 642 3138 female mature   7\n#R|  2   2010019   2 1005 2010 686 3360 female mature   7\n#R|  3   2010019   2 1005 2010 685 3489 female mature  11\n#R|  919 2010058   2 1230 2010 715 4169 female mature  10\n#R|  920 2010058   2 1230 2010 549 1853 female mature   3\n#R|  921 2010058   2 1230 2010 596 2432 female mature   7\n\n\nSuppose we want to make a simple length frequency histogram like that below,9 but with Gabelhouse length categories (“stock”, “quality”, etc.) listed below the lengths but above the title on the x-axis.10\n9 See this post about making length frequency histograms in ggplot2.10 There are probably better ways to create these labels, but this works as a demonstration of the secondary x-axis.\nggplot(data=waedat,mapping=aes(x=tl)) +\n  geom_histogram(binwidth=10,boundary=0,closed=\"left\",\n                 color=\"black\",fill=\"gray50\") +\n  scale_y_continuous(name=\"Frequency\",expand=expansion(mult=c(0,0.025))) +\n  scale_x_continuous(name=\"Total Length (mm)\",breaks=scales::breaks_width(50)) +\n  theme_bw() +\n  theme(axis.text=element_text(color=\"black\"))\n\n\n\n\n\n\n\n\nThe values for the Gabelhouse length categories are obtained from psdVal() in FSA.\n\n( waepsd &lt;- FSA::psdVal(\"Walleye\",incl.zero=FALSE) )\n\n#R|      stock   quality preferred memorable    trophy \n#R|        250       380       510       630       760\n\n\nThese values are used to make the endpoints of the segment lines, noting that each stops 2 mm before the next segment starts so as to provide a visible break in the segments. A data frame of labels was created as above, except that the text for the labels came from the names provided with psdVal(). Also note that 810 (i.e., the end of the x-axis) was appended to the segment ends so that the “trophy” segment would have an end point.\n\nx2lns &lt;- data.frame(xstart=waepsd,xend=c(waepsd[-1]-2,810))\nx2lns\n\n#R|            xstart xend\n#R|  stock        250  378\n#R|  quality      380  508\n#R|  preferred    510  628\n#R|  memorable    630  758\n#R|  trophy       760  810\n\nx2lbls &lt;- x2lns |&gt;\n  rowwise() |&gt;\n  summarize(x=mean(c(xstart,xend))) |&gt;\n  ungroup() |&gt;\n  mutate(lbl=names(waepsd))\nx2lbls\n\n#R|  # A tibble: 5 × 2\n#R|        x lbl      \n#R|    &lt;dbl&gt; &lt;chr&gt;    \n#R|  1   314 stock    \n#R|  2   444 quality  \n#R|  3   569 preferred\n#R|  4   694 memorable\n#R|  5   785 trophy\n\n\nMaking room for the secondary labels is a little different here because the x-axis title is maintained. Thus, instead of changing plot.margin() as above, more space is added by increasing the top margin of the x-axis title with axis.title.x=.\n\nggplot(data=waedat,mapping=aes(x=tl)) +\n  geom_histogram(binwidth=10,boundary=0,closed=\"left\",\n                 color=\"black\",fill=\"gray50\") +\n  scale_y_continuous(name=\"Frequency\",expand=expansion(mult=c(0,0.025))) +\n  scale_x_continuous(name=\"Total Length (mm)\",breaks=scales::breaks_width(50),\n                     expand=expansion(mult=0.025)) +\n  theme_bw() +\n  theme(axis.text=element_text(color=\"black\"),\n        axis.title.x=element_text(margin=margin(t=3,unit=\"lines\")))\n\n\n\n\n\n\n\n\nThis space is then filled with the secondary axis labels as before.\n\nggplot(data=waedat,mapping=aes(x=tl)) +\n  geom_histogram(binwidth=10,boundary=0,closed=\"left\",\n                 color=\"black\",fill=\"gray50\") +\n  scale_y_continuous(name=\"Frequency\",expand=expansion(mult=c(0,0.025))) +\n  scale_x_continuous(name=\"Total Length (mm)\",breaks=scales::breaks_width(50),\n                     expand=expansion(mult=0.025)) +\n  theme_bw() +\n  theme(axis.text=element_text(color=\"black\"),\n        axis.title.x=element_text(margin=margin(t=3,unit=\"lines\"))) +\n  coord_cartesian(xlim=c(250,800),clip=\"off\") +\n  geom_segment(data=x2lns,mapping=aes(x=xstart,xend=xend),y=-7,yend=-7,\n               linewidth=0.75) +\n  geom_text(data=x2lbls,mapping=aes(x=x,label=lbl),y=-10,\n            size=11/.pt)"
  },
  {
    "objectID": "blog/posts/2023-3-28_McCarricketal2022_Fig6/index.html",
    "href": "blog/posts/2023-3-28_McCarricketal2022_Fig6/index.html",
    "title": "McCarrick et al. (2022) Back-Calculated TL Plots",
    "section": "",
    "text": "Series Note\n\n\n\nThis is the fifth, and last, of several posts related to McCarrick et al. (2022). I thank the authors for making their data available with their publication.\n\n\n\nIntroduction\nMcCarrick et al. (2022) examined the population dynamics of Yellowstone Cutthroat Trout (Oncorhynchus clarkii bouvieri) in Henrys Lake, Idaho over a nearly two decade period. Their Figure 6 showed the back-calculated total length of both Cutthroat Trout at three ages separated by decade and their Figure 7 showed the mean (and SE) back-calculated TL at age for two time periods. I use ggplot2 here to recreate both figures.\nThe following packages are loaded for use below. A few functions from each of readxl, FSA, scales, and lemon are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\n\n\n\n\n\n\n\nNote\n\n\n\nYou will see below that I could recreate the structure but not the exact results of the author’s Figure 6 or Figure 7. This may be due to issues present in the data provided with the publication that I discuss below, but it could also be that my data wrangling differs from theirs. I defer to the authors here as they are the experts with their data; I am just an interloper. I actually appreciate these data issues from an educational perspective as they provide rich opportunities to demonstrate a variety of techniques with “real” “messy” data.\n\n\n \n\n\nData Wrangling\n\nReading Excel File and Handling Some Initial Issuees\nMcCarrick et al. (2022) provided raw data for these figures as an Excel file in their Data Supplement S2. An initial glance at the Excel file revealed that every other line in the file was essentially a header line for the annular measurements on the scales or otoliths.1 Fortunately, when these data were read in below, those lines appeared as missing data for the age-at-capture variable (as well as several others). Thus, I immediately removed rows with missing values in the age-at-capture variable. The variable names in Excel were also longer than I prefer so I renamed the variables that I chose to retain.\n1 This was not immediately obvious in the Excel file as a filter had been applied to hide those rows. I had to remove the filter in Excel to see the issue.\ndat &lt;- readxl::read_excel(\"Download.xlsx\") |&gt;\n  select(Year,ID=Fish_Number,Structure,capAge=Age_at_Capture,capTL=Total_Length_mm,\n         edge,starts_with(\"annulus\")) |&gt;\n  filter(!is.na(capAge))\n\nFSA::headtail(dat)\n\n#R|       Year   ID Structure capAge capTL edge annulus1 annulus2 annulus3 annulus4\n#R|  1    2020  143   Otolith      3   402  982      464      754      982       NA\n#R|  2    2020  122   Otolith      2   266  692      428      692       NA       NA\n#R|  3    2020 1599   Otolith      2   452  994      702      994       NA       NA\n#R|  3257 1987   NA     Scale      2   240  435      155      352       NA       NA\n#R|  3258 1987   NA     Scale      3   305  531      137      331      431       NA\n#R|  3259 1987   NA     Scale      3   453  695      145      338      537       NA\n#R|       annulus5 annulus6 annulus7 annulus8 annulus9 annulus10 annulus11\n#R|  1          NA       NA       NA       NA       NA        NA        NA\n#R|  2          NA       NA       NA       NA       NA        NA        NA\n#R|  3          NA       NA       NA       NA       NA        NA        NA\n#R|  3257       NA       NA       NA       NA       NA        NA        NA\n#R|  3258       NA       NA       NA       NA       NA        NA        NA\n#R|  3259       NA       NA       NA       NA       NA        NA        NA\n\n\nThese data are in what I call “one-fish-per-line” format. This is the “tidy” format for performing analyses on a fish, but it is not in a “tidy” format for performing analyses on lengths at specific ages. This will become more evident in the following sections.\nFor consideration below, note that Structure contains only Scale and Otolith.\n\nunique(dat$Structure)\n\n#R|  [1] \"Otolith\" \"Scale\"\n\n\n\n\nBack-Calculation\nThe authors used the Dahl-Lea method of back-calculation for otoliths and the Fraser-Lee method for scales.2 The length adjustment term in the Fraser-Lee method came from the intercept of the regression of length-at-capture on scale radius-at-capture. This regression is performed using the “per fish” data in dat, but filtered to only the data from scales. The intercept is extracted from the regression results and stored in a for use below.\n2 I assume the reader is familiar with back-calculation methods. If not see here or examine the methods in McCarrick et al. (2022).\nLonR &lt;- lm(capTL~edge,data=filter(dat,Structure==\"Scale\"))\n( a &lt;- coef(LonR)[[\"(Intercept)\"]] )\n\n#R|  [1] 54.99758\n\n\nBack-calculating length at a previous age requires the data to be in a “one-annulus-per-line” format. For this purpose, the data in dat is considered “wide” (annuli are in multiple columns of each row) and need to be “pivoted” to “long” format with pivot_longer(). The columns that contain the annular measurements are given to cols=, values_to= gets a name for the column to contain these annular measurements, and names_to= gets a name for the column to contain the “label” for the annular measurements. The new bcAge variable (from names_to=) will contain the old columns names (i.e., “annulus 1”, “annulus 2”, etc.) by default. names_prefix= is used below to remove “annulus” from these labels and leave just the numeric age labels (e.g., “1”, “2”, etc.). The age “labels” are converted to numeric values with as.numeric() in mutate(). Finally, many rows in the new Radius variable will be missing because annular measurements were not made for ages older than the age-at-capture for the fish (e.g., Radius will be missing for all ages greater than 3 for an age-3 fish). These rows are removed with filter() below.\n\ndat2 &lt;- dat |&gt;\n  pivot_longer(cols=annulus1:annulus11,values_to=\"Radius\",\n               names_to=\"bcAge\",names_prefix=\"annulus\") |&gt;\n  mutate(bcAge=as.numeric(bcAge)) |&gt;\n  filter(!is.na(Radius))\n\nFSA::headtail(dat2)\n\n#R|       Year  ID Structure capAge capTL edge bcAge Radius\n#R|  1    2020 143   Otolith      3   402  982     1    464\n#R|  2    2020 143   Otolith      3   402  982     2    754\n#R|  3    2020 143   Otolith      3   402  982     3    982\n#R|  8754 1987  NA     Scale      3   453  695     1    145\n#R|  8755 1987  NA     Scale      3   453  695     2    338\n#R|  8756 1987  NA     Scale      3   453  695     3    537\n\n\nThis data frame is now prepared to back-calculate lengths at previous ages from the measurements in Radius, the structure size-at-capture in edge, and the fish’s length at capture in capTL. This calculation is somewhat complicated by the fact that different back-calculation equations are used depending on the structure examined. This is handled below using case_when() within mutate().\n\ndat2 &lt;- dat2 |&gt;\n  mutate(bcTL=case_when(\n    Structure==\"Scale\" ~ ((capTL-a)/edge)*Radius+a,\n    Structure==\"Otolith\" ~ capTL*Radius/edge\n  ))\n\nFSA::headtail(dat2)\n\n#R|       Year  ID Structure capAge capTL edge bcAge Radius     bcTL\n#R|  1    2020 143   Otolith      3   402  982     1    464 189.9470\n#R|  2    2020 143   Otolith      3   402  982     2    754 308.6640\n#R|  3    2020 143   Otolith      3   402  982     3    982 402.0000\n#R|  8754 1987  NA     Scale      3   453  695     1    145 138.0341\n#R|  8755 1987  NA     Scale      3   453  695     2    338 248.5585\n#R|  8756 1987  NA     Scale      3   453  695     3    537 362.5189\n\n\nAt this point, I felt the need to see if these calculations “worked.” I made a plot of back-calculated length-at-age and noticed that most results were reasonable, but there were a handful of VERY large back-calculated lengths. This seemed like date error rather than systematic calculation error. I was further concerned that there may be errors of very low back-calculated lengths; thus, I used a log scale for the y-axis.\n\nggplot(data=dat2,mapping=aes(x=bcAge,y=bcTL,color=Structure)) +\n  geom_jitter(width=0.2,height=0,alpha=0.5) +\n  scale_y_continuous(trans=\"log10\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe large back-calculated lengths were from four individual fish, three of which had lengths-at-capture that were clearly errors (i.e., capTL over 2000 mm). Additionally, at least two of the small back-calculated lengths were from errors in length-at-capture (i.e., capTL less that 50 mm). A few of the other problems appeared related to bad edge measurements.\n\ndat2 |&gt; filter(bcTL&gt;1000)\n\n#R|  # A tibble: 13 × 9\n#R|      Year    ID Structure capAge capTL  edge bcAge Radius  bcTL\n#R|     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#R|   1  2012  2055 Otolith        2  2778   665     1    394 1646.\n#R|   2  2012  2055 Otolith        2  2778   665     2    665 2778 \n#R|   3  2009   208 Otolith        3  3370   740     1    344 1567.\n#R|   4  2009   208 Otolith        3  3370   740     2    547 2491.\n#R|   5  2009   208 Otolith        3  3370   740     3    740 3370 \n#R|   6  2008    69 Otolith        4  4692   925     1    352 1785.\n#R|   7  2008    69 Otolith        4  4692   925     2    531 2693.\n#R|   8  2008    69 Otolith        4  4692   925     3    726 3683.\n#R|   9  2008    69 Otolith        4  4692   925     4    925 4692 \n#R|  10  2004  1098 Otolith        4   460   100     1    333 1532.\n#R|  11  2004  1098 Otolith        4   460   100     2    619 2847.\n#R|  12  2004  1098 Otolith        4   460   100     3    811 3731.\n#R|  13  2004  1098 Otolith        4   460   100     4   1000 4600\n\ndat2 |&gt; filter(bcTL&lt;80)\n\n#R|  # A tibble: 10 × 9\n#R|      Year    ID Structure capAge capTL  edge bcAge Radius  bcTL\n#R|     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#R|   1  2014    19 Otolith        2   145   610     1    313 74.4 \n#R|   2  2008   257 Otolith        2    15   667     1    321  7.22\n#R|   3  2008   257 Otolith        2    15   667     2    667 15   \n#R|   4  2006   149 Otolith        3    43   938     1    363 16.6 \n#R|   5  2006   149 Otolith        3    43   938     2    636 29.2 \n#R|   6  2006   149 Otolith        3    43   938     3    938 43   \n#R|   7  2007   319 Otolith        2   302  7110     1    374 15.9 \n#R|   8  2007   319 Otolith        2   302  7110     2    710 30.2 \n#R|   9  2004   135 Otolith        3   160   986     1    482 78.2 \n#R|  10  2004   218 Otolith        3   190   841     1    305 68.9\n\n\nI examined the length-at-capture and radius-at-capture data more closely.\n\nggplot(data=dat,mapping=aes(x=capTL)) +\n  geom_histogram(bins=100) +\n  scale_x_continuous(trans=\"log10\") +\n  coord_cartesian(ylim=c(0,10))\n## &lt;100 and &gt;1000 seem to be errors\n\nggplot(data=dat,mapping=aes(x=edge)) +\n  geom_histogram(bins=100) +\n  scale_x_continuous(trans=\"log10\") +\n  coord_cartesian(ylim=c(0,10))\n## &lt;200? and &gt;1500 seem to be errors\n\n\n\n\n\n\n\n\n\n\nFrom this analysis, it seems that lengths-at-capture less than 100 mm and greater than 1000 mm are data errors. In addition, it seems that edge measurements of less than 200 and greater than 1500 are also data errors. If I had access to the original records, I would attempt to determine if these are data entry errors and fix them. I don’t have that access, so I deleted fish with these measurements from the dat data frame.\n\ndat &lt;- dat |&gt;\n  filter(capTL&gt;100,capTL&lt;1000) |&gt;\n  filter(edge&gt;200,edge&lt;3000)\n\nBecause the original dat data frame has now been altered, the regression and creation of dat2 above needed to be repeated before continuing. I did not show the repeated code here.\n\n\n\n\n\n\n\n\n#R|       Year  ID Structure capAge capTL edge bcAge Radius     bcTL\n#R|  1    2020 143   Otolith      3   402  982     1    464 189.9470\n#R|  2    2020 143   Otolith      3   402  982     2    754 308.6640\n#R|  3    2020 143   Otolith      3   402  982     3    982 402.0000\n#R|  8732 1987  NA     Scale      3   453  695     1    145 139.1931\n#R|  8733 1987  NA     Scale      3   453  695     2    338 249.3108\n#R|  8734 1987  NA     Scale      3   453  695     3    537 362.8518\n\n\n \n\n\n\nRecreating Figure 6\nA decade variable is added to dat2 and the data was reduced to only age-2 to age-4 fish, as those were the only ages used in Figure 6.\n\ndat3 &lt;- dat2 |&gt;\n  mutate(Decade=floor(Year/10)*10,\n         Decade=ifelse(Decade==2020,2010,Decade),\n         Decade=factor(Decade)) |&gt;\n  filter(bcAge&gt;=2,bcAge&lt;=4)\n\nFigure 6 is a boxplot that can be produced with geom_boxplot(), with Decade mapped to the x-axis and bcTL mapped to the y-axis. Note that the x-axis variables should not be numeric, which is why Decade was converted to a factor above. Because of this, the x-axis is modified with scale_x_discrete() rather than scale_x_continuous(). The rest of functions used here were described in the previous posts related to McCarrick et al. (2022).\n\nggplot(data=dat3,mapping=aes(x=Decade,y=bcTL)) +\n  geom_boxplot() +\n  geom_text(mapping=aes(label=paste(\"Age\",bcAge)),check_overlap=TRUE,\n            x=Inf,y=Inf,vjust=1.2,hjust=1.1) +\n  scale_y_continuous(name=\"Back-calculated length (mm)\",\n                     limits=c(100,700),breaks=scales::breaks_width(100),\n                     expand=expansion(mult=0)) +\n  scale_x_discrete(name=\"Decade\") +\n  lemon::facet_rep_wrap(vars(bcAge),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.text=element_blank())\n\n\n\n\n\n\n\n\n \n\n\nRecreating Figure 7\nFigure 7 is a bar chart based on summarized data, which needs to be constructed from dat2. Below, dat2 is restricted to only otoliths, the two periods shown in Figure 7 are created and converted to a factor, and then the sample size and the mean and standard error of back-calculated total lengths are calculated by age for each period. .drop=FALSE is used in group_by() so that the same number of ages will be shown for both periods, though both periods don’t have the same range of ages.3 This will make the bars in the plot before appear in their proper locations even if a result is missing for the other period.\n3 So, some ages will have NA for the mean and SE.\ndat4 &lt;- dat2 |&gt; \n  filter(Structure==\"Otolith\") |&gt;\n  mutate(Period=ifelse(Year&lt;2011,\"2002-2010\",\"2011-2020\"),\n         Period=factor(Period,levels=c(\"2002-2010\",\"2011-2020\")),\n         bcAge=factor(bcAge)) |&gt;\n  group_by(Period,bcAge,.drop=FALSE) |&gt;\n  summarize(n=n(),\n            mnTL=mean(bcTL,na.rm=TRUE),\n            seTL=FSA::se(bcTL,na.rm=TRUE)) |&gt;\n  ungroup()\nFSA::headtail(dat4)\n\n#R|        Period bcAge    n     mnTL       seTL\n#R|  1  2002-2010     1 1190 174.2867  0.9124273\n#R|  2  2002-2010     2 1048 302.2455  1.2765006\n#R|  3  2002-2010     3  455 405.1318  2.2456338\n#R|  20 2011-2020     9    4 498.3248 29.0737028\n#R|  21 2011-2020    10    4 534.1058 26.7635020\n#R|  22 2011-2020    11    1 524.0000         NA\n\n\nThese summary data are then ready to make a bar chart, very similar to what was done in this post. Note here, though, that the width of the bars (in geom_col()) was reduced to create more room between the bars at each age. The width= of dodging in position_dodge() had to then be reduced a commensurate amount so that the bars at each age would touch. Finally, legend.key.width= and legend.key.height= were used in theme() to make the elongated “keys” in the legend as was done in Figure 7.\n\npd &lt;- position_dodge(width=0.7)\n\nggplot(dat=dat4,mapping=aes(x=bcAge,y=mnTL,fill=Period)) +\n  geom_errorbar(mapping=aes(ymin=mnTL-seTL,ymax=mnTL+seTL),\n                position=pd,width=0.5) +\n  geom_col(position=pd,color=\"black\",width=0.7) +\n  scale_x_discrete(name=\"Age (years)\") +\n  scale_y_continuous(name=\"Mean back-calculated length (mm)\",\n                     limits=c(0,600),breaks=scales::breaks_width(100),\n                     expand=expansion(mult=0)) +\n  scale_fill_manual(values=c(\"2002-2010\"=\"gray10\",\"2011-2020\"=\"gray70\")) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.text=element_blank(),\n        legend.position=c(0,1),\n        legend.justification=c(-0.1,1.1),\n        legend.title=element_blank(),\n        legend.key.width=unit(15,units=\"mm\"),\n        legend.key.height=unit(3,units=\"mm\"),\n        legend.text=element_text(size=11))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI am not a fan of showing “error bars” that are only one SE. This is akin to showing a 68% confidence interval, which is not something that someone would usually do. I did that here because that is what was done in the published Figure 7, but I urge you not to do this … show a proper confidence interval instead.\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nMcCarrick, D. K., J. C. Dillon, B. High, and M. C. Quist. 2022. Population dynamics of Yellowstone Cutthroat Trout in Henrys Lake, Idaho. Journal of Fish and Wildlife Management 13(1):169–181.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {McCarrick Et Al. (2022) {Back-Calculated} {TL} {Plots}},\n  date = {2023-03-28},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-3-28_McCarricketal2022_Fig6},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, March 28. McCarrick et al. (2022) Back-Calculated TL\nPlots. https://fishr-core-team.github.io/fishR//blog/posts/2023-3-28_McCarricketal2022_Fig6."
  },
  {
    "objectID": "blog/posts/2023-3-26_McCarricketal2022_Fig4/index.html",
    "href": "blog/posts/2023-3-26_McCarricketal2022_Fig4/index.html",
    "title": "McCarrick et al. (2022) Wr Plot",
    "section": "",
    "text": "Series Note\n\n\n\nThis is the third of several posts related to McCarrick et al. (2022).\n\n\n\nIntroduction\nMcCarrick et al. (2022) examined the population dynamics of Yellowstone Cutthroat Trout (Oncorhynchus clarkii bouvieri) in Henrys Lake, Idaho over a nearly two decade period. Their Figure 4 showed mean (with confidence interval) relative weight (Wr) for various length categories of Cutthroat Trout across years. I use ggplot2 to recreate that figure here.\nThe following packages are loaded for use below. A few functions from each of lubridate, FSA, plyr, scales, gghrx, lemon, and ggtext are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\n\nMcCarrick et al. (2022) computed what I am calling an overall PSD1 and what are called “incremental” PSD indices. I assume that these are familar to most fisheries scientists, but they are described in more detail in Chapter 8 of Ogle (2016).2\n1 This is the most common PSD measure.2 Also see Blackwell et al. (2000). \n\n\nData Wrangling\n\nIndividual Fish Data Frame\nMcCarrick et al. (2022) provided raw data for Figure 2 as an Excel file in their Data Supplement S1. The same data wrangling, up to where catch-per-unit-effort was calculated, used in this previous post is used here3 and, thus, will not be discussed in detail.\n3 And in this post\ndat &lt;- readxl::read_xlsx(\"../2023-3-22_McCarricketal2022_Fig2/download.xlsx\",\n                         na=c(\"\",\"??\",\"QTY =\",\"QTY=\",\"UNK\",\"NO TAG\"),\n                         col_types=c(\"date\",\"numeric\",\"text\",\n                                     \"numeric\",\"numeric\",\"text\")) |&gt;\n  mutate(year=lubridate::year(Date),\n         year=ifelse(year==1905,2002,year)) |&gt;\n  filter(!is.na(year)) |&gt;\n  select(species=Species,year,length,weight)  |&gt;\n  mutate(species=case_when(\n    species %in% c(\"YCT\",\"Yct\") ~ \"YCT\",\n    species %in% c(\"UTC\",\"CHB\",\"CHUB\") ~ \"UTC\",\n    TRUE ~ species\n  )) |&gt;\n  filter(species %in% c(\"YCT\",\"UTC\")) |&gt;\n  mutate(species2=plyr::mapvalues(species,\n                                  from=c(\"YCT\",\"UTC\"),\n                                  to=c(\"Cutthroat Trout\",\"Utah Chub\"))) |&gt;\n  mutate(gcat=FSA::psdAdd(len=length,species=species2))\n\nFSA::headtail(dat)\n\n#R|        species year length weight        species2     gcat\n#R|  1         YCT 2002    150     NA Cutthroat Trout substock\n#R|  2         YCT 2002    160     NA Cutthroat Trout substock\n#R|  3         YCT 2002    160     NA Cutthroat Trout substock\n#R|  19900     YCT 2020    391    550 Cutthroat Trout  quality\n#R|  19901     YCT 2020    284    229 Cutthroat Trout    stock\n#R|  19902     YCT 2020    440    853 Cutthroat Trout  quality\n\n\nThe relative weigt for each fish needs to be added to this data frame, preferably with wrAdd() from FSA. Note, however, that there are two standard weight equations for Cutthroat Trout, one for lentic and one for lotic populations. We want to make sure to use the one for lentic populations here and, thus, cannot use the species name found in the data frame within wrAdd(). Note below that the data frame was also reduced to just Cutthroat Trout for which a weight was recorded, and the two species variables were dropped (just to simplify the output).\n\ndat &lt;- dat |&gt;\n  filter(species==\"YCT\",!is.na(weight)) |&gt;\n  mutate(wr=FSA::wrAdd(weight,length,spec=\"Cutthroat Trout (lentic)\")) |&gt;\n  select(-starts_with(\"species\"))\nFSA::headtail(dat)\n\n#R|       year length weight     gcat        wr\n#R|  1    2004    174     54 substock 102.34315\n#R|  2    2004    227    130    stock 108.45450\n#R|  3    2004    305    380    stock 127.41903\n#R|  5208 2020    391    550  quality  85.68508\n#R|  5209 2020    284    229    stock  95.69629\n#R|  5210 2020    440    853  quality  92.31113\n\n\n\n\nWr Summary Data Frame\nSummary statistics of relative weight for all fish is computed below.\n\nwr_all &lt;- dat |&gt;\n  group_by(year) |&gt;\n  summarize(n=n(),\n            mn_wr=mean(wr,na.rm=TRUE),\n            sd_wr=sd(wr,na.rm=TRUE),\n            se_wr=FSA::se(wr,na.rm=TRUE))\nFSA::headtail(wr_all)\n\n#R|     year   n     mn_wr     sd_wr     se_wr\n#R|  1  2004 300 116.50388 16.405045 0.9650026\n#R|  2  2005 305 103.70058 14.860984 0.8509375\n#R|  3  2006 269 107.66011 20.258635 1.2351908\n#R|  15 2018  76 107.05779 25.899713 2.9709008\n#R|  16 2019 219  97.20635  7.485436 0.5058189\n#R|  17 2020 198  94.78333  8.648857 0.6146475\n\n\nSummary statistics of relative weight for each length category is computed below.\n\nwr_gcat &lt;- dat |&gt;\n  group_by(year,gcat) |&gt;\n  summarize(n=n(),\n            mn_wr=mean(wr,na.rm=TRUE),\n            sd_wr=sd(wr,na.rm=TRUE),\n            se_wr=FSA::se(wr,na.rm=TRUE)) |&gt;\n  ungroup()\nFSA::headtail(wr_gcat)\n\n#R|     year      gcat   n     mn_wr     sd_wr     se_wr\n#R|  1  2004  substock 110 103.54857 13.477955 1.3545854\n#R|  2  2004     stock 130 123.09751 11.535665 1.0117447\n#R|  3  2004   quality  34 127.50055 12.158543 2.0851728\n#R|  68 2020     stock  52  95.40331 10.326851 1.4320766\n#R|  69 2020   quality 132  94.78427  8.119609 0.7067213\n#R|  70 2020 preferred  13  92.28008  6.826683 1.8933812\n\n\nMcCarrick et al. (2022) only plotted fish in the “stock”, “quality”, and “preferred” length categories in Figure 4; thus, only these length categories are retained below.\n\nwr_gcat &lt;- wr_gcat |&gt;\n  filter(gcat %in% c(\"stock\",\"quality\",\"preferred\"))\n\nThe summaries for all fish and fish by length categories are row-bound (i.e., stacked) together to form an overall summary data frame. Because the data frame for all fish did not have a gcat variable, that variable will be populated with NA when the two data frames are bound. The ifelse() below converts these NA values to All, before gcat is factored with levels ordered as they would appear in Figure 4 and with more descriptive labels. Finally, for aesthetic purposes, I moved gcat to be the first variable and sorted the results by year within length category.\n\nwr_dat &lt;- bind_rows(wr_all,wr_gcat) |&gt;\n  mutate(gcat=ifelse(is.na(gcat),\"All\",as.character(gcat)),\n         gcat=factor(gcat,levels=c(\"All\",\"stock\",\"quality\",\"preferred\"),\n                     labels=c(\"All fish\",\"Stock to Quality\",\n                              \"Quality to Preferred\",\"Preferred to Memorable\"))) |&gt;\n  relocate(gcat) |&gt;\n  arrange(gcat,year)\nFSA::headtail(wr_dat)\n\n#R|                       gcat year   n     mn_wr     sd_wr     se_wr\n#R|  1                All fish 2004 300 116.50388 16.405045 0.9650026\n#R|  2                All fish 2005 305 103.70058 14.860984 0.8509375\n#R|  3                All fish 2006 269 107.66011 20.258635 1.2351908\n#R|  66 Preferred to Memorable 2018  22  98.79180 17.476556 3.7260144\n#R|  67 Preferred to Memorable 2019  17  98.61326  8.666677 2.1019780\n#R|  68 Preferred to Memorable 2020  13  92.28008  6.826683 1.8933812\n\n\nFinally, the lower and upper confidence intervals for each mean are added to the data frame using normal distribution theory.4\n4 qt() returns a the 97.5th percent critical value from a t-distribution with df degrees-of-freedom.\nwr_dat &lt;- wr_dat |&gt;\n  mutate(lci=mn_wr-qt(0.975,df=n-1)*se_wr,\n         uci=mn_wr+qt(0.975,df=n-1)*se_wr)\nFSA::headtail(wr_dat)\n\n#R|                       gcat year   n     mn_wr     sd_wr     se_wr       lci\n#R|  1                All fish 2004 300 116.50388 16.405045 0.9650026 114.60482\n#R|  2                All fish 2005 305 103.70058 14.860984 0.8509375 102.02610\n#R|  3                All fish 2006 269 107.66011 20.258635 1.2351908 105.22820\n#R|  66 Preferred to Memorable 2018  22  98.79180 17.476556 3.7260144  91.04313\n#R|  67 Preferred to Memorable 2019  17  98.61326  8.666677 2.1019780  94.15727\n#R|  68 Preferred to Memorable 2020  13  92.28008  6.826683 1.8933812  88.15476\n#R|          uci\n#R|  1  118.4029\n#R|  2  105.3750\n#R|  3  110.0920\n#R|  66 106.5405\n#R|  67 103.0693\n#R|  68  96.4054\n\n\nThis data frame, now called wr_dat, is ready for recreating Figure 4.\n \n\n\n\nRecreating Figure 4\nFigure 4 is a simple bar plot facetted across years similar to the PSD plot in this previous post. Confidence intervals are added as described in that same post. Thus, many of the details below are not discussed further here. However, there are two new things to consider here.\nFirst, the y-axis in Figure 4 in (mccarricketal_2020?) is limited from 70 to 140. If limits=c(70,140) is used in scale_y_continuous() as described in this post the bars will be removed because their “data” extends to zero.5 Thus, the limits of the y-axis must be set with ylim= in coord_cartesian() to preserved the bars.\n5 Data points outside axis limits created in this way are treated as missing and are removed from the figure.Second, the y-axis label in Figure 4 in (mccaricketal_2020?) is italicized with a subscript “r”. In the markdown language, asterisks are used to denoted italics and the tilde is used to create a subscript, which can be seen in name= in scale_y_continuous(). However, to make this markdown code render properly axis.title.y= in theme must be set to element_markdown() from ggtext.\n\nggplot(data=wr_dat,mapping=aes(x=year,y=mn_wr)) +\n  geom_errorbar(mapping=aes(ymin=lci,ymax=uci),width=0.25) +\n  geom_col(color=\"black\",fill=\"gray70\",width=1) +\n  geom_text(mapping=aes(label=gcat),x=Inf,y=Inf,vjust=1.25,hjust=1.05,size=3,\n            check_overlap=TRUE) +\n  scale_y_continuous(name=\"*W~r~*\",expand=expansion(mult=0),\n                     breaks=scales::breaks_width(10)) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  coord_cartesian(ylim=c(70,140)) +\n  lemon::facet_rep_wrap(vars(gcat),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.background=element_blank(),\n        strip.text=element_blank(),\n        axis.title.y=ggtext::element_markdown())\n\n\n\n\n\n\n\n\n \n\n\nFurther Thoughts\n\nPoint-and-Lines Plot\nAs mentioned in this previous post I understand that these are the much derided “dynamite plots”. Personally, I find the bars distracting (so much gray with little purpose and starting at 70) and find a point-and-lines plot, possibly with a horizontal line at the common reference value of 100.\n\nggplot(data=wr_dat,mapping=aes(x=year,y=mn_wr)) +\n  geom_errorbar(mapping=aes(ymin=lci,ymax=uci),width=0.25) +\n  geom_line(linewidth=0.75,color=\"gray70\") +\n  geom_point(size=1) +\n  geom_hline(yintercept=100,color=\"gray30\",linetype=\"dashed\",linewidth=0.5) +\n  geom_text(mapping=aes(label=gcat),x=Inf,y=Inf,vjust=1.25,hjust=1.05,size=3,\n            check_overlap=TRUE) +\n  scale_y_continuous(name=\"*W~r~*\",limits=c(70,140),expand=expansion(mult=0),\n                     breaks=scales::breaks_width(10)) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  lemon::facet_rep_wrap(vars(gcat),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.background=element_blank(),\n        strip.text=element_blank(),\n        axis.title.y=ggtext::element_markdown())\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nReferences\n\nBlackwell, B., M. Brown, and D. Willis. 2000. Relative Weight (Wr) Status and Current Use in Fisheries Assessment and Management. Reviews in Fisheries Science 8:1–44.\n\n\nMcCarrick, D. K., J. C. Dillon, B. High, and M. C. Quist. 2022. Population dynamics of Yellowstone Cutthroat Trout in Henrys Lake, Idaho. Journal of Fish and Wildlife Management 13(1):169–181.\n\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {McCarrick Et Al. (2022) {Wr} {Plot}},\n  date = {2023-03-26},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-3-26_McCarricketal2022_Fig4},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, March 26. McCarrick et al. (2022) Wr Plot. https://fishr-core-team.github.io/fishR//blog/posts/2023-3-26_McCarricketal2022_Fig4."
  },
  {
    "objectID": "blog/posts/2023-3-22_McCarricketal2022_Fig2/index.html",
    "href": "blog/posts/2023-3-22_McCarricketal2022_Fig2/index.html",
    "title": "McCarrick et al. (2022) CPE Plot",
    "section": "",
    "text": "Series Note\n\n\n\nThis is the first of several posts related to McCarrick et al. (2022).\n\n\n\nIntroduction\nMcCarrick et al. (2022) examined the population dynamics of Yellowstone Cutthroat Trout (Oncorhynchus clarkii bouvieri) in Henrys Lake, Idaho over a nearly two decade period. Their Figure 2 shows the catch-per-unit-effort (CPE) of both Cutthroat Trout and Utah Chub (Gila atraria) by size categories across years. I use ggplot2 to recreate that figure here.\nThe following packages are loaded for use below. A few functions from each of lubridate, FSA, plyr, scales, gghrx, and lemon are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\nlibrary(patchwork)  # for arranging multiple plots\n\n\n\n\n\n\n\nNote\n\n\n\nYou will see below that I could recreate the structure but not the exact results of the author’s Figure 2. This may be due to issues present in the data provided with the publication that I discuss below, but it could also be that my data wrangling differs from theirs. I defer to the authors here as they are the experts with their day; I am just an interloper. I actually appreciate these data issues from an educational perspective as they provide rich opportunities to demonstrate a variety of techniques with “real” “messy” data.\n\n\n \n\n\nData Wrangling\n\nReading Excel File and Handling Dates\nMcCarrick et al. (2022) provided raw data for Figure 2 as an Excel file in their Data Supplement S1. Several “issues” were apparent on my first attempts to load these data.\n\nMissing data was recorded as missing cells, with “??”, and with “UNK”.\nThere were oddly placed strings in some data fields (e.g., “Qty=”, “Qty =”, and “No Tag”). [I treated these as missing data below.]\nDates were recorded as just year, full date with two-digit year, and full date with four-digit year. [More on this below.]\nInconsistent “species” abbreviations. [More on this below.]\nIncorrect length and weight measurements (e.g., &lt;10 mm, &gt;2000 mm). [I treated these “as is” (i.e., without correction) for this post.]\n\nThe missing data and oddly named cells were handled with na= below. In addition, the column types were specifically defined in col_types=, primarily so that the first column would be largely correctly read as a date. The year was then extracted from the Date variable using year() from lubridate. However, the dates recorded in the original file as only a year were interpreted as an “Excel date code” rather than a date and, thus, the year was not extracted correctly. Fortunately, this “date” format was only used for one year (2002) such that the extracted year was consistently “1905.” Thus, the “1905” was replaced with “2002” for the extracted year with ifelse() as shown below. Several years did not exist (because the date was not recorded) and were removed with filter(). Finally, the variables were rearranged slightly, Species was changed to species for consistency with other variables, and the long effort variable was changed to the shorter netNights.1\n1 Note that this code still produced a large number of warnings, all related to the “2002” “date”.\ndat &lt;- readxl::read_xlsx(\"download.xlsx\",\n                         na=c(\"\",\"??\",\"QTY =\",\"QTY=\",\"UNK\",\"NO TAG\"),\n                         col_types=c(\"date\",\"numeric\",\"text\",\n                                     \"numeric\",\"numeric\",\"text\")) |&gt;\n  mutate(year=lubridate::year(Date),\n         year=ifelse(year==1905,2002,year)) |&gt;\n  filter(!is.na(year)) |&gt;\n  select(species=Species,year,length,weight,netNights=`number of net nights`)\nFSA::headtail(dat)\n\n#R|        species year length weight netNights\n#R|  1         YCT 2002    150     NA        18\n#R|  2         YCT 2002    160     NA        18\n#R|  3         YCT 2002    160     NA        18\n#R|  23971     YCT 2020    391    550        30\n#R|  23972     YCT 2020    284    229        30\n#R|  23973     YCT 2020    440    853        30\n\n\n\n\nNew Effort Data Frame\nThe effort (i.e., netNights) variable seems out-of-place in the sense that each row of this data frame is a single fish. I suspected that this effort is actually for a year and was repeated for each fish captured in the same year. To examine my suspicion, I computed the minimum and maximum effort (and the difference between the two) for each year.\n\ndat |&gt;\n  group_by(year) |&gt;\n  summarize(minF=min(netNights,na.rm=TRUE),\n            maxF=max(netNights,na.rm=TRUE),\n            diff=maxF-minF)\n\n#R|  # A tibble: 19 × 4\n#R|      year  minF  maxF  diff\n#R|     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#R|   1  2002    18    18     0\n#R|   2  2003    18    18     0\n#R|   3  2004    50    50     0\n#R|   4  2005    46    46     0\n#R|   5  2006    48    48     0\n#R|   6  2007    44    44     0\n#R|   7  2008    12    12     0\n#R|   8  2009    23    23     0\n#R|   9  2010    50    50     0\n#R|  10  2011    30    30     0\n#R|  11  2012    50    50     0\n#R|  12  2013    50    50     0\n#R|  13  2014    48    48     0\n#R|  14  2015    50    50     0\n#R|  15  2016    48    48     0\n#R|  16  2017    50    50     0\n#R|  17  2018    50    50     0\n#R|  18  2019    50    50     0\n#R|  19  2020    30    50    20\n\n\nThe minimum and maximum efforts are the same for each, with the exception of 2020. I suspect that this is a typo. A quick table of efforts for 2020 shows that the vast majority are 30, rather than 50, net-nights. Further, my initial analysis with these data resulted in CPE values for 2020 that were lower than what was shown in McCarrick et al. (2022). Thus, I am going to assume that 30 net-nights of effort was used in 2020 (see how this is corrected below).\n\nxtabs(~netNights,data=filter(dat,year==2020))\n\n#R|  netNights\n#R|   30  50 \n#R|  865   9\n\n\nFor my purposes, I am going to create a new data frame of annual levels of effort by finding the minimum netNights in each year. The minimum is used here because it is the actual effort for all years that properly had a constant netNights entered and it is the assumed correct netNights for 2020 when a constant value was not entered.\n\neff &lt;- dat |&gt;\n  group_by(year) |&gt;\n  summarize(netNights=min(netNights,na.rm=TRUE))\neff\n\n#R|  # A tibble: 19 × 2\n#R|      year netNights\n#R|     &lt;dbl&gt;     &lt;dbl&gt;\n#R|   1  2002        18\n#R|   2  2003        18\n#R|   3  2004        50\n#R|   4  2005        46\n#R|   5  2006        48\n#R|   6  2007        44\n#R|   7  2008        12\n#R|   8  2009        23\n#R|   9  2010        50\n#R|  10  2011        30\n#R|  11  2012        50\n#R|  12  2013        50\n#R|  13  2014        48\n#R|  14  2015        50\n#R|  15  2016        48\n#R|  16  2017        50\n#R|  17  2018        50\n#R|  18  2019        50\n#R|  19  2020        30\n\n\nThe netNights variables is no longer needed in the individual fish data frame.\n\ndat &lt;- dat |&gt;\n  select(-netNights)\n\n\n\nFiltering to Two Species\nFigure 2 in McCarrick et al. (2022) only displays results for Cutthroat Trout and Utah Chub. Thus, the data frame should be reduced to just these species. First, a look at how species are identified.\n\nunique(dat$species)\n\n#R|   [1] \"YCT\"     \"BKT\"     \"HYB\"     \"CHB\"     \"RSS\"     \"UTC\"     \"KOK\"    \n#R|   [8] \"BRT\"     \"CHUB\"    NA        \"HB\"      \"Yct\"     \"NO FISH\" \"SCU\"\n\n\nFrom the published paper it is apparent that “YCT” is for Yellowstone Cutthroat Trout and “UTC” is for Utah Chub. But note the “Yct” and the “CHB” and “CHUB”. A little closer look at just these abbreviations by year suggests that “Yct” is a “one-off” typo, but that “CHB” and “CHUB” appeared to be used in some years when “UTC” was mostly not used. This suggested to me that “CHB” and “CHUB” were likely synonyms for “UTC”.\n\nxtabs(~year+species,data=filter(dat,species %in% c(\"YCT\",\"Yct\",\"UTC\",\"CHB\",\"CHUB\")))\n\n#R|        species\n#R|  year    CHB CHUB  UTC  Yct  YCT\n#R|    2002    0    0    0    0   55\n#R|    2003   84    0    2    0  162\n#R|    2004    0    0  728    0  323\n#R|    2005    0    0  294    0  305\n#R|    2006    0    0  424    0  269\n#R|    2007    0    0  895    0  658\n#R|    2008    0  606    0    0  100\n#R|    2009    0    0  512    0   91\n#R|    2010    0    0    0    0  502\n#R|    2011    0  440    0    0  374\n#R|    2012    0    0  692    0  500\n#R|    2013    0    0 1858    0  478\n#R|    2014    0    0 1767    1  475\n#R|    2015    0    0 1660    0  254\n#R|    2016    0    0 1078    0  265\n#R|    2017    0    0 1025    0  149\n#R|    2018    0    0 1273    0   76\n#R|    2019    0    0  696    0  219\n#R|    2020    0    0  414    0  198\n\n\nThus, species is modified below by combining “YCT” and “Yct” into “YCT”, and combining “UTC”, “CHB”, and “CHUB” into “UTC”.2 The data frame was subsequently reduced to just these two species.\n2 And leaving all other species “names” as-is.\ndat &lt;- dat |&gt;\n  mutate(species=case_when(\n    species %in% c(\"YCT\",\"Yct\") ~ \"YCT\",\n    species %in% c(\"UTC\",\"CHB\",\"CHUB\") ~ \"UTC\",\n    TRUE ~ species\n    )\n  ) |&gt;\n  filter(species %in% c(\"YCT\",\"UTC\"))\n\n# just checking\nunique(dat$species)\n\n#R|  [1] \"YCT\" \"UTC\"\n\n\n\n\nAdding Gabelhouse Length Categories\nFigure 2 separates the data into panels based on Gabelhouse length categories (i.e., “Stock”, “Quality”, etc.). These categories can be efficiently added to the data frame using psdAdd() from FSA, but psdAdd() requires full species names rather than abbreviations. mapvalues() from plyr is used below to create a new variable with the full names derived from the abbreviations.\n\ndat &lt;- dat |&gt;\n  mutate(species2=plyr::mapvalues(species,\n                                  from=c(\"YCT\",\"UTC\"),\n                                  to=c(\"Cutthroat Trout\",\"Utah Chub\")))\n# just checking\nxtabs(~species+species2,data=dat)\n\n#R|         species2\n#R|  species Cutthroat Trout Utah Chub\n#R|      UTC               0     14448\n#R|      YCT            5454         0\n\n\nNow the length categories may be added using species2.\n\ndat &lt;- dat |&gt;\n  mutate(gcat=FSA::psdAdd(len=length,species=species2))\nFSA::headtail(dat)\n\n#R|        species year length weight        species2     gcat\n#R|  1         YCT 2002    150     NA Cutthroat Trout substock\n#R|  2         YCT 2002    160     NA Cutthroat Trout substock\n#R|  3         YCT 2002    160     NA Cutthroat Trout substock\n#R|  19900     YCT 2020    391    550 Cutthroat Trout  quality\n#R|  19901     YCT 2020    284    229 Cutthroat Trout    stock\n#R|  19902     YCT 2020    440    853 Cutthroat Trout  quality\n\n\n\n\nComputing CPE\nFrom this, the data needs to be summarized to produce the CPE for each species for each year for each length category and for all fish regardless of length category. Development of this data frame will begin by counting the number of fish (i.e., rows) for each length category within each year within each species.\n\ncatch_gcat &lt;- dat |&gt;\n  group_by(species,year,gcat) |&gt;\n  summarize(catch=n())\ncatch_gcat\n\n#R|  # A tibble: 162 × 4\n#R|  # Groups:   species, year [36]\n#R|     species  year gcat      catch\n#R|     &lt;chr&gt;   &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;\n#R|   1 UTC      2003 substock      5\n#R|   2 UTC      2003 stock        41\n#R|   3 UTC      2003 quality      21\n#R|   4 UTC      2003 preferred    15\n#R|   5 UTC      2003 memorable     4\n#R|   6 UTC      2004 substock      2\n#R|   7 UTC      2004 stock       535\n#R|   8 UTC      2004 quality     128\n#R|   9 UTC      2004 preferred    52\n#R|  10 UTC      2004 memorable    11\n#R|  # … with 152 more rows\n\n\nNote in the previous output that catch_gcat is still “grouped” by year within species. Thus, catch can summed from this data frame to find the total catch per year for each species.3\n3 Ungroup this data frame as we will not summarize it further.\ncatch_all &lt;- catch_gcat |&gt;\n  summarize(catch=sum(catch)) |&gt;\n  ungroup()\ncatch_all\n\n#R|  # A tibble: 36 × 3\n#R|     species  year catch\n#R|     &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;\n#R|   1 UTC      2003    86\n#R|   2 UTC      2004   728\n#R|   3 UTC      2005   294\n#R|   4 UTC      2006   424\n#R|   5 UTC      2007   895\n#R|   6 UTC      2008   606\n#R|   7 UTC      2009   512\n#R|   8 UTC      2011   440\n#R|   9 UTC      2012   692\n#R|  10 UTC      2013  1858\n#R|  # … with 26 more rows\n\n\nAs Figure 2 displays all fish and fish by length category, these two data frames should be row-bound (i.e., stacked) together. Figure 2 does not have panels for “substock”, “memorable”, or “trophy” sized fish so fish in those length categories were removed from the resultant data frame. Finally, a new variable called type was created that has the names from gcat, or “All” if gcat was NA4.\n4 gcat for “all fish” after bind_rows() was NA as gcat was not present in catch_all\ncpe_dat &lt;- bind_rows(catch_all,catch_gcat) |&gt;\n  filter(!(gcat %in% c(\"substock\",\"memorable\",\"trophy\"))) |&gt;\n  mutate(type=ifelse(is.na(gcat),\"All\",as.character(gcat)))\nFSA::headtail(cpe_dat)\n\n#R|      species year catch      gcat      type\n#R|  1       UTC 2003    86      &lt;NA&gt;       All\n#R|  2       UTC 2004   728      &lt;NA&gt;       All\n#R|  3       UTC 2005   294      &lt;NA&gt;       All\n#R|  143     YCT 2020    52     stock     stock\n#R|  144     YCT 2020   132   quality   quality\n#R|  145     YCT 2020    13 preferred preferred\n\n\nFinally, these data are left-join()ed (by year) with eff to add netNights, from which a new cpe variable is created. type was converted to a factor to match the vertical order of Figure 2, and the levels were given longer labels to match the labels in Figure 2. Finally, the variables were re-ordered (and gcat was dropped) for personal preference.\n\ncpe_dat &lt;- cpe_dat |&gt;\n  left_join(eff,by=\"year\") |&gt;\n  mutate(cpe=catch/netNights,\n         type=factor(type,levels=c(\"All\",\"stock\",\"quality\",\"preferred\"),\n                     labels=c(\"All fish\",\"Stock to Quality\",\n                              \"Quality to Preferred\",\"Preferred to Memorable\"))) |&gt;\n  select(species,year,type,catch,netNights,cpe)\nFSA::headtail(cpe_dat)\n\n#R|      species year                   type catch netNights        cpe\n#R|  1       UTC 2003               All fish    86        18  4.7777778\n#R|  2       UTC 2004               All fish   728        50 14.5600000\n#R|  3       UTC 2005               All fish   294        46  6.3913043\n#R|  143     YCT 2020       Stock to Quality    52        30  1.7333333\n#R|  144     YCT 2020   Quality to Preferred   132        30  4.4000000\n#R|  145     YCT 2020 Preferred to Memorable    13        30  0.4333333\n\n\nThis data frame, now called cpe_dat, is now ready for recreating Figure 2.\n\n\nQuick Summaries of CPE\nBefore recreating Figure 2, I summarized the CPE data by species and year for comparison to results in McCarrick et al. (2022).\n\ncpe_dat |&gt;\n  filter(type==\"All fish\") |&gt;\n  group_by(species) |&gt;\n  summarize(n=n(),\n            ttlcatch=sum(catch),\n            mncpe=mean(cpe),\n            sdcpe=sd(cpe),\n            mincpe=min(cpe),\n            minyr=year[cpe==mincpe],\n            maxcpe=max(cpe),\n            maxyr=year[cpe==maxcpe])\n\n#R|  # A tibble: 2 × 9\n#R|    species     n ttlcatch mncpe sdcpe mincpe minyr maxcpe maxyr\n#R|    &lt;chr&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#R|  1 UTC        18    14459 20.0  12.9   0.239  2005   50.5  2008\n#R|  2 YCT        19     5454  7.16  3.48  1.52   2018   15.0  2007\n\n\nIn the first paragraph of the results, McCarrick et al. (2022) found the overall mean (and SD) CPE was 7.4 (3.6) for YCT and 19.9 (13.0) for UTC, which is close to what I obtained for UTC but not for YTC. In addition, they found a peak CPE of 15.4 for YCT in 2007 and 50.5 for UTC in 2008, again very close to my results for UTC but not that close for YCT. Finally, in the label for Figure 2 they reported 5,524 YCT were captured, whereas my results only show 5,454.\n \n\n\n\nRecreating Figure 2\nMy general strategy for recreating Figure 2 was to create the left and right panels of subplots separately and then combine them using patchwork. Each side can be constructed via faceting on type, but I will use facet_rep_wrap() from lemon so as to have the x-axis tick marks on each panel.5 Each panel will be create with geom_col() using width=1 so that the bars touch and geom_text() to place the labels.6 The x- and y-axes were modified as is commonly done.7 Finally, I used theme_bw() as a base, removed the gridlines and removed the facet labels and background.8 As you examine the code below, make sure to realize that the data was filtered to just “YCT”.\n5 See this post about facet_rep_wrap().6 See this post for more explanation of how to use geom_text() in this way.7 And is described more in this post.8 As described in this post.\nYCT &lt;- ggplot(data=filter(cpe_dat,species==\"YCT\"),\n              mapping=aes(x=year,y=cpe)) +\n  geom_col(color=\"black\",fill=\"gray70\",width=1) +\n  geom_text(mapping=aes(label=type),x=Inf,y=Inf,vjust=1.25,hjust=1.05,\n            check_overlap=TRUE,size=3) +\n  scale_y_continuous(name=\"YCT per net night\",\n                     limits=c(0,16),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  lemon::facet_rep_wrap(vars(type),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.background=element_blank(),\n        strip.text=element_blank())\nYCT\n\n\n\n\n\n\n\n\nThe code above was largely repeated but with data filtered to just “UTC” and adjusting the y-axis accordingly. The plot is not shown here but is the right side in the plot further below.\n\nUTC &lt;- ggplot(data=filter(cpe_dat,species==\"UTC\"),mapping=aes(x=year,y=cpe)) +\n  geom_col(color=\"black\",fill=\"gray70\",width=1) +\n  geom_text(mapping=aes(label=type),x=Inf,y=Inf,vjust=1.25,hjust=1.05,\n            check_overlap=TRUE,size=3) +\n  scale_y_continuous(name=\"UTC per net night\",\n                     limits=c(0,60),breaks=scales::breaks_width(10),\n                     expand=expansion(mult=0)) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  lemon::facet_rep_wrap(vars(type),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.background=element_blank(),\n        strip.text=element_blank())\n\nThe two subplots are then placed side-by-side.\n\nYCT + UTC\n\n\n\n\n\n\n\n\n \n\n\nFurther Thoughts\nI wanted to see if I could make a similar looking plot using facet_grid(). It turns out to be a bit of a challenge because each panel on the left- and right-sides has the same y-axis, but those axes differ between the two sides. I found a way around this.\nFirst, I changed species to a factor with “YCT” in the first position so it would be plotted on the left (i.e., first). I also gave the levels longer labels so that the facets would have useful labels.\n\ncpe_dat &lt;- cpe_dat |&gt;\n  mutate(species=factor(species,levels=c(\"YCT\",\"UTC\"),\n                        labels=c(\"Yellowstone Cutthroat Trout\",\"Utah Chub\")))\n\nThe y-axis needs to be defined for each facet. However, these definitions should be constant within species. Thus, I define limits and breaks for both YCT and UTC and saved them as objects. In addition, to save typing, I also defined the expansion factor as an object.\n\nYCT_lmts &lt;- c(0,16)\nYCT_brks &lt;- scales::breaks_width(2)\nUTC_lmts &lt;- c(0,60)\nUTC_brks &lt;- scales::breaks_width(10)\nyexp &lt;- expansion(mult=0)\n\nThe basic plot uses the same ggplot() (except the data is not filtered), geom_col(), scale_x_continous(), and theme_bw() as before. The geom_text() has been eliminated because the “type” labels are going to be in the facet labels rather than “on the plot.” The part of theme() that removed the facet labels has been eliminated as facet labels are needed here. The spacing between panels was increased in theme() here. facet_rep_wrap() was replaced with facet_grid2() from ggh4x, where the rows are defined by type, the columns are defined by species, the y-axes are “free” and “independent” from facet to facet.9 Finally, facetted_pos_scales() from ggh4x is used to apply a different scale_y_continous() to each panel.10\n9 The independent=\"y\" part is the main thing that facet_grid2() adds over facet_grid() from ggplot2.10 facetted_pos_scales() is described in more detail in this post.\nggplot(data=cpe_dat,mapping=aes(x=year,y=cpe)) +\n  geom_col(color=\"black\",fill=\"gray70\",width=1) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  ggh4x::facet_grid2(rows=vars(type),cols=vars(species),\n                     scales=\"free_y\",independent=\"y\") +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        panel.spacing=unit(10,units=\"pt\")) +\n  ggh4x::facetted_pos_scales(\n    y=list(\n      scale_y_continuous(name=\"# Fish per net night\",\n                         limits=YCT_lmts,breaks=YCT_brks,expand=yexp),\n      scale_y_continuous(limits=UTC_lmts,breaks=UTC_brks,expand=yexp),\n      scale_y_continuous(limits=YCT_lmts,breaks=YCT_brks,expand=yexp),\n      scale_y_continuous(limits=UTC_lmts,breaks=UTC_brks,expand=yexp),\n      scale_y_continuous(limits=YCT_lmts,breaks=YCT_brks,expand=yexp),\n      scale_y_continuous(limits=UTC_lmts,breaks=UTC_brks,expand=yexp),\n      scale_y_continuous(limits=YCT_lmts,breaks=YCT_brks,expand=yexp),\n      scale_y_continuous(limits=UTC_lmts,breaks=UTC_brks,expand=yexp)\n    )\n  )\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nMcCarrick, D. K., J. C. Dillon, B. High, and M. C. Quist. 2022. Population dynamics of Yellowstone Cutthroat Trout in Henrys Lake, Idaho. Journal of Fish and Wildlife Management 13(1):169–181.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {McCarrick Et Al. (2022) {CPE} {Plot}},\n  date = {2023-03-22},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-3-22_McCarricketal2022_Fig2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, March 22. McCarrick et al. (2022) CPE Plot. https://fishr-core-team.github.io/fishR//blog/posts/2023-3-22_McCarricketal2022_Fig2."
  },
  {
    "objectID": "blog/posts/2023-3-18-Rooketal2022_CiscoHarvest/index.html",
    "href": "blog/posts/2023-3-18-Rooketal2022_CiscoHarvest/index.html",
    "title": "Rook et al. (2022) Cisco Harvest Figure",
    "section": "",
    "text": "Introduction\nRook et al. (2022) analyzed historical data to answer the question of how many ciscoes are needed for stocking in the Laurentian Great Lakes. Their Figure 2 shows the commercial harvest of Cisco (Coregonus artedii) and deepwater ciscoes (Coregonus spp.) in the five main Great Lakes and St. Clair. The main point of their figure was to demonstrate the great reduction in harvest of these species by the 1970s.\nThe following packages are loaded for use below.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\nlibrary(scales)     # for breaks_width()\nlibrary(ggh4x)      # for minor tick functionality and facetted_pos_scales()\n\n \n\n\nGet Data\nRook et al. (2022) did not provide data for Figure 2, but they did provide a reference that linked to a database maintained by the Great Lakes Fishery Commission that contained an Excel spreadsheet with harvest data for each lake (in separate sheets) for ALL species. Below each sheet is read into into its own data frame and reduced to just the variables needed for this post,1\n1 Note that the total harvest variable was inconsistently named across sheets.\neri &lt;- readxl::read_excel(\"commercial.xlsx\",sheet=\"Erie\") |&gt;\n  select(Year,Lake,Species,Total=`Grand Total`)\nont &lt;- readxl::read_excel(\"commercial.xlsx\",sheet=\"Ontario\") |&gt;\n  select(Year,Lake,Species,Total=`Grand Totals`) #!!\nhur &lt;- readxl::read_excel(\"commercial.xlsx\",sheet=\"Huron\") |&gt;\n  select(Year,Lake,Species,Total=`Grand Total`)\nsup &lt;- readxl::read_excel(\"commercial.xlsx\",sheet=\"Superior\") |&gt;\n  select(Year,Lake,Species,Total=`Grand Total`)\nmic &lt;- readxl::read_excel(\"commercial.xlsx\",sheet=\"Michigan\") |&gt;\n  select(Year,Lake,Species,Total=`U.S. Total`) #!!\nstc &lt;- readxl::read_excel(\"commercial.xlsx\",sheet=\"St. Clair\") |&gt;\n  select(Year,Lake,Species,Total=`Grand Total`)\n\nThe individual data frames are then row-bound (i.e., stacked) together, the Lake variable is moved to the first column, and the data are sorted by Lake, Year, and Species.2\n2 Moving Lake and sorting was for personal aesthetics.\ndat &lt;- bind_rows(eri,ont,hur,sup,mic,stc) |&gt;\n  relocate(Lake) |&gt;\n  arrange(Lake,Year,Species)\n\nI examined the names in Species so that I could restrict the data frame to Cisco and deepwater ciscoes.\n\nunique(dat$Species)\n\n#R|   [1] \"Cisco\"                         \"Lake Whitefish\"               \n#R|   [3] \"Walleye and Blue Pike\"         \"Northern Pike\"                \n#R|   [5] \"Lake Sturgeon\"                 \"Blue Pike\"                    \n#R|   [7] \"Channel Catfish and Bullheads\" \"Sauger\"                       \n#R|   [9] \"Walleye\"                       \"Yellow Perch\"                 \n#R|  [11] \"Suckers\"                       \"Carp\"                         \n#R|  [13] \"Burbot\"                        \"Freshwater Drum\"              \n#R|  [15] \"White Bass\"                    \"Bullheads\"                    \n#R|  [17] \"Channel Catfish\"               \"Goldfish\"                     \n#R|  [19] \"Rainbow Smelt\"                 \"Pacific Salmon\"               \n#R|  [21] \"Bowfin\"                        \"Buffalo\"                      \n#R|  [23] \"Gizzard Shad\"                  \"Lake Trout\"                   \n#R|  [25] \"Minnows\"                       \"Quillback\"                    \n#R|  [27] \"Rock Bass\"                     \"Sunfish\"                      \n#R|  [29] \"White Perch\"                   \"American Eel\"                 \n#R|  [31] \"Crappie\"                       \"Channel catfish\"              \n#R|  [33] \"Gar\"                           \"Round Whitefish\"              \n#R|  [35] \"Cisco and Chubs\"               \"Chubs\"                        \n#R|  [37] \"Pacific salmon\"                \"Crappies\"                     \n#R|  [39] \"Cisco and Chub\"                \"Alewife\"                      \n#R|  [41] \"Coho Salmon\"                   \"Chinook Salmon\"               \n#R|  [43] \"Amercian Eel\"                  \"White bass\"                   \n#R|  [45] \"Herring\"                       \"Smallmouth Bass\"              \n#R|  [47] \"Bullhead\"                      \"Drum\"                         \n#R|  [49] \"Rock Bass and Crappie\"         \"Sheepshead\"                   \n#R|  [51] \"Cisco and chubs\"               \"Lake Trout - siscowet\"\n\n\nThis was a little messier than I had hoped. First, “Cisco” are sometimes called “Herring”, so both of these “species” must be retained. Deepwater ciscoes are often called “chubs”, so this “species” must be retained. However, there were also “species” called various versions of “Cisco and Chubs”, all of which need to be retained. A new data frame is created below with just these “species” and, to match Rook et al. (2022), only years of 1880 to 2015 (inclusive) were retained.\n\ncdat &lt;- dat |&gt;\n  filter(Species %in% c(\"Cisco\",\"Chubs\",\"Cisco and Chubs\",\n                        \"Cisco and chubs\",\"Cisco and chub\",\"Herring\")) |&gt;\n  filter(Year&gt;=1880,Year&lt;=2015)\n\nFor what it is worth, it seems that the use of “Cisco and chubs” for Lake Superior should be corrected in the database, and “Herring” for Lake St. Clair should be changed to “Cisco.”\n\nxtabs(~Lake+Species,data=cdat)\n\n#R|               Species\n#R|  Lake          Chubs Cisco Cisco and chubs Cisco and Chubs Herring\n#R|    Erie            0   102               0               0       0\n#R|    Huron         113   113               0              29       0\n#R|    Michigan      126   107               0               0       0\n#R|    Ontario        64    64               0              72       0\n#R|    Saint Clair     0     0               0               0      39\n#R|    Superior      123   136              61               0       0\n\n\nFigure 2 ultimately lumps all of these “species” together within each year and lake combination. For example, in the portion of cdat below the two entries for 2015 for Lake Superior should be combined to form one entry.\n\nFSA::headtail(cdat)\n\n#R|           Lake Year Species    Total\n#R|  1        Erie 1880   Cisco  882.000\n#R|  2        Erie 1881   Cisco 1868.000\n#R|  3        Erie 1882   Cisco 2008.000\n#R|  1147 Superior 2014   Cisco 1587.571\n#R|  1148 Superior 2015   Chubs   87.638\n#R|  1149 Superior 2015   Cisco 1983.159\n\n\nAdditionally, it is important to note that some Total values are missing (coded as NA). Thus, an adjustment will be made below for these NAs when summing to make a composite harvest value for each year and lake.\n\nany(is.na(cdat$Total))\n\n#R|  [1] TRUE\n\n\nA new data frame is created from the data frame above that groups the data by Year within Lake and then sums the Total harvest for all “species” in each lake-year combination into a variable called Coregonids. na.rm=TRUE is used in sum to ignore the NAs.3 The data frame is ungroup()ed before moving on.\n3 Otherwise, any sum with an NA value will return NA.\ncdat2 &lt;- cdat |&gt;\n  group_by(Lake,Year) |&gt;\n  summarize(Coregonids=sum(Total,na.rm=TRUE)) |&gt;\n  ungroup()\nFSA::peek(cdat2,n=10)\n\n#R|             Lake Year Coregonids\n#R|  1          Erie 1880    882.000\n#R|  75         Erie 1954    381.000\n#R|  150       Huron 1927   7694.000\n#R|  225       Huron 2002    118.622\n#R|  300    Michigan 1951  18675.000\n#R|  375     Ontario 1890   3084.000\n#R|  450     Ontario 1965     31.000\n#R|  525 Saint Clair 1904     27.000\n#R|  600    Superior 1940  36879.000\n#R|  675    Superior 2015   2070.797\n\n\nIt was not obvious to me from the Great Lakes Fisheries Commission website what the units are for these harvest data. In comparison to Rook et al. (2022) they appear to be hundreds of thousands of pounds. Rook et al. (2022) plotted millions of kgs so I first convert Coregonids to kg and then divide by 1000 to get to millions of kgs.\n\ncdat2 &lt;- cdat2 |&gt;\n  mutate(Coregonids=Coregonids*0.45359237,\n         Coregonids=Coregonids/1000)\n\nAn issue that happens regularly with these type of data with this type of plot occurs when the data does not have continuous years. For example, suppose that harvest was recorded for 1900 and 1902 but not 1901. If 1901 is completely excluded from the data, rather than being entered as an NA, then the line in the plot will connect directly from 1900 to 1902 suggesting, if the reader is not paying close attention, that a value exists in 1901. You can see many unconnected dots in Figure 2 of Rook et al. (2022) that illustrate how these missing data should be handled.\nWith these data, it appears that the years are contiguous within each lake except for Lake Erie, but only complete (for 1880-2015) for lakes Huron, Ontario, and Superior.\n\ncdat2 |&gt;\n  group_by(Lake) |&gt;\n  summarize(numYrs=n(),\n            minYr=min(Year),\n            maxYr=max(Year),\n            rngYr=maxYr-minYr+1,\n            contigYr=ifelse(rngYr==numYrs,\"Yes\",\"No\"),\n            compltYr=ifelse(numYrs==(2015-1880+1),\"Yes\",\"No\"))\n\n#R|  # A tibble: 6 × 7\n#R|    Lake        numYrs minYr maxYr rngYr contigYr compltYr\n#R|    &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n#R|  1 Erie           102  1880  2015   136 No       No      \n#R|  2 Huron          136  1880  2015   136 Yes      Yes     \n#R|  3 Michigan       126  1890  2015   126 Yes      No      \n#R|  4 Ontario        136  1880  2015   136 Yes      Yes     \n#R|  5 Saint Clair     39  1880  1918    39 Yes      No      \n#R|  6 Superior       136  1880  2015   136 Yes      Yes\n\n\nThe issue with Lake Erie appears to be many years of missing data from 1977 to 2012.\n\ntmp &lt;- cdat2 |&gt;\n  filter(Lake==\"Erie\")\ntmp$Year\n\n#R|    [1] 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894\n#R|   [16] 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909\n#R|   [31] 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924\n#R|   [46] 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939\n#R|   [61] 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954\n#R|   [76] 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969\n#R|   [91] 1970 1971 1972 1973 1974 1975 1976 1977 2012 2013 2014 2015\n\n\nTo make sure that the data are contiguous and cover the range of years, expand_grid() was used first to create a temporary data frame with each year from 1880 to 2015 (inclusive) listed for each lake.\n\ntmp &lt;- expand_grid(Lake=unique(cdat2$Lake),\n                   Year=min(cdat2$Year):max(cdat2$Year))\n\nFSA::headtail(tmp)\n\n#R|          Lake Year\n#R|  1       Erie 1880\n#R|  2       Erie 1881\n#R|  3       Erie 1882\n#R|  814 Superior 2013\n#R|  815 Superior 2014\n#R|  816 Superior 2015\n\n\nThis data frame was left_join() with cdat2, using both Lake and Year as the id variables, so that every lake-year combination in tmp was matched with a Total from the corresponding lake-year combination in cdat2 or an NA if the lake-year combination did not exist in cdat2.\n\ncdat3 &lt;- left_join(tmp,cdat2,by=c(\"Lake\",\"Year\"))\n\nNow the years are contiguous and complete for each lake.\n\ncdat3 |&gt;\n  group_by(Lake) |&gt;\n  summarize(numYrs=n(),\n            minYr=min(Year),\n            maxYr=max(Year),\n            rngYr=maxYr-minYr+1,\n            contigYr=ifelse(rngYr==numYrs,\"Yes\",\"No\"),\n            compltYr=ifelse(numYrs==(2015-1880+1),\"Yes\",\"No\"))\n\n#R|  # A tibble: 6 × 7\n#R|    Lake        numYrs minYr maxYr rngYr contigYr compltYr\n#R|    &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n#R|  1 Erie           136  1880  2015   136 Yes      Yes     \n#R|  2 Huron          136  1880  2015   136 Yes      Yes     \n#R|  3 Michigan       136  1880  2015   136 Yes      Yes     \n#R|  4 Ontario        136  1880  2015   136 Yes      Yes     \n#R|  5 Saint Clair    136  1880  2015   136 Yes      Yes     \n#R|  6 Superior       136  1880  2015   136 Yes      Yes\n\n\nFinally, Lake is made a factor with the levels specified so that the lakes will be ordered as in Figure 2 of Rook et al. (2022)\n\ncdat3 &lt;- cdat3 |&gt;\n  mutate(Lake=factor(Lake,levels=c(\"Superior\",\"Huron\",\"Michigan\",\n                                   \"Erie\",\"Ontario\",\"Saint Clair\")))\n\nAs will be shown further below, these data do not appear to be exactly the data used in Rook et al. (2022). One of several issues that is evident is that their Figure 2 shows many years of missing data (i.e., points not connected with lines) early in the time series for most lakes, whereas there were few missing years in my data frame. I suspected that Rook et al. (2022) may have coded a harvest of 0 as NA so, to explore this, I created a new variable called is0 that I will plot differently further below. I did this only to troubleshoot this data issue, not as part of recreating their Figure 2.\n\ncdat3 &lt;- cdat3 |&gt;\n  mutate(is0=Coregonids==0)\n\n \n\n\nRecreating Figure 2\nThe ggplot2 theme was set to theme_classic() but with modifications to remove the background for facet labels, increase the spacing between facets, remove the legend,4 make tick marks slightly longer, and make minor tick marks 50% as big as the major tick marks.5\n4 The only reason there is a legend is because I am going to highlight points where the harvest was 0.5 This requires the ggh4x package attached above.\ntheme_set(\n  theme_classic() +\n    theme(strip.background=element_blank(),\n          panel.spacing=unit(5,\"mm\"),\n          legend.position=\"none\",\n          axis.ticks.length.x=unit(5,units=\"pt\"),\n          ggh4x.axis.ticks.length.minor=rel(0.5)))\n\nI could not find a simple way to have minor ticks, ticks that crossed the axes, and differing y-axis limits for each facet as used in Figure 2 of Rook et al. (2022). Thus, I opted to not have ticks that crossed the axes.\nA basic plot is constructed below from cdat3 with Year mapped to the x-axis and Coregonids mapped to the y-axis with geom_line() and geom_point() to plot the line with points. facet_wrap() was used to separate the plots by lake. In facet_wrap() both scales were “freed” because the y-axes need to change by lake and, though the x-axes don’t differ by lake, freeing the x-axes will force an x-axis to be shown for each lake as in Rook et al. (2022). is0 is mapped to a color in geom_point(), and the colors are defined in scale_color_manual(), to show years that had 0 harvest as red.&(Again, this is not in Rook et al. 2022, but I was trying to better understand why my data appeared different than theirs.)\n\nggplot(data=cdat3,mapping=aes(x=Year,y=Coregonids)) +\n  geom_line() +\n  geom_point(mapping=aes(color=is0),size=1) +\n  scale_color_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"red\")) +\n  facet_wrap(vars(Lake),scales=\"free\",ncol=2)\n\n\n\n\n\n\n\n\nSome challenges remain to try to match Figure 2 in Rook et al. (2022). First, the x-axes need to be bound from 1880 to 2020, the tick mark labels need to be shown at intervals of 20 years, and minor ticks need to be shown at 5 year intervals. The axes bounds are set with limits=, major (i.e., labeled) breaks are set with breaks= using breaks_width(),6 and minor breaks are set with minor_breaks= also using breaks_width().7 Note the use of guide= so that the minor breaks will be shown.\n6 breaks_width() requires the scales package attached above.7 As described in this post.\nggplot(data=cdat3,mapping=aes(x=Year,y=Coregonids)) +\n  geom_line() +\n  geom_point(mapping=aes(color=is0),size=1) +\n  scale_x_continuous(limits=c(1880,2020),expand=expansion(mult=0),\n                     breaks=breaks_width(20),\n                     minor_breaks=breaks_width(5),guide=\"axis_minor\") +\n  scale_color_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"red\")) +\n  facet_wrap(vars(Lake),scales=\"free\",ncol=2)\n\n\n\n\n\n\n\n\nNow the y-axes need a different label and different upper limits, major tick labels, and minor ticks per facet/panel. All of these items may be adjusted on a per-facet basis with facetted_pos_scales() which is described in more detail in this post. For these purposes, y= in facetted_pos_scales() will receive a list with as many scale_y_continuous() items in it as facets (i.e., six). Each scale_y_continuous() item in this list will define the y-axis for a facet, ordered across rows and down columns. For example, the first scale_y_continuous() item is for Lake Superior and the last is for Lake St. Clair. Note that the name for the y-axis only needs to be defined in the first scale_y_continous() item in the list.\n\nsyc &lt;- list(\n  scale_y_continuous(name=\"Commercial Harvest (millions of kg)\",\n                     limits=c(0,20),expand=expansion(mult=0),\n                     breaks=breaks_width(5),minor_breaks=breaks_width(5/5),\n                     guide=\"axis_minor\"),\n  scale_y_continuous(limits=c(0,5),expand=expansion(mult=0),\n                     breaks=breaks_width(1),minor_breaks=breaks_width(1/5),\n                     guide=\"axis_minor\"),\n  scale_y_continuous(limits=c(0,15),expand=expansion(mult=0),\n                     breaks=breaks_width(5),minor_breaks=breaks_width(5/5),\n                     guide=\"axis_minor\"),\n  scale_y_continuous(limits=c(0,25),expand=expansion(mult=0),\n                     breaks=breaks_width(5),minor_breaks=breaks_width(5/5),\n                     guide=\"axis_minor\"),\n  scale_y_continuous(limits=c(0,2.5),expand=expansion(mult=0),\n                     breaks=breaks_width(0.5),minor_breaks=breaks_width(0.5/5),\n                     guide=\"axis_minor\"),\n  scale_y_continuous(limits=c(0,0.5),expand=expansion(mult=0),\n                     breaks=breaks_width(0.1),minor_breaks=breaks_width(0.1/5),\n                     guide=\"axis_minor\")\n  )\n\nFor convenience the list was entered into an object above and given to facetted_pos_scale() below.\n\nggplot(data=cdat3,mapping=aes(x=Year,y=Coregonids)) +\n  geom_line() +\n  geom_point(mapping=aes(color=is0),size=1) +\n  scale_x_continuous(limits=c(1880,2020),expand=expansion(mult=0),\n                     breaks=breaks_width(20),\n                     minor_breaks=breaks_width(5),guide=\"axis_minor\") +\n  scale_color_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"red\")) +\n  facet_wrap(vars(Lake),scales=\"free\",ncol=2) +\n  facetted_pos_scales(y=syc)\n\n\n\n\n\n\n\n\nFinally, Rook et al. (2022) placed the lake labels in the upper-right corner of each facet rather than centered above each facet. To accomplish this use geom_text() with Lake mapped to a label. This will, by default, print the lake name for each facet over-and-over for as many rows as there are for the lake in the data frame. To eliminate these overlapping labels, include check_overlap=TRUE. The label position was set to the maximum x and maximum y value for each facet by using both x=Inf and y=Inf. The label was then moved left (with vjust=) and down (with hjust=) as described in this post. The font size was reduced from the default slightly. Finally, the default facet labels must be removed by setting strip.text= to element_blank() in theme().\n\nggplot(data=cdat3,mapping=aes(x=Year,y=Coregonids)) +\n  geom_line() +\n  geom_point(mapping=aes(color=is0),size=1) +\n  geom_text(mapping=aes(label=Lake),check_overlap=TRUE,size=3.5,\n            x=Inf,y=Inf,vjust=1.2,hjust=1.2) +\n  scale_x_continuous(limits=c(1880,2020),expand=expansion(mult=0),\n                     breaks=breaks_width(20),\n                     minor_breaks=breaks_width(5),guide=\"axis_minor\") +\n  scale_color_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"red\")) +\n  facet_wrap(vars(Lake),scales=\"free\",ncol=2) +\n  facetted_pos_scales(y=syc) +\n  theme(strip.text=element_blank())\n\n\n\n\n\n\n\n\nThis largely recreates Figure 2 in Rook et al. (2022) with the exceptions of (a) tick marks that do not cross the axes and (b) discrepancies in the data used.\n \n\n\n\n\n\n\n\n\n\nReferences\n\nRook, B. J., M. J. Hansen, and C. R. Bronte. 2022. How many Ciscoes are needed for stocking in the Laurentian Great Lakes? Journal of Fish and Wildlife Management 13(1):28–49.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Rook Et Al. (2022) {Cisco} {Harvest} {Figure}},\n  date = {2023-03-17},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-3-18-Rooketal2022_CiscoHarvest},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, March 17. Rook et al. (2022) Cisco Harvest Figure. https://fishr-core-team.github.io/fishR//blog/posts/2023-3-18-Rooketal2022_CiscoHarvest."
  },
  {
    "objectID": "blog/posts/2023-3-10-Text_Annotation_Position/index.html",
    "href": "blog/posts/2023-3-10-Text_Annotation_Position/index.html",
    "title": "Using vjust= and hjust=",
    "section": "",
    "text": "It seems that many of my “positioning” issues in ggplot2 are ultimately solved with the use of vjust=, hjust=, or both. However, when I need to use these arguments/concepts I often find myself “googling” for a solution and then not fully internalizing how the solution worked. In this post I attempt to provide some explanation around vjust= and hjust= in hopes that I, and maybe you, will better understand how they work, and will be able to better apply them in the future."
  },
  {
    "objectID": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#labels",
    "href": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#labels",
    "title": "Using vjust= and hjust=",
    "section": "Labels",
    "text": "Labels\nSuppose that we want to highlight the peak temperature point with a label. With this plot, the label would be best above and horizontally centered on the peak point. Thus, hjust= will be left at the default of 0.5 and vjust= will be set to a negative number (i.e., 0 would place the bottom of the label on the point, we want to move it up further so a negative number is needed).3\n3 Here and below I colored the text red to show the change being made.\np1 +\n  annotate(geom=\"text\",x=as.Date(\"28-Jun-2021\",format=dfmt),\n           y=max(dat$temp),label=\"Warmest Day!\",vjust=-1,color=\"red\")\n\n\n\n\n\n\n\n\nThe same concept works for using geom_text() to label all points.\n\np1 +\n  geom_text(data=dat,mapping=aes(x=date,y=temp,label=temp),\n            vjust=-1,size=3,color=\"red\")\n\n\n\n\n\n\n\n\nOf course, this does not work well here because the same justification is used for every point, so there is overlap with the line, other labels, etc. If you really want to do something like this then look at functionality in the ggrepel and ggpp packages."
  },
  {
    "objectID": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#plot-title",
    "href": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#plot-title",
    "title": "Using vjust= and hjust=",
    "section": "Plot Title",
    "text": "Plot Title\nA title may be added to a plot with title= in labs(). By default, the title is left-justified with respect to the plot body. The title can be centered by modifying plot.title= in theme() with hjust=0.5 within element_text().4\n4 plot.title= is a text element, thus the use of element_text() here.\np1 +\n  labs(title=\"Pacific Lamprey Mortality Events\") +\n  theme(plot.title=element_text(hjust=0.5,color=\"red\"))\n\n\n\n\n\n\n\n\nThis is also demonstrated in this previous post."
  },
  {
    "objectID": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#plot-tag-label",
    "href": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#plot-tag-label",
    "title": "Using vjust= and hjust=",
    "section": "Plot Tag Label",
    "text": "Plot Tag Label\nIf multiple plots are placed together it is common to adorn each plot with a letter or some other signifier. Suppose that this plot will be the first in a series of plots and, thus, an “(A)” should be placed in the upper-left corner. It is best not to specific numeric values for the upper-left corner because (a) that can be tricky if the axis limits have been expanded, (b) it depends on the data in the plot, and (c) it is hard to be consistent across plots. One trick here is that ggplot2 will define the upper-left corner at -Inf for the x and Inf for the y-coordinate.5 The problem with this is that the label is centered on those extreme coordinates and is cutoff and over the axis (see below).\n5 The -Inf had to be converted to a date because the x-axis is dates.\np1 +\n  annotate(geom=\"text\",x=as.Date(-Inf,format=dfmt),y=Inf,label=\"(A)\",color=\"red\")\n\n\n\n\n\n\n\n\nWe can set the upper-left corner of the label to be at the coordinate to move the label into the plotting region. However, to further move the point off the extreme coordinate position, use a value greater than 1 for vjust= (i.e., move the top further down) and a negative value for hjust= (i.e., move the left side further to the right).\n\np1 +\n  annotate(geom=\"text\",x=as.Date(-Inf,format=dfmt),y=Inf,label=\"(A)\",color=\"red\",\n           vjust=1.5,hjust=-1)\n\n\n\n\n\n\n\n\nThis is also demonstrated in this previous post."
  },
  {
    "objectID": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#rotated-axis-labels",
    "href": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#rotated-axis-labels",
    "title": "Using vjust= and hjust=",
    "section": "Rotated Axis Labels",
    "text": "Rotated Axis Labels\nThe labels on the x-axis are crowded. One way to adjust for this is to rotate the labels to a vertical position using angle=90 in element_text() for axis.text.x= in theme=. However, as seen below, the labels are then not horizontally aligned with the tick marks.\n\np1 +\n  theme(axis.text.x=element_text(angle=90,color=\"red\"))\n\n\n\n\n\n\n\n\nIt would seem that hjust= is needed to correct this horizontal alignment problem. However, it is vjust= that is used. I am not sure that I fully understand this but I think that it means that the “top” and the “bottom” of the labels were not “renamed” with the rotation and, thus, are still controlled with vjust=.\n\np1 +\n  theme(axis.text.x=element_text(angle=90,vjust=0.5,color=\"red\"))"
  },
  {
    "objectID": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#legend-position",
    "href": "blog/posts/2023-3-10-Text_Annotation_Position/index.html#legend-position",
    "title": "Using vjust= and hjust=",
    "section": "Legend Position",
    "text": "Legend Position\nThe concepts of vjust= and hjust= are also used when placing a legend within the plotting area, though those arguments are not used explicitly.\nI modified the base plot from above to have a legend. Note that the grouping here is not really useful, I am just using this as an illustration of how to move the legend. This object is called p2.\n\n\n\n\n\n\n\n\n\nThe position of the legend is controlled by legend.position= in theme(). The legend can be moved into the plot by providing a vector of length two that defines the proportional distance down the x-axis (0 is all the way to the left) and proportional distance up the y-axis (0 is at the bottom). Thus, the code below puts the legend in the upper-left corner of the plot (i.e., 0 is to the left, 1 is top).\n\np2 +\n  theme(legend.position=c(0,1))\n\n\n\n\n\n\n\n\nHowever, the legend is centered on this coordinate. The justification of the legend is adjusted with legend.justification= in theme(). Again a vector of length two is required here with the first value serving like hjust= and the second value like vjust=. Here I would like the upper-left corner of the legend to be a little right (so a negative first value) and a little down (so a value greater than 1 for the second value) from the “coordinate” in legend.position=.\n\np2 +\n  theme(legend.position=c(0,1),\n        legend.justification=c(-0.25,1.1))\n\n\n\n\n\n\n\n\nAnother example where legend.position= puts the legend in the upper-right corner and legend.justification= positions the upper-right corner of the legend a little to the left and down from the legend.position= “coordinate.”\n\np2 +\n  theme(legend.position=c(1,1),\n        legend.justification=c(1.25,1.1))"
  },
  {
    "objectID": "blog/posts/2023-2-15_Vaisviletal2022_ROC/index.html",
    "href": "blog/posts/2023-2-15_Vaisviletal2022_ROC/index.html",
    "title": "Vaisvil et al. (2022) Hatching Date Figure",
    "section": "",
    "text": "Introduction\nVaisvil et al. (2022) examined the effect of varying water levels on the timing of hatch dates and relative abundance of young-of-the-year Largemouth Bass (Micropterus salmoides) in a southwestern U.S. reservoir. Figure 4 of their publication showed the rate of change of water levels in the reservoir by day for 2017 and 2018, with the range of bass hatch dates noted. Interestingly they also included satellite images of the reservoir at the begninning of the hatch date range. In this post I recreate their Figure 4 using ggplot2.\n \n\n\nGetting Setup\nThe following packages are loaded for use below.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\nlibrary(scales)     # for breaks_width(), label_date()\nlibrary(patchwork)  # for placing subpanels into one plot\n\n \n\n\nPlotting Raster Image\nFigure 4 in Vaisvil et al. (2022) consists of four panels with the left two panels being satellite images. I “clipped” both satellite images and saved them as PNG files for use here. Note that both images are the same size (number of pixels in both the horizontal and vertical directions).\nreadPNG() from png reads a PNG file and returns a three dimensional matrix of x- and y- pixel positions as the rows and columns and four “layers” corresponding to the red, green, blue, and transparency values for a pixel. This matrix of RGBT values, however, is indexed from the upper-left whereas plotting is usually indexed from the lower-left (i.e., from 0,0). This provides several problems that are commonly discussed on forums1. To address these issues I created a small helper function below based on several suggestions I found in the forums.\n1 For example, here.\npng2df &lt;- function(f) {\n  ## Read the PNG file\n  df &lt;- png::readPNG(f)\n  ## Convert to data.frame of x-y pixel positions and RGBT color for that pixel.\n  ##   Note that the Y variable is reversed as indexing starts in the\n  ##   upper-left (rather than lower-left for plotting)\n  grd &lt;- expand.grid(1:nrow(df),1:ncol(df))\n  dim(df) &lt;- c(nrow(df)*ncol(df),4)\n  df &lt;- cbind(grd,as.data.frame(df)) |&gt;\n    mutate(RGB=rgb(V1,V2,V3,V4),\n           y=-Var1,\n           y=y+max(abs(y))+1) |&gt;\n    select(x=Var2,y=y,RGB)\n  df\n}\n\nThis function takes the PNG filename as its sole argument and returns a data frame of x-y pixel coordinates and the RGBT values to plot at those coordinates.\n\nimg17 &lt;- png2df(\"EB_2017.png\")\nhead(img17)\n\n#R|    x   y       RGB\n#R|  1 1 455 #838577FF\n#R|  2 1 454 #717365FF\n#R|  3 1 453 #7A7C6CFF\n#R|  4 1 452 #737665FF\n#R|  5 1 451 #7C7F6DFF\n#R|  6 1 450 #787B69FF\n\n\nThe y-to-x aspect ratio of this image is needed for plotting purposes below (and is saved here in ar).\n\nar &lt;- max(img17$y)/max(img17$x)\n\nThe image can be “plotted” by giving this image data frame object with appropriate mapping of aesthetics to ggplot() and then using geom_raster() along with scale_fill_identity()2. theme_void() is used here to remove all axes, margins around the plot, etc.\n2 scale_fill_identity() ensures that the actual RGBT values in the RGB variable are used, rather than assigning colors to those values.\nggplot(data=img17,mapping=aes(x=x,y=y,fill=RGB)) + \n  geom_raster() +\n  scale_fill_identity() +\n  theme_void()\n\n\n\n\n\n\n\n\nIt is evident here that the plotted image will be “stretched” to fill the shape of the plotting device (square here). This unwanted behavior can be avoided by setting aspect.ratio= in theme() to the ar value calculated above. Unwanted white space3 around the plot can be removed by eliminating the scale expansion with expansion().\n3 This space is not evident on the default white background of this webpage. Change to dark mode (slider in upper-right corner of the title bar) to see the white space.\nggplot(data=img17,mapping=aes(x=x,y=y,fill=RGB)) + \n  geom_raster() +\n  scale_fill_identity() +\n  scale_x_continuous(expand=expansion(mult=0)) +\n  scale_y_continuous(expand=expansion(mult=0)) +\n  theme_void() +\n  theme(aspect.ratio=ar)\n\n\n\n\n\n\n\n\nFigure 4 in Vaisvil et al. (2022) also had a letter “tag” to the left of the figure. Adding this is a bit of work. First, the letter can be added with annotate() using geom=\"text\" with the tag label in label= and x= at the smallest value (i.e.,-Inf) and y= at the largest value (i.e., Inf) to center the label on the very upper-left corner of the plot. An hjust= and vjust= of 1.5 will move the label down and to the left so that it is fully visible. To make room for this label the left-side plot margin must be increased (from 0, the default in theme_void()), the aspect ratio must be adjusted slightly,4 and coord_cartesian(clip=\"off\") must be used so that the label is not “clipped” because it is outside the plot region.\n4 I am not sure how to automatically modify the aspect ratio to adjust for the width of the letter. In this example I used trial-and-error to increase it by 5% (i.e., multiplying ar by 1.05).\neb17 &lt;- ggplot(data=img17,mapping=aes(x=x,y=y,fill=RGB)) + \n  geom_raster() +\n  scale_fill_identity() +\n  annotate(geom=\"text\",label=\"A\",x=-Inf,y=Inf,\n           hjust=1.5,vjust=1.5,size=5) +\n  coord_cartesian(clip=\"off\") +\n  scale_x_continuous(expand=expansion(mult=0)) +\n  scale_y_continuous(expand=expansion(mult=0)) +\n  theme_void() +\n  theme(plot.margin=margin(l=5,unit=\"mm\"),\n        aspect.ratio=1.05*ar)\neb17\n\n\n\n\n\n\n\n\nThe plot above is assigned to the eb17 object for later use. I created a similar object for the 2018 image.5\n5 Called eb18, but code not shown.\n\n\n \n\n\nPlotting Water Level Fluctuations\n\nWater Fluctuation Data\nThe two right panels of Figure 4 in Vaisvil et al. (2022) are line plots that show the daily rate of change in water levels by day for each year, with the beginning and ending hatch dates for the bass highlighted. The data to construct this plot for both years were provided in JFWM-21-071.S3.csv from Supplement S3 with the published paper. One of the cells in the rate of change variable shows as “#REF!”, which implies a formula error in when I suspect was the original Excel file. I decided to treat this as missing data and thus needed to include the na= code in read.csv() below. I also immediately changed most of the variable names to be more concise.\n\ndfroc &lt;- read.csv(\"JFWM-21-071.S3.csv\",na=c(\"\",\"#REF!\")) |&gt;\n  select(date=Date_Time,\n         storage=Storage..km.3.,\n         elev=Elevation..m.,\n         roc=Rate.of.Change..cm.)\n\nThe date variable is appropriately read in as a character variable and, thus, needs to be converted to a “date” type for plotting. Unfortunately, the date data were not all recorded in the same format; some “dates” included “0:00”, whereas others did not, as shown below.\n\nFSA::headtail(dfroc)\n\n#R|               date   storage     elev    roc\n#R|  1   1/1/2017 0:00 0.2514777 1314.295 5.4864\n#R|  2   1/2/2017 0:00 0.2531534 1314.349 5.4864\n#R|  3   1/3/2017 0:00 0.2548389 1314.404 6.0960\n#R|  726    12/29/2018 0.1396767 1310.006 4.5720\n#R|  727    12/30/2018 0.1406878 1310.052 4.2672\n#R|  728    12/31/2018 0.1416372 1310.094 6.0960\n\n\nTo address this I used word() from stringr which finds each “word” (i.e., separated by a space) in a string. If the second argument to word() is set to 1 then it will return the first “word”, in this case just the date without the “0:00”. mdy() from lubridate was then used to convert date to a proper date variable and year() from lubridate was used to create a new variable that was just the year.\n\ndfroc &lt;- dfroc |&gt;\n  mutate(date=stringr::word(date,1),\n         date=lubridate::mdy(date),\n         yr=lubridate::year(date))\nFSA::headtail(dfroc)\n\n#R|            date   storage     elev    roc   yr\n#R|  1   2017-01-01 0.2514777 1314.295 5.4864 2017\n#R|  2   2017-01-02 0.2531534 1314.349 5.4864 2017\n#R|  3   2017-01-03 0.2548389 1314.404 6.0960 2017\n#R|  726 2018-12-29 0.1396767 1310.006 4.5720 2018\n#R|  727 2018-12-30 0.1406878 1310.052 4.2672 2018\n#R|  728 2018-12-31 0.1416372 1310.094 6.0960 2018\n\n\nFinally, separate data frames for 2017 and 2018 were created.\n\ndfroc17 &lt;- dfroc |&gt;\n  filter(yr==2017)\ndfroc18 &lt;- dfroc |&gt;\n  filter(yr==2018)\n\n \n\n\nBass Hatching Data\nHatching dates for individual bass were recorded and provided in JFWM-21-071.S2.csv from Supplement S2 with the published paper. There were no issues with these data so I simply made the variable names more concise and converted the dates to a proper date variable similar to above.\n\ndfhat &lt;- read.csv(\"JFWM-21-071.S2.csv\") |&gt;\n  select(id=FISH.ID,\n         yr=Year,\n         date=Hatch_date) |&gt;\n  mutate(date=lubridate::mdy(date))\nFSA::headtail(dfhat)\n\n#R|     id   yr       date\n#R|  1   2 2018 2018-04-13\n#R|  2   3 2018 2018-04-18\n#R|  3   5 2018 2018-04-27\n#R|  62 82 2017 2017-05-05\n#R|  63 83 2017 2017-04-25\n#R|  64 84 2017 2017-05-07\n\n\nFigure 4 in Vaisvil et al. (2022) requires the earliest and latest hatch dates for each year, which are compute below and then separated by year.\n\ndfhatsum &lt;- dfhat |&gt;\n  group_by(yr) |&gt;\n  summarize(minhat=min(date),\n            maxhat=max(date))\ndfhatsum\n\n#R|  # A tibble: 2 × 3\n#R|       yr minhat     maxhat    \n#R|    &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n#R|  1  2017 2017-04-14 2017-05-29\n#R|  2  2018 2018-04-13 2018-05-28\n\n\n\ndfhatsum17 &lt;- dfhatsum |&gt;\n  filter(yr==2017)\ndfhatsum18 &lt;- dfhatsum |&gt;\n  filter(yr==2018)\n\n \n\n\nGraphing Water Fluctuations\nBoth water fluctuation graphs in Figure 4 in Vaisvil et al. (2022) have the same y-axis range and breaks, based on the range of the data in both years combined. Those limits are computed below from the combined data frame.\n\n( lmts &lt;- range(dfroc$roc,na.rm=TRUE) )\n\n#R|  [1] -18.2880  22.5552\n\nbrks &lt;- seq(-15,20,5)\n\nThe code below creates the water fluctuations plot for 2017. There are four main parts to this code. First, ggplot() and geom_line() are used to plot the rate of change versus date as a line, which was made slightly larger with linewidth= to match Figure 4. Second, two geom_vline()s are used to mark the beginning and end hatch dates (as computed above) and geom_hline() is used to place a reference line at a rate of change of 0. Third, the axes are more specifically defined. The x-axis is defined with scale_x_date() using breaks=breaks_width(\"month\") to show a break at each month and labels=label_date(\"%b\") to use character abbreviations for those months as described in this post. Fourth, the black-and-white theme is applied, the title for the x-axis is removed, and the gridlines on the plot are removed.\n\nroc17 &lt;- ggplot(data=dfroc17,mapping=aes(x=date,y=roc)) +\n  geom_line(linewidth=1) +\n  geom_vline(xintercept=dfhatsum17$minhat,linetype=\"dashed\",linewidth=1) +\n  geom_vline(xintercept=dfhatsum17$maxhat,linetype=\"dashed\",linewidth=1) +\n  geom_hline(yintercept=0,linetype=\"dotted\",linewidth=1) +\n  scale_y_continuous(name=\"Rate-of-change (cm/d)\",expand=expansion(mult=0.01),\n                     limits=lmts,breaks=brks) +\n  scale_x_date(expand=expansion(mult=0),\n               breaks=breaks_width(\"month\"),labels=label_date(\"%b\")) +\n  theme_bw(base_size=14) +\n  theme(axis.title.x=element_blank(),\n        panel.grid=element_blank())\nroc17\n\n\n\n\n\n\n\n\nSimilar code was used to construct a similar plot for the 2018 data.6\n6 Called roc18, but code not shown.\n\n\n \n\n\n\nRecreating Figure 4\nBefore Figure 4 can be recreated the x-axis labels must be removed from the water fluctuations plot for 2017.7\n7 I would have done this originally, but I was illustrating above how to make a proper stand-alone plot for that year.\nroc17 &lt;- roc17 +\n  theme(axis.text.x=element_blank())\n\nWith that, Figure 4 in Vaisvil et al. (2022) can be constructed by combining the four plots created above. Note that the the widths= in plot_layout() took some trial-and-error so that the satellite images looked correct.8\n8 Explore how to do this by trying different values in that argument.\neb17 + roc17 + eb18 + roc18 +\n  plot_layout(ncol=2,widths=c(0.3,0.7))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMy recreation of Figure 4 appears different from that in Vaisvil et al. (2022). It appears that they removed some rates-of-change values greater than 20 cm/d.\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nVaisvil, A., C. A. Caldwell, and E. Frey. 2022. Water-level fluctuations and water temperature effects on young-of-year Largemouth Bass in a Southwest irrigation reservoir. Journal of Fish and Wildlife Management 13(2):534–543.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Vaisvil Et Al. (2022) {Hatching} {Date} {Figure}},\n  date = {2023-02-15},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-2-15_Vaisviletal2022_ROC},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, February 15. Vaisvil et al. (2022) Hatching Date\nFigure. https://fishr-core-team.github.io/fishR//blog/posts/2023-2-15_Vaisviletal2022_ROC."
  },
  {
    "objectID": "blog/posts/2021-6-1_residPlot-replacement/index.html",
    "href": "blog/posts/2021-6-1_residPlot-replacement/index.html",
    "title": "Replace residPlot() with ggplot",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. One function from nlstools is also used but the entire package is not loaded here. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)          ## for peek(), vbFuns(), vbStarts(), SpotVA1\nlibrary(dplyr)        ## for mutate()\nlibrary(ggplot2)\nlibrary(patchwork)    ## placing plots (in conclusion)\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nWarning\n\n\n\nSome functions illustrated below were in the FSA package but have now been removed and put into the non-released FSAmisc package that I maintain. These functions are used below only to show what could be done in older versions of FSA but should now be done as described in this post. DO NOT USE any of the functions below that begin with FSAmisc::.\n\n\n \n\nIntroduction\nWe deprecated residPlot() from FSA v0.9.0 and fully removed it by the start of 2022. We took this action to make FSA more focused on fisheries applications and to eliminate “black box” functions. residPlot() was originally designed for students to quickly visualize residuals from one- and two-way ANOVAs and simple, indicator variable, and logistic regressions.1 We now feel that students are better served by learning how to create these visualizations using methods provided by ggplot2, which require more code, but are more modern, flexible, and transparent.\n1 Over time functionality for non-linear regressions was added.The basic plots produced by residPlot() are recreated here using ggplot2 to provide a resource to help users that relied on residPlot() transition to ggplot2.\n \n\n\nExample Data\nMost examples below use the Mirex data set from FSA, which contains the concentration of mirex in the tissue and the body weight of two species of salmon (chinook and coho) captured in six years. The year variable is converted to a factor for modeling purposes and a new variable is created that indicates if the mirex concentration was greater that 0.2 or not. These same data were used in this post about removing fitPlot() from FSA.2\n2 peek() from FSA is used to examine a portion of the data from evenly-spaced row.\ndata(Mirex,package=\"FSA\")\nMirex &lt;- Mirex |&gt;\n  mutate(year=factor(year),\n         gt2=ifelse(mirex&gt;0.2,1,0))\npeek(Mirex,n=10)\n\n#R|      year weight mirex species gt2\n#R|  1   1977   0.41  0.16 chinook   0\n#R|  14  1977   3.29  0.23    coho   1\n#R|  27  1982   0.70  0.10    coho   0\n#R|  41  1982   5.09  0.27    coho   1\n#R|  54  1986   1.82  0.12 chinook   0\n#R|  68  1986   8.40  0.13 chinook   0\n#R|  81  1992  10.00  0.48 chinook   1\n#R|  95  1996   5.70  0.16    coho   0\n#R|  108 1999   5.11  0.11    coho   0\n#R|  122 1999  11.82  0.09 chinook   0\n\n\n \n\n\nOne-Way ANOVA\nThe code below fits a one-way ANOVA model to examine if mean weight differs by species.\n\naov1 &lt;- lm(weight~species,data=Mirex)\nanova(aov1)\n\n#R|  Analysis of Variance Table\n#R|  \n#R|  Response: weight\n#R|             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n#R|  species     1  282.4 282.399  27.657 6.404e-07 ***\n#R|  Residuals 120 1225.3  10.211                      \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \nresidPlot() from FSA produced a boxplot of residuals by group (left) and a histogram of residuals (right).\n\nFSAmisc::residPlot(aov1)\n\n\n\n\n\n\n\n\n \nA data frame of the two variables used in the ANOVA appended with the fitted values and residuals from the model fit must be made to construct this plot using ggplot(). Studentized residuals are included below in case you would prefer to plot them.3\n3 These are “internally” Studentized residuals. “Externally” Studentized residuals can be obtained with rstandard().\ntmp &lt;- Mirex |&gt;\n  select(weight,species) |&gt;\n  mutate(fits=fitted(aov1),\n         resids=resid(aov1),\n         sresids=rstudent(aov1))\npeek(tmp,n=8)\n\n#R|      weight species     fits     resids    sresids\n#R|  1     0.41 chinook 6.314776 -5.9047761 -1.8814369\n#R|  17    4.77    coho 3.257091  1.5129091  0.4762846\n#R|  35    2.92 chinook 6.314776 -3.3947761 -1.0710643\n#R|  52    1.70    coho 3.257091 -1.5570909 -0.4902213\n#R|  70    9.53 chinook 6.314776  3.2152239  1.0139117\n#R|  87    0.90    coho 3.257091 -2.3570909 -0.7430562\n#R|  105   2.61    coho 3.257091 -0.6470909 -0.2035546\n#R|  122  11.82 chinook 6.314776  5.5052239  1.7507263\n\n\n \nThe histogram of residuals is constructed with geom_histogram() below. Note that the color of the histogram bars are modified and the bin width is set to better control the number of bars in the histogram. Finally, the bottom multiplier for the y-axis is set to zero so that that histogram bars do not “hover” above the x-axis.\n\nggplot(data=tmp,mapping=aes(x=resids)) +\n  geom_histogram(color=\"gray30\",fill=\"gray70\",binwidth=0.5) +\n  scale_y_continuous(expand=expansion(mult=c(0,0.05)))\n\n\n\n\n\n\n\n\n \nThe boxplot of residuals by group (species in this case) is constructed with geom_boxplot() below (again controlling the colors of the boxplot).\n\nggplot(data=tmp,mapping=aes(x=species,y=resids)) +\n  geom_boxplot(color=\"gray30\",fill=\"gray70\")\n\n\n\n\n\n\n\n\n \nThese plots can be further modified using methods typical for ggplot (see conclusion section).\n \n\n\nTwo-Way ANOVA\nThe code below fits a two-way ANOVA model to examine if mean weight differs by species, by year, or by the interaction between species and year.\n\naov2 &lt;- lm(weight~year*species,data=Mirex)\nanova(aov2)\n\n#R|  Analysis of Variance Table\n#R|  \n#R|  Response: weight\n#R|                Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n#R|  year           5 281.86  56.373  6.9954 1.039e-05 ***\n#R|  species        1 221.69 221.689 27.5099 7.648e-07 ***\n#R|  year:species   5 117.69  23.538  2.9208   0.01628 *  \n#R|  Residuals    110 886.44   8.059                      \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \nresidPlot() from FSA showed a boxplot of residuals by each combination of the two factor variables (left) and a histogram of the residuals (right).\n\nFSAmisc::residPlot(aov2)\n\n\n\n\n\n\n\n\n \nA data frame of the three variables used in the ANOVA appended with the fitted values and residuals from the model fit must be constructed.\n\ntmp &lt;- Mirex |&gt;\n  select(weight,species,year) |&gt;\n  mutate(fits=fitted(aov2),\n         resids=resid(aov2),\n         sresids=rstudent(aov2))\n\n \nThe histogram of residuals is constructed exactly as before and won’t be repeated here. The boxplot of residuals by group is constructed with one of the factor variables on the x-axis and the other factor variable as separate facets.4\n4 These two variables can, of course, be exchanged. However, I generally prefer to have the variable with more levels on the x-axis.\nggplot(data=tmp,mapping=aes(x=year,y=resids)) +\n  geom_boxplot(color=\"gray30\",fill=\"gray70\") +\n  facet_wrap(vars(species))\n\n\n\n\n\n\n\n\n \n\n\nSimple Linear Regression\nThe code below fits a simple linear regression for examining the relationship between mirex concentration and salmon weight.\n\nslr &lt;- lm(mirex~weight,data=Mirex)\nanova(slr)\n\n#R|  Analysis of Variance Table\n#R|  \n#R|  Response: mirex\n#R|             Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \n#R|  weight      1 0.22298 0.222980  26.556 1.019e-06 ***\n#R|  Residuals 120 1.00758 0.008396                      \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \nresidPlot() from FSA showed a scatterplot of residuals versus fitted values (left) and a histogram of residuals (right).\n\nFSAmisc::residPlot(slr)\n\n\n\n\n\n\n\n\n \nA data frame of the two variables used in the ANOVA appended with the fitted values and residuals from the model fit must be constructed.\n\ntmp &lt;- Mirex |&gt;\n  select(weight,mirex) |&gt;\n  mutate(fits=fitted(slr),\n         resids=resid(slr),\n         sresids=rstudent(slr))\n\n \nThe histogram of residuals is constructed exactly as before and won’t be repeated here. The scatterplot of residuals versus fitted values is constructed with geom_point() as below. Note that geom_hline() is used to place the horizontal line at 0 on the y-axis.\n\nggplot(data=tmp,mapping=aes(x=fits,y=resids)) +\n  geom_point() +\n  geom_hline(yintercept=0,linetype=\"dashed\")\n\n\n\n\n\n\n\n\n \nIt is also possible to include a loess smoother to help identify a possible nonlinearity in this residual plot.\n\nggplot(data=tmp,mapping=aes(x=fits,y=resids)) +\n  geom_point() +\n  geom_hline(yintercept=0,linetype=\"dashed\") +\n  geom_smooth()\n\n\n\n\n\n\n\n\n \n\n\nIndicator Variable Regression\nThe code below fits an indicator variable regression to examine if the relationship between mirex concentration and salmon weight differs betwen species.\n\nivr &lt;- lm(mirex~weight*species,data=Mirex)\nanova(ivr)\n\n#R|  Analysis of Variance Table\n#R|  \n#R|  Response: mirex\n#R|                  Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \n#R|  weight           1 0.22298 0.222980 26.8586 9.155e-07 ***\n#R|  species          1 0.00050 0.000498  0.0600   0.80690    \n#R|  weight:species   1 0.02744 0.027444  3.3057   0.07158 .  \n#R|  Residuals      118 0.97964 0.008302                      \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \nresidPlot() from FSA is the same for an IVR as for an SLR, except that the points on the residual plot (left) had different colors for the different groups.\n\nFSAmisc::residPlot(ivr)\n\n\n\n\n\n\n\n\n \nA data frame of the three variables used in the ANOVA appended with the fitted values and residuals from the model fit must be constructed.\n\ntmp &lt;- Mirex |&gt;\n  select(weight,mirex,species) |&gt;\n  mutate(fits=fitted(ivr),\n         resids=resid(ivr),\n         sresids=rstudent(ivr))\n\n \nThe histogram of residuals is constructed exactly as before and won’t be repeated here. The scatterplot of residuals versus fitted values is constructed with geom_point(). Note that color= and shape= are both set equal to the factor variable to change the color and plotting character to represent the different groups.\n\nggplot(data=tmp,mapping=aes(x=fits,y=resids,color=species,shape=species)) +\n  geom_point() +\n  geom_hline(yintercept=0,linetype=\"dashed\")\n\n\n\n\n\n\n\n\n \n\n\nNonlinear Regression\nThe following code fits a von Bertalanffy growth function (VBGF) to the total length and age data for spot found in the SpotVA1 data frame built into FSA. Fitting the VBGF is described in more detail here.\n\nvb &lt;- vbFuns()\nvbs &lt;- vbStarts(tl~age,data=SpotVA1)\nnlreg &lt;- nls(tl~vb(age,Linf,K,t0),data=SpotVA1,start=vbs)\n\n \nresidPlot() from FSA produced plots exactly as for a simple linear regression.\n\nFSAmisc::residPlot(nlreg)\n\n\n\n\n\n\n\n\n \nA data frame of the two variables used in the ANOVA appended with the fitted values and residuals from the model fit must be constructed. The rstudent() function does not work for non-linear models, but the Studentized residuals are computed with nlsResiduals() from nlstools. However, these values are “buried” in the Standardized residuals column of the resi2 matrix returned by that function; thus, it takes a little work to extract them as shown below.\n\ntmp &lt;- SpotVA1 |&gt;\n  select(tl,age) |&gt;\n  mutate(fits=fitted(nlreg),\n         resids=resid(nlreg),\n         sresids=nlstools::nlsResiduals(nlreg)$resi2[,\"Standardized residuals\"])\npeek(tmp,n=8)\n\n#R|        tl age      fits     resids    sresids\n#R|  1    6.5   0  7.348034 -0.8480343 -0.8051925\n#R|  58   8.3   1  9.251581 -0.9515812 -0.9035082\n#R|  115  8.5   1  9.251581 -0.7515812 -0.7136121\n#R|  173  9.7   1  9.251581  0.4484188  0.4257650\n#R|  230  9.8   2 10.771696 -0.9716957 -0.9226066\n#R|  288 10.5   2 10.771696 -0.2716957 -0.2579700\n#R|  345 11.5   3 11.985613 -0.4856128 -0.4610802\n#R|  403 13.9   4 12.955010  0.9449900  0.8972498\n\n\n \nOnce this data frame is constructed the residual plot and histogram of residuals are constructed exactly as for linear regression and won’t be repeated here.\n \n\n\nConclusion\nThe residPlot() function in FSA was removed by 2022. This post describes a more transparent (i.e., not a “black box”) and flexible set of methods for constructing similar plots using ggplot2 for those who will need to transition away from using residPlot().5\n5 Different “residual plots” are available in plot() (from base R when given an object from lm()), car::residualPlots(), DHARMa::plotResiduals(), and ggResidpanel::resid_panel().6 patchwork is used here to place the plots side-by-side.As mentioned in the examples above, each plot can be modified further using typical methods for ggplot2. These changes were not illustrated above to minimize the amount of code shown in this post. However, as an example, the code below shows a possible modification of the IVR residual plot shown above.6\n\n## Recreate the data frame of results for the IVR\ntmp &lt;- Mirex |&gt;\n  select(weight,mirex,species) |&gt;\n  mutate(fits=fitted(ivr),\n         resids=resid(ivr),\n         sresids=rstudent(ivr))\n\n## Create a general theme that can be applied to both plots\ntheme_DHO &lt;- theme_bw() +\n  theme(panel.grid.major=element_line(color=\"gray90\",linetype=\"dashed\"),\n        panel.grid.minor=element_blank(),\n        axis.title=element_text(size=rel(1.25)),\n        axis.text=element_text(size=rel(1.1)),\n        legend.position=c(0,1),\n        legend.justification=c(-0.05,1.02),\n        legend.title=element_blank(),\n        legend.text=element_text(size=rel(1.1)))\n\n## Construct the residual plot\nr1 &lt;- ggplot(tmp,aes(x=fits,y=sresids,color=species)) +\n  geom_point(size=1.5,alpha=0.5) +\n  geom_hline(yintercept=0,linetype=\"dashed\") +\n  geom_smooth(se=FALSE) +\n  scale_y_continuous(name=\"Studentized Residuals\") +\n  scale_x_continuous(name=\"Fitted Values\") +\n  scale_color_manual(values=c(\"#E69F00\",\"#0072B2\"),guide=\"none\") +\n  theme_DHO\n\n## Construct the histogram of residuals\nr2 &lt;- ggplot(tmp,aes(x=sresids,color=species,fill=species)) +\n  geom_histogram(alpha=0.5,binwidth=0.25) +\n  scale_y_continuous(name=\"Frequency\",expand=expansion(mult=c(0,0.05))) +\n  scale_x_continuous(name=\"Studentized Residuals\") +\n  scale_color_manual(values=c(\"#E69F00\",\"#0072B2\"),\n                     aesthetics=c(\"color\",\"fill\")) +\n  theme_DHO\n\n## Put plots side-by-side (the \"+\" is provided by patchwork)\nr1 + r2\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2021,\n  author = {H. Ogle, Derek},\n  title = {Replace {residPlot()} with Ggplot},\n  date = {2021-06-01},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2021-6-1_residPlot-replacement},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2021, June 1. Replace residPlot() with ggplot. https://fishr-core-team.github.io/fishR//blog/posts/2021-6-1_residPlot-replacement."
  },
  {
    "objectID": "blog/posts/2021-5-25_fitPlot-replacement/index.html",
    "href": "blog/posts/2021-5-25_fitPlot-replacement/index.html",
    "title": "Replace fitPlot() with ggplot",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)         ## for peek()\nlibrary(emmeans)     ## for emmeans()\nlibrary(dplyr)       ## for mutate(), select(), filter(), group_by(), summarize()\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nWarning\n\n\n\nSome functions illustrated below were in the FSA package but have now been removed and put into the non-released FSAmisc package that I maintain. These functions are used below only to show what could be done in older versions of FSA but should now be done as described in this post. DO NOT USE any of the functions below that begin with FSAmisc::.\n\n\n \n\nIntroduction\nWe deprecated fitPlot() from FSA v0.9.0 and fully removed it by the start of 2022. We took this action to make FSA more focused on fisheries applications and to eliminate “black box” functions. fitPlot() was originally designed for students to quickly visualize the results of one- and two-way ANOVAs and simple, indicator variable, and logistic regressions.1 We now feel that students are better served by learning how to create these visualizations using methods provided by ggplot2, which require more code, but are more modern, flexible, and transparent.\n1 Over time functionality for non-linear regressions was added.The basic plots produced by fitPlot() are recreated here using ggplot2 to provide a resource to help users that relied on fitPlot() transition to ggplot2.\n \n\n\nExample Data\nExamples below use the Mirex data set from FSA, which contains the concentration of mirex in the tissue and the body weight of two species of salmon (chinook and coho) captured in six years. The year variable is converted to a factor for modeling purposes and a new variable is created that indicates if the mirex concentration was greater that 0.2 or not. This new variable is used to demonstrate a logistic regression.2\n2 peek() from FSA is used to examine a portion of the data from evenly-spaced row.\ndata(Mirex,package=\"FSA\")\nMirex &lt;- Mirex |&gt;\n  mutate(year=factor(year),\n         gt2=ifelse(mirex&gt;0.2,1,0))\npeek(Mirex,n=10)\n\n#R|      year weight mirex species gt2\n#R|  1   1977   0.41  0.16 chinook   0\n#R|  14  1977   3.29  0.23    coho   1\n#R|  27  1982   0.70  0.10    coho   0\n#R|  41  1982   5.09  0.27    coho   1\n#R|  54  1986   1.82  0.12 chinook   0\n#R|  68  1986   8.40  0.13 chinook   0\n#R|  81  1992  10.00  0.48 chinook   1\n#R|  95  1996   5.70  0.16    coho   0\n#R|  108 1999   5.11  0.11    coho   0\n#R|  122 1999  11.82  0.09 chinook   0\n\n\n \n\n\nOne-Way ANOVA\nThe code below fits a one-way ANOVA model to examine if mean weight differs by species.\n\naov1 &lt;- lm(weight~species,data=Mirex)\nanova(aov1)\n\n#R|  Analysis of Variance Table\n#R|  \n#R|  Response: weight\n#R|             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n#R|  species     1  282.4 282.399  27.657 6.404e-07 ***\n#R|  Residuals 120 1225.3  10.211                      \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \nThere are at least two simple ways to visualize results from a one-way ANOVA. First, summarized means of raw data with 95% confidence intervals derived from the standard error, sample size, and degrees-of-freedom specific to each group are shown. These are computed below.\n\nsumdata &lt;- Mirex |&gt;\n  group_by(species) |&gt;\n  summarize(n=n(),\n            mn=mean(weight),\n            se=se(weight)) |&gt;\n  mutate(lci=mn-qt(0.975,df=n-1)*se,\n         uci=mn+qt(0.975,df=n-1)*se)\nsumdata\n\n#R|  # A tibble: 2 × 6\n#R|    species     n    mn    se   lci   uci\n#R|    &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#R|  1 chinook    67  6.31 0.474  5.37  7.26\n#R|  2 coho       55  3.26 0.279  2.70  3.82\n\n\n \nSecond, marginal means may be predicted or estimated from the fitted model.3 The main difference from above is that confidence intervals for the marginal means use an “overall” standard deviation and degrees-of-freedom estimated from across all groups. The estimated marginal means may be computed with with emmeans() from emmeans.4\n3 These are discussed in detail in this vignette from the emmeans package.4 They may also be computed with predict(aov1,newdata=data.frame(species=c(\"chinook\",\"coho\")),interval=\"confidence\").\naov1mc &lt;- emmeans::emmeans(aov1,specs=pairwise~species)\naov1mcs &lt;- summary(aov1mc)\naov1mcs$emmeans\n\n#R|   species emmean    SE  df lower.CL upper.CL\n#R|   chinook   6.31 0.390 120     5.54     7.09\n#R|   coho      3.26 0.431 120     2.40     4.11\n#R|  \n#R|  Confidence level used: 0.95\n\n\n \nfitPlot() from FSA used the summarized means with 95% confidence intervals for both species.\n\nFSAmisc::fitPlot(aov1)\n\n\n\n\n\n\n\n\n \n\nUsing Manually Summarized Means\nThe summarized means saved in sumdata above can be plotted as shown below to recreate the fitPlot() result. width=0.1 in geom_errorbar() is used to reduce the width of the “caps” at the confidence values and group=1 is needed in geom_line() as there is only one point for each factor level. Changes (themes, colors, labels, etc) to this basic plot can be made as usual for ggplot()s (and is illustrated further below).\n\nggplot(data=sumdata,mapping=aes(x=species)) +\n  geom_errorbar(mapping=aes(ymin=lci,ymax=uci),width=0.1) +\n  geom_line(mapping=aes(y=mn,group=1)) +\n  geom_point(mapping=aes(y=mn))\n\n\n\n\n\n\n\n\n \n\n\nUsing Built-In Functions for Summarized Means\nThis plot can also be constructed without having previously summarized the group means by using stat_summary() coupled with mean_cl_normal() and mean(). Below note how each geom= in each stat_summary() mirrors what was used above. Also note the use of width=0.1 and group=1 here as done above.\n\nggplot(data=Mirex,mapping=aes(x=species,y=weight)) +  \n  stat_summary(fun.data=mean_cl_normal,geom=\"errorbar\",width=0.1) +  \n  stat_summary(fun=mean,geom=\"line\",mapping=aes(group=1)) +  \n  stat_summary(fun=mean,geom=\"point\")\n\n\n\n\n\n\n\n\n \n\n\nUsing Marginal Means from emmeans\nThe estimated marginal means may be plotted similarly to the manually summarized means. However, the aov1mcs$emmeans data frame created above is used, which also requires using lower.CL and upper.CL for the ymin= and ymax= in geom_errorbar() and emmean for the y= mean value in geom_line() and geom_point().5\n5 Review the output from aov1mcs$emmeans above taking special note of the variable names.\nggplot(data=aov1mcs$emmeans,mapping=aes(x=species)) +\n  geom_errorbar(mapping=aes(ymin=lower.CL,ymax=upper.CL),width=0.1) +\n  geom_line(mapping=aes(y=emmean,group=1)) +\n  geom_point(mapping=aes(y=emmean))\n\n\n\n\n\n\n\n\n \n\n\n\nTwo-Way ANOVA\nThe code below fits a two-way ANOVA model to examine if mean weight differs by species, by year, or by the interaction between species and year.\n\naov2 &lt;- lm(weight~year*species,data=Mirex)\nanova(aov2)\n\n#R|  Analysis of Variance Table\n#R|  \n#R|  Response: weight\n#R|                Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n#R|  year           5 281.86  56.373  6.9954 1.039e-05 ***\n#R|  species        1 221.69 221.689 27.5099 7.648e-07 ***\n#R|  year:species   5 117.69  23.538  2.9208   0.01628 *  \n#R|  Residuals    110 886.44   8.059                      \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \nfitPlot() from FSA used the mean with 95% confidence interval for all combinations of species and year.\n\nFSAmisc::fitPlot(aov2)\n\n\n\n\n\n\n\n\n \n\nUsing Built-In Functions for Summarized Means\nAgain, stat_summary() can be used to efficiently calculate and then plot the 95% confidence intervals and means similar to what was shown above for a one-way ANOVA. However, there are three major differences.\nFirst, in the main ggplot() call the color of the points and lines is mapped to one of the two factor variables (species in this case) whereas the other factor variable is mapped to x=.6 Second, the group= aesthetic for the line geom must be set to the factor that describes how the lines should be connected, which will be the same as the variable mapped to the color aesthetic (e.g., species in this case). Third, the intervals and (possibly) the points at each level on the x-axis will overlap if they are not “dodged” a small amount.7 The “dodge” amount should be set outside the geom()s so that each geom uses the same amount of “dodging.” This will assure that the intervals, points, and connecting lines for each level defined by the colors align. Below this “dodge” amount is set with position_dodge() and saved to an object called pd which is then set equal to position= in each geom().\n6 These two variables can, of course, be exchanged. However, I generally prefer to have the variable with more levels on the x-axis.7 Note this same problem occurs for the fitPlot(), though there is no simple solution for it.\npd &lt;- position_dodge(width=0.2)\nggplot(data=Mirex,mapping=aes(x=year,y=weight,color=species)) +  \n  stat_summary(fun.data=mean_cl_normal,geom=\"errorbar\",width=0.2,position=pd) + \n  stat_summary(fun=mean,geom=\"line\",mapping=aes(group=species),position=pd) +  \n  stat_summary(fun=mean,geom=\"point\",position=pd)\n\n\n\n\n\n\n\n\n \n\n\nUsing Marginal Means from emmeans\nThe mearginal means are again computed with emmeans(), but with year:species so that the marginal means and confidence intervals are estimated for each combination of year and species.\n\naov2mc &lt;- emmeans::emmeans(aov2,specs=pairwise~year:species)\naov2mcs &lt;- summary(aov2mc)\naov2mcs$emmeans\n\n#R|   year species emmean    SE  df lower.CL upper.CL\n#R|   1977 chinook   3.35 0.819 110    1.723     4.97\n#R|   1982 chinook   3.94 0.819 110    2.319     5.57\n#R|   1986 chinook   6.20 0.819 110    4.571     7.82\n#R|   1992 chinook   8.91 1.073 110    6.788    11.04\n#R|   1996 chinook   7.79 0.856 110    6.090     9.48\n#R|   1999 chinook   8.71 0.787 110    7.148    10.27\n#R|   1977 coho      3.06 0.819 110    1.436     4.68\n#R|   1982 coho      3.43 0.819 110    1.808     5.06\n#R|   1986 coho      2.71 0.819 110    1.090     4.34\n#R|   1992 coho      4.60 1.270 110    2.084     7.12\n#R|   1996 coho      2.67 1.004 110    0.681     4.66\n#R|   1999 coho      4.05 1.159 110    1.753     6.35\n#R|  \n#R|  Confidence level used: 0.95\n\n\nThe plot of these marginal means is constructed similarly to that for the one-way ANOVA but using the dodging and color aesthetics described above.\n\npd &lt;- position_dodge(width=0.2)\nggplot(data=aov2mcs$emmeans,mapping=aes(x=year,color=species)) +\n  geom_errorbar(mapping=aes(ymin=lower.CL,ymax=upper.CL),width=0.2,position=pd) +\n  geom_line(mapping=aes(y=emmean,group=species),position=pd) +\n  geom_point(mapping=aes(y=emmean),position=pd)\n\n\n\n\n\n\n\n\n \n\n\n\nSimple Linear Regression\nThe code below fits a simple linear regression for examining the relationship between mirex concentration and salmon weight.\n\nslr &lt;- lm(mirex~weight,data=Mirex)\nanova(slr)\n\n#R|  Analysis of Variance Table\n#R|  \n#R|  Response: mirex\n#R|             Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \n#R|  weight      1 0.22298 0.222980  26.556 1.019e-06 ***\n#R|  Residuals 120 1.00758 0.008396                      \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \nfitPlot() from FSA showed the best-fit line with a 95% confidence band.\n\nFSAmisc::fitPlot(slr,interval=\"confidence\")\n\n\n\n\n\n\n\n\n \n\nUsing Manually Predicted Values I\nOne method for recreating this plot is to create a new data frame that first has the two variables of observed data and then adds on predicted values of the response at each observed value of the explanatory variable with 95% confidence intervals. The two observed variables are selected from the original data frame with select(). If this new data frame is given to newdata= in predict() with interval=\"confidence\" then the predicted values (and 95% confidence intervals) will be constructed at each value of the explanatory variable. The two data frames are then column-bound together with cbind() to make one data frame for plotting (further below).\n\nslrdf &lt;- Mirex |&gt;\n  select(weight,mirex)\nslrdf &lt;- cbind(slrdf,predict(slr,newdata=slrdf,interval=\"confidence\"))\npeek(slrdf,n=6)\n\n#R|      weight mirex       fit        lwr       upr\n#R|  1     0.41  0.16 0.1226593 0.09588108 0.1494376\n#R|  24    7.75  0.34 0.2119229 0.19088398 0.2329618\n#R|  49    0.34  0.02 0.1218080 0.09477073 0.1488453\n#R|  73    1.90  0.10 0.1407796 0.11907549 0.1624837\n#R|  98    9.10  0.29 0.2283406 0.20287925 0.2538019\n#R|  122  11.82  0.09 0.2614192 0.22530410 0.2975342\n\n\nThe confidence band is first plotted as a “ribbon” with the best-fit line then added followed by the observed points. In this plot, weight is globally mapped to x= in ggplot() so that it will be used for each geom. The lower and upper confidence values are mapped to ymin= and ymax= in geom_ribbon(), whereas the predicted or “fit”ted values are mapped to y= geom_line() to make the line and the observed mirex concentrations are mapped to y= in geom_point() to plot the observed points. Further note the use of alpha= to make the confidence band semi-transparent and size= to make the fitted line slightly larger than the default. Again all aspects of this plot can be changed in the usual ggplot way.\n\nggplot(data=slrdf,mapping=aes(x=weight)) +\n  geom_ribbon(mapping=aes(ymin=lwr,ymax=upr),alpha=0.2) +\n  geom_line(mapping=aes(y=fit),size=1) +\n  geom_point(mapping=aes(y=mirex))\n\n\n\n\n\n\n\n\n \n\n\nUsing Manually Predicted Values II\nWith more sparse data sets there may not be enough predicted values to make a smooth plot. In these cases, a separate data frame with more designated values for the explanatory variable is useful. The first line below creates a data frame of weights that consists of 101 evenly spaced values from the minimum to maximum observed weight in the original data frame. Concentrations of mirex at each of these weights are then predicted from the regression line and bound to the data frame.\n\nnd &lt;- data.frame(weight=seq(min(slrdf$weight),max(slrdf$weight),length.out=101))\nnd &lt;- cbind(nd,predict(slr,newdata=nd,interval=\"confidence\"))\npeek(slrdf,n=6)\n\n#R|      weight mirex       fit        lwr       upr\n#R|  1     0.41  0.16 0.1226593 0.09588108 0.1494376\n#R|  24    7.75  0.34 0.2119229 0.19088398 0.2329618\n#R|  49    0.34  0.02 0.1218080 0.09477073 0.1488453\n#R|  73    1.90  0.10 0.1407796 0.11907549 0.1624837\n#R|  98    9.10  0.29 0.2283406 0.20287925 0.2538019\n#R|  122  11.82  0.09 0.2614192 0.22530410 0.2975342\n\n\nThe plot is now constructed from two data frames – slrdf with the original observed data and nd with predicted concentrations of mirex at specifically chosen weights. Given this, the data to use must be specifically declared within each geom, with geom_ribbon() and geom_line() using the predicted data frame (i.e., nd) and geom_point() using the observed data (i.e., slrdf).\n\nggplot() +\n  geom_ribbon(data=nd,mapping=aes(x=weight,ymin=lwr,ymax=upr),alpha=0.2) +\n  geom_line(data=nd,mapping=aes(x=weight,y=fit),size=1) +\n  geom_point(data=slrdf,mapping=aes(x=weight,y=mirex))\n\n\n\n\n\n\n\n\n \n\n\nUsing A Built-In Function\nThe best-fit line can also be added to a scatterplot with geom_smooth(), where method=\"lm\" makes sure that a linear model is used for the “smoother.”\n\nggplot(data=Mirex,mapping=aes(x=weight,y=mirex)) +\n  geom_smooth(method=\"lm\",alpha=0.2) +\n  geom_point()\n\n\n\n\n\n\n\n\n \n\n\n\nIndicator Variable Regression\nThe code below fits an indicator variable regression to examine if the relationship between mirex concentration and salmon weight differs betwen species.\n\nivr &lt;- lm(mirex~weight*species,data=Mirex)\nanova(ivr)\n\n#R|  Analysis of Variance Table\n#R|  \n#R|  Response: mirex\n#R|                  Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \n#R|  weight           1 0.22298 0.222980 26.8586 9.155e-07 ***\n#R|  species          1 0.00050 0.000498  0.0600   0.80690    \n#R|  weight:species   1 0.02744 0.027444  3.3057   0.07158 .  \n#R|  Residuals      118 0.97964 0.008302                      \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n \nfitPlot() from FSA showed the best-fit line for both species.\n\nFSAmisc::fitPlot(ivr,interval=\"confidence\")\n\n\n\n\n\n\n\n\n \n\nUsing Manually Predicted Values\nThe process of constructing a similar plot in ggplot() follows the same general procedure as that for a simple linear regression. First, make a data frame that has the observed variables used in the model and predicted values and confidence limits for each observation.8\n8 Again, one may want to make a data frame with more values of the explanatory variable if the observed data is sparse.\nivrdf &lt;- select(Mirex,weight,mirex,species)\nivrdf &lt;- cbind(ivrdf,predict(ivr,newdata=ivrdf,interval=\"confidence\"))\npeek(ivrdf,n=6)\n\n#R|      weight mirex species        fit        lwr       upr\n#R|  1     0.41  0.16 chinook 0.13939054 0.09905499 0.1797261\n#R|  24    7.75  0.34 chinook 0.20990801 0.18638517 0.2334308\n#R|  49    0.34  0.02    coho 0.09192064 0.04956596 0.1342753\n#R|  73    1.90  0.10    coho 0.12580003 0.09660968 0.1549904\n#R|  98    9.10  0.29 chinook 0.22287784 0.19567885 0.2500768\n#R|  122  11.82  0.09 chinook 0.24900965 0.21056798 0.2874513\n\n\n \nThen plot the data as before but making sure to map color= and fill= (just for the ribbon) to the species factor variable.\n\nggplot(data=ivrdf,mapping=aes(x=weight,color=species)) +\n  geom_ribbon(mapping=aes(ymin=lwr,ymax=upr,fill=species),alpha=0.2) +\n  geom_line(mapping=aes(y=fit),size=1) +\n  geom_point(mapping=aes(y=mirex))\n\n\n\n\n\n\n\n\n \n\n\nUsing a Built-In Function\nThis plot can also be constructed with geom_smooth(), again making sure to map the color= and fill= to the species factor variable.\n\nggplot(data=Mirex,mapping=aes(x=weight,y=mirex,color=species,fill=species)) +\n  geom_smooth(method=\"lm\",alpha=0.2) +\n  geom_point()\n\n\n\n\n\n\n\n\n \n\n\n\nLogistic Regression\nThe code below fits a logistic regression to examine the relationship between the probability that mirex concentration is greater than 0.2 and salmon weight.\n\nlogreg &lt;- glm(gt2~weight,data=Mirex,family=\"binomial\")\nsummary(logreg)\n\n#R|  \n#R|  Call:\n#R|  glm(formula = gt2 ~ weight, family = \"binomial\", data = Mirex)\n#R|  \n#R|  Deviance Residuals: \n#R|      Min       1Q   Median       3Q      Max  \n#R|  -1.7696  -0.7462  -0.5566   0.9537   1.9789  \n#R|  \n#R|  Coefficients:\n#R|              Estimate Std. Error z value Pr(&gt;|z|)    \n#R|  (Intercept) -2.19359    0.41871  -5.239 1.61e-07 ***\n#R|  weight       0.29822    0.06496   4.591 4.41e-06 ***\n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#R|  \n#R|  (Dispersion parameter for binomial family taken to be 1)\n#R|  \n#R|      Null deviance: 158.35  on 121  degrees of freedom\n#R|  Residual deviance: 132.32  on 120  degrees of freedom\n#R|  AIC: 136.32\n#R|  \n#R|  Number of Fisher Scoring iterations: 3\n\n\n \nfitPlot() from FSA showed the fitted logistic regression curve with the observed values transparently at the top and bottom of the y-axis and symbols at the proportion of “successes” for “windows” of the x-axis.\n\nFSAmisc::fitPlot(logreg)\n\n\n\n\n\n\n\n\n \n\nUsing Manually Predicted Values\nThe first method for showing the logistic regression curve follows the general methodology for simple linear regression shown above. Note, however, that predict() does not produce confidence interval values for a logistic regression. Thus, the plot created in this way cannot have a confidence band.\n\nlogregdf &lt;- select(Mirex,gt2,weight)\nlogregdf$fit &lt;- predict(logreg,newdata=logregdf,\n                        type=\"response\",interval=\"confidence\")\npeek(logregdf,n=6)\n\n#R|      gt2 weight       fit\n#R|  1     0   0.41 0.1119161\n#R|  24    1   7.75 0.5293765\n#R|  49    0   0.34 0.1098580\n#R|  73    0   1.90 0.1642466\n#R|  98    1   9.10 0.6272046\n#R|  122   0  11.82 0.7910738\n\n\n\nggplot(data=logregdf,mapping=aes(x=weight)) +\n  geom_point(mapping=aes(y=gt2),alpha=0.25) +\n  geom_line(mapping=aes(y=fit),size=1)\n\n\n\n\n\n\n\n\n \n\n\nUsing a Built-In Function\nThe best-fit logistic regression curve with a confidence band can, however, be added to a scatterplot with geom_smooth(). In this case, method= must be changed to glm and method.args= must be used as shown below so that glm will construct a logistic (rather than linear) regression.\n\nggplot(data=Mirex,mapping=aes(x=weight,y=gt2)) +\n  geom_smooth(method=\"glm\",alpha=0.2,method.args=list(family=\"binomial\")) +\n  geom_point(alpha=0.25)\n\n\n\n\n\n\n\n\n \nNote that this method easily generalizes to an indicator variable logistic regression (note that color= and fill= are mapped to the species factor variable).\n\nlogreg2 &lt;- glm(gt2~weight*species,data=Mirex,family=\"binomial\")\n\nggplot(data=Mirex,mapping=aes(x=weight,y=gt2,color=species,fill=species)) +\n  geom_smooth(method=\"glm\",alpha=0.2,\n              method.args=list(family=\"binomial\")) +\n  geom_point(alpha=0.25)\n\n\n\n\n\n\n\n\n \n\n\n\nPolynomial Regression\nThe code below fits a quadratic (second degree polynomial) regression for the relationship between mirex concentration and salmon weight.\n\npoly2 &lt;- lm(mirex~weight+I(weight^2),data=Mirex)\nsummary(poly2)\n\n#R|  \n#R|  Call:\n#R|  lm(formula = mirex ~ weight + I(weight^2), data = Mirex)\n#R|  \n#R|  Residuals:\n#R|        Min        1Q    Median        3Q       Max \n#R|  -0.208068 -0.048257  0.000994  0.060883  0.244424 \n#R|  \n#R|  Coefficients:\n#R|                Estimate Std. Error t value Pr(&gt;|t|)    \n#R|  (Intercept)  0.0875361  0.0209754   4.173 5.74e-05 ***\n#R|  weight       0.0283282  0.0086331   3.281  0.00136 ** \n#R|  I(weight^2) -0.0013524  0.0006953  -1.945  0.05413 .  \n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#R|  \n#R|  Residual standard error: 0.09059 on 119 degrees of freedom\n#R|  Multiple R-squared:  0.2064,   Adjusted R-squared:  0.1931 \n#R|  F-statistic: 15.48 on 2 and 119 DF,  p-value: 1.06e-06\n\n\n \nfitPlot() from FSA showed the best-fit regression curve.\n\nFSAmisc::fitPlot(poly2,interval=\"confidence\")\n\n\n\n\n\n\n\n\n \n\nUsing Manually Predicted Values\nThis regression can be viewed similarly to the way the simple linear regressions was viewed.\n\npolydf &lt;- select(Mirex,weight,mirex)\npolydf &lt;- cbind(polydf,predict(poly2,newdata=polydf,interval=\"confidence\"))\nggplot(polydf,mapping=aes(x=weight)) +\n  geom_ribbon(mapping=aes(ymin=lwr,ymax=upr),alpha=0.2) +\n  geom_line(mapping=aes(y=fit),size=1) +\n  geom_point(mapping=aes(y=mirex))\n\n\n\n\n\n\n\n\n \n\n\nUsing a Built-In Function\nThis type of regression can also be viewed using geom_smooth() but the formula for the polynomial must be given to formula=. However, note that in this formula you put y and x rather than the names of the variables that are mapped to y and x.\n\nggplot(data=Mirex,mapping=aes(x=weight,y=mirex)) +\n  geom_smooth(method=\"lm\",formula=\"y~x+I(x^2)\",alpha=0.2) +\n  geom_point()\n\n\n\n\n\n\n\n\n \n\n\n\nNonlinear Regression\nThe concepts about producing a fitted line plot for a non-linear regression in ggplot is described in detail, with respect to a von Bertalanffy growth function, in this post and this post.\n \n\n\nConclusion\nfitPlot() in FSA was removed in early 2022. This post describes a more transparent (i.e., not a “black box”) and flexible set of methods for constructing similar plots using ggplot2 for those who will need to transition away from using fitPlot().\nAs mentioned in the examples above, each plot can be modified further using typical methods for ggplot2. These changes were not illustrated above to minimize the amount of code shown in this post. However, as an example, the code below shows a possible modification of the IVR plot shown above.\n\nggplot(data=Mirex,mapping=aes(x=weight,y=mirex,color=species,fill=species)) +\n  geom_smooth(method=\"lm\",alpha=0.1,size=1.25) +\n  geom_point(size=1.5) +\n  scale_y_continuous(name=\"Mirex Concentration in Tissue\",limits=c(0,0.5),\n                     expand=expansion(mult=0)) +\n  scale_x_continuous(name=\"Salmon Weight (kg)\",limits=c(0,15),\n                     expand=expansion(mult=0)) +\n  scale_color_manual(values=c(\"#E69F00\",\"#0072B2\"),\n                     aesthetics=c(\"color\",\"fill\")) +\n  theme(panel.grid.major=element_line(color=\"gray90\",linetype=\"dashed\"),\n        panel.grid.minor=element_blank(),\n        axis.title=element_text(size=rel(1.25)),\n        axis.text=element_text(size=rel(1.1)),\n        legend.position=c(0,1),\n        legend.justification=c(-0.05,1.02),\n        legend.title=element_blank(),\n        legend.text=element_text(size=rel(1.1)))\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2021,\n  author = {H. Ogle, Derek},\n  title = {Replace {fitPlot()} with Ggplot},\n  date = {2021-05-25},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2021-5-25_fitPlot-replacement},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2021, May 25. Replace fitPlot() with ggplot. https://fishr-core-team.github.io/fishR//blog/posts/2021-5-25_fitPlot-replacement."
  },
  {
    "objectID": "blog/posts/2021-5-11_compSlopes-replacement/index.html",
    "href": "blog/posts/2021-5-11_compSlopes-replacement/index.html",
    "title": "Replace compSlopes() with emtrends()",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below.\nlibrary(dplyr)    ## for filter(), mutate()\nlibrary(emmeans)  ## for emtrends()"
  },
  {
    "objectID": "blog/posts/2021-5-11_compSlopes-replacement/index.html#conclusion",
    "href": "blog/posts/2021-5-11_compSlopes-replacement/index.html#conclusion",
    "title": "Replace compSlopes() with emtrends()",
    "section": "Conclusion",
    "text": "Conclusion\nemtrends() in emmeans provides a more general solution to comparing multiple slopes than what was used in compSlopes() in FSA prior to v0.9.0. As compSlopes() was removed from FSA in 2022, you should now use emtrends() for this purpose.\nemmeans has extensive vignettes that further explain its use. Please see this discussion for the use case described in this post. Their “Basics” vignette is also useful.\nIn the next post I will demonstrate how to use emmeans() from the emmeans package to replace compIntercepts(), which was also removed from FSA.\n\n\n\n\n\n\nNote\n\n\n\nThis change to FSA does not affect anything in Ogle (2016)."
  },
  {
    "objectID": "blog/posts/2020-1-2_vonB_plots_2/index.html",
    "href": "blog/posts/2020-1-2_vonB_plots_2/index.html",
    "title": "von Bertalanffy Growth Plots II",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. One function from investr is also used but the whole package is not loaded here. The data are also from FSAdata, which is not loaded below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)     # for vbFuns(), vbStarts(), headtail(), peek()\nlibrary(dplyr)   # for filter(), mutate(), group_by()\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\n \n\nIntroduction\nIn a previous post I demonstrated how to plot the fit of a von Bertalanffy growth function (VBGF) for a single group of observed data. In this post, I explore methods to do the same for multiple groups of observed data (e.g., multiple sexes, locations, years).\nI will again use the lengths and ages of Lake Erie Walleye (Sander vitreus) captured during October-November, 2003-2014 available in FSAdata. These data formed many of the examples in Ogle et al. (2017). My primary interest here is in the tl (total length in mm), age, and sex variables.1 I initially focus on Walleye from location “1” captured in 2014 in this example.2\n1 See more details about the data.2 For succinctness, I removed year as it only had one level after filtering and three variables related to the location of capture.\ndata(WalleyeErie2,package=\"FSAdata\")\nw14T &lt;- WalleyeErie2 |&gt;\n  filter(year==2014,loc==1) |&gt;\n  select(-year,-setID,-loc,-grid)\nheadtail(w14T)\n\n#R|       tl    w    sex      mat age\n#R|  1   445  737 female immature   2\n#R|  2   528 1571 female   mature   4\n#R|  3   499 1138   male   mature   4\n#R|  612 521 1408 female   mature   3\n#R|  613 565 1745 female   mature   3\n#R|  614 530 1553   male   mature   5\n\n\nAs in the previous post, a function that uses the typical VBGF to predict mean length given a set of VBGF parameters and age is needed.3\n3 Other parameterizations of the VBGF can be used with param= in vbFuns() as described in its documentation.\n( vb &lt;- vbFuns(param=\"Typical\") )\n\n#R|  function(t,Linf,K=NULL,t0=NULL) {\n#R|    if (length(Linf)==3) { K &lt;- Linf[[2]]\n#R|                           t0 &lt;- Linf[[3]]\n#R|                           Linf &lt;- Linf[[1]] }\n#R|    Linf*(1-exp(-K*(t-t0)))\n#R|    }\n#R|  &lt;bytecode: 0x000001ca99a60fb0&gt;\n#R|  &lt;environment: 0x000001ca99ba5060&gt;\n\n\n \n\n\nModel Fits Using geom_smooth()\nOne of the simpler ways to plot multiple VBGF fits uses geom_smooth(), which, as described in the previous post, uses nls() to fit the model “behind-the-scenes.” Thus, a set of starting values is needed. It is critical to note that using geom_smooth() requires the same starting values for each group. With this warning, possible starting values for the optimization algorithm may be obtained with vbStarts() as shown in the previous post.\n\n( sv0 &lt;- vbStarts(tl~age,data=w14T) )\n\n#R|  $Linf\n#R|  [1] 591.1587\n#R|  \n#R|  $K\n#R|  [1] 0.3930388\n#R|  \n#R|  $t0\n#R|  [1] -1.544479\n\n\nThe use of geom_smooth() to produce separate lines for the groups is exactly as described in the previous post, except that the variable for identifyng the groups (i.e., sex in this example) must be mapped to the color= aes()thetic. For Figure 1 I also used scale_color_manual() to demonstrate how to change the default colors of the points and lines. Additionally, in theme() I moved the legend into the plot panel4 and removed the default sex label that would appear on top of the legend.\n4 In this case the legend is placed 80% down the x-axis and 20% up the y-axis.\nggplot(data=w14T,aes(x=age,y=tl,color=sex)) +\n  geom_point(size=2,alpha=0.3) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  scale_color_manual(values=c(\"male\"=\"darkblue\",\"female\"=\"darkred\")) +\n  geom_smooth(method=\"nls\",se=FALSE,\n              method.args=list(formula=y~vb(x,Linf,K,t0),start=sv0),\n              linewidth=1) +\n  theme(panel.grid.minor.x=element_blank(),\n        legend.position=c(0.8,0.2),\n        legend.title=element_blank())\n\n\n\n\n\n\n\nFigure 1: Fit of typical von Bertalanffy growth function to male and female Lake Erie Walleye in 2014.\n\n\n\n\n\n \nAn alternative visualization is to put the two groups into separate panels using facet_wrap().5 Note that color is superfluous in Figure 2, but I kept it for comparison to Figure 1. Mapping color to a variable will produce a legend by default. This legend was removed (in theme()), however, because it is redundant with the panel labels.\n5 Simply put the grouping variable inside of vars() as the argument to facet_wrap().\nggplot(data=w14T,aes(x=age,y=tl,color=sex)) +\n  geom_point(size=2,alpha=0.3) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  scale_color_manual(values=c(\"male\"=\"darkblue\",\"female\"=\"darkred\")) +\n  geom_smooth(method=\"nls\",se=FALSE,\n              method.args=list(formula=y~vb(x,Linf,K,t0),start=sv0),\n              linewidth=1) +\n  theme(panel.grid.minor.x=element_blank(),\n        legend.position=\"none\") +\n  facet_wrap(vars(sex))\n\n\n\n\n\n\n\nFigure 2: Fit of typical von Bertalanffy growth function to male and female Lake Erie Walleye in 2014.\n\n\n\n\n\n \n\n\nFitting the VBGF to Multiple Groups\nMethods for fitting a von Bertalanfy growth function (VBGF) to multiple groups are detailed in Ogle (2016) and Ogle et al. (2017). Thus, this methodology will only be briefly explained here.\nFitting the VBGF to multiple groups requires creating an expression, rather than a function, with the VBGF defined over the multiple groups. The expression below codes the VBGF using the length and age variables from the data set used here (i.e., tl and age) and assuming different parameters should be estimated for each group.6 Identifying the different groups is accomplished here by appending [group], where group is replaced with the specific variable name that identifies the groups (i.e., sex here). The expression is assigned to an object name (i.e., vbLKt here).\n6 See Ogle (2016) and Ogle et al. (2017) for examples of how to fit the VBGF assuming situations where some parameters differ and some do not across groups.\nvbLKt &lt;- tl~Linf[sex]*(1-exp(-K[sex]*(age-t0[sex])))\n\n \nI created vectors of group names (with unique()) and the number of groups (with length()) for ease of use below.\n\n( grps &lt;- unique(w14T$sex) )\n\n#R|  [1] female male  \n#R|  Levels: female male\n\n( ngrps &lt;- length(grps) )\n\n#R|  [1] 2\n\n\n \nThis model has six parameters – \\(L_{\\infty}\\), \\(K\\), and \\(t_{0}\\) for each of the two groups (male and female Walleye). Starting values must be declared for each of these parameters. Here I find starting values from the data ignoring the groups (using vbStarts() as described in the previous post) and replicate those starting values for both groups.7\n7 In some instances using the same starting values for both groups will not result in model convergence. See Ogle (2016) and Ogle et al. (2017) for suggestions for handling those cases.\n( sv0 &lt;- vbStarts(tl~age,data=w14T) )\n\n#R|  $Linf\n#R|  [1] 591.1587\n#R|  \n#R|  $K\n#R|  [1] 0.3930388\n#R|  \n#R|  $t0\n#R|  [1] -1.544479\n\n\nMap replicates the starting values in the list from vbStarts() with rep (the replicated function), the object returned from vbStarts(), and a numeric vector that explains how many times each parameter should be repeated (i.e., the same as the number of groups).\n\n( svLKt &lt;- Map(rep,sv0,c(2,2,2)) )\n\n#R|  $Linf\n#R|  [1] 591.1587 591.1587\n#R|  \n#R|  $K\n#R|  [1] 0.3930388 0.3930388\n#R|  \n#R|  $t0\n#R|  [1] -1.544479 -1.544479\n\n\nThe model is fit to the data by including the VBGF expression object, data=, and start= as arguments to nls(). The parameter estimates and confidence intervals may be extracted from the saved nls() object with coef() and confint().8 Note that the parameters will be appended with numbers in the alphabetical order of the groups.9 Thus, in this example, it is seen that the estimated \\(L_{\\infty}\\) for females (appended with a “1”) is greater than that for males.\n8 Column-bound here for aesthetic reasons.9 Unless your chose to order the levels of the group variable differently.\nfitLKt &lt;- nls(vbLKt,data=w14T,start=svLKt)\ncbind(Ests=coef(fitLKt),confint(fitLKt))\n\n#R|               Ests        2.5%       97.5%\n#R|  Linf1 648.2083813 627.2685519 672.1242978\n#R|  Linf2 574.1512545 561.6722232 588.0005006\n#R|  K1      0.3615399   0.3171679   0.4106412\n#R|  K2      0.3354067   0.2946359   0.3805994\n#R|  t01    -1.2836317  -1.4846294  -1.0991040\n#R|  t02    -1.9702829  -2.2978880  -1.6783578\n\n\n \n\n\nModel Fits from Predicted Values\nThe model fit above can be used to predict the mean length at age for all groups, both within and outside the observed range of ages. These results can then be used to show the model fit by group. However, a bit of work outside of ggplot is required to prepare the relevant data.\nA data frame that contains predicted mean lengths at age over a range of ages for all groups is needed. I begin this process by finding (and assigning to an object for later use) the range of ages for each group.10\n10 as.data.frame() removes the tibble returned by group_by(), which I don’t prefer\nobsagerng &lt;- w14T |&gt;\n  group_by(sex) |&gt;\n  summarize(min=min(age),\n            max=max(age)) |&gt;\n  as.data.frame()\nobsagerng\n\n#R|       sex min max\n#R|  1 female   0  11\n#R|  2   male   1  11\n\n\nFrom this, I create a temporary vector of 101 ages11 evenly spaced over a range of ages larger than what was observed.12\n11 Use a larger value for length.out= to make the line produced further below more smooth.12 Zero was an observed age here, if not I usually make sure it is included.\nages &lt;- seq(-1,12,length.out=101)\n\nI then create a data frame that has the name of each group repeated as many times as there are ages in the temporary age vector (i.e., 101 here) and has ages from the temporary age vector repeated for each group (i.e., twice here). Note the careful use of each= and times= in rep() below.\n\npreds &lt;- data.frame(sex=rep(grps,each=length(ages)),\n                    age=rep(ages,times=ngrps))\nheadtail(preds)\n\n#R|         sex   age\n#R|  1   female -1.00\n#R|  2   female -0.87\n#R|  3   female -0.74\n#R|  200   male 11.74\n#R|  201   male 11.87\n#R|  202   male 12.00\n\n\nPredicted mean lengths at each age for each group are then appended to this data frame by giving the fitted nls() object and this data frame to predict().\n\npreds &lt;- preds |&gt;\n  mutate(fit=predict(fitLKt,preds))\nheadtail(preds)\n\n#R|         sex   age       fit\n#R|  1   female -1.00  63.17550\n#R|  2   female -0.87  90.03599\n#R|  3   female -0.74 115.66324\n#R|  200   male 11.74 568.37143\n#R|  201   male 11.87 568.61804\n#R|  202   male 12.00 568.85411\n\n\nFinally, I add a fourth variable that will be TRUE if the age is within the range of observed ages for the group. group_by() below behaves like a loop here in the sense that what follows that line will be completed for the first group, then the second group, and so on. For the mutate() lines suppose that we are working with the first group (i.e., “female”). In this case, unique(sex) will return female such that obsagerng[obsagerng$sex==unique(sex),] will find the row of obsagerng where its sex variable is equal to “female”. The $min appended to that will extract just the value in the min variable of that row (so the minimum observed age for females). The age&gt;=obsagerng[obsagerng$sex==unique(sex),]$min portion is thus asking if the age variable in preds is greater than or equal to the minimum observed age for females. If it is it will return TRUE, otherwise FALSE will be returned. The second part mutate() asks if the age variable in preds is less than or equal to the maximum observed age for females, again returning TRUE or FALSE as appropriate. Those two conditions are connected with an & such that if they are both TRUE then a TRUE is returned, otherwise a FALSE is returned. The result is that the new inrng variable will be TRUE if the age variable within preds is within the observed range of ages for each sex in preds.13\n13 FSA::peek() is used here to show an evenly spaced 20 rows in the data frame. This provides more output than headtail() for observing the results of code.\npreds &lt;- preds |&gt;\n  group_by(sex) |&gt;\n  mutate(inrng=age&gt;=obsagerng[obsagerng$sex==unique(sex),]$min &\n               age&lt;=obsagerng[obsagerng$sex==unique(sex),]$max)\nFSA::peek(preds)\n\n#R|         sex   age      fit inrng\n#R|  1   female -1.00  63.1755 FALSE\n#R|  11  female  0.30 282.5622  TRUE\n#R|  21  female  1.60 419.6791  TRUE\n#R|  32  female  3.03 511.9351  TRUE\n#R|  43  female  4.46 566.9479  TRUE\n#R|  53  female  5.76 597.4205  TRUE\n#R|  64  female  7.19 617.9233  TRUE\n#R|  74  female  8.49 629.2802  TRUE\n#R|  85  female  9.92 636.9214  TRUE\n#R|  96  female 11.35 641.4779 FALSE\n#R|  106   male -0.48 225.8590 FALSE\n#R|  117   male  0.95 358.5543 FALSE\n#R|  128   male  2.38 440.6943  TRUE\n#R|  138   male  3.68 487.8581  TRUE\n#R|  149   male  5.11 520.7348  TRUE\n#R|  159   male  6.41 539.6122  TRUE\n#R|  170   male  7.84 552.7712  TRUE\n#R|  181   male  9.27 560.9168  TRUE\n#R|  191   male 10.57 565.5938  TRUE\n#R|  202   male 12.00 568.8541 FALSE\n\n\n \nFigure 3 is created by combining similar code from the previous post and the use of color= from above. In brief, geom_point() adds points at the observed lengths at age, the first geom_line() adds the fitted models from the predicted mean lengths at all ages in preds as dashed lines, and the second geom_line() adds the fitted models from the predicted mean lengths only at observed ages for each group as solid lines.14\n14 Note the use filter() in the second geom_line() so that only data for which inrng==TRUE is used.\nggplot() +\n  geom_point(data=w14T,aes(x=age,y=tl,color=sex),\n             size=2,alpha=0.3) +\n  geom_line(data=preds,aes(x=age,y=fit,color=sex),\n              linewidth=1,linetype=\"dashed\") +\n  geom_line(data=filter(preds,inrng),aes(x=age,y=fit,color=sex),\n              linewidth=1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  scale_color_manual(values=c(\"male\"=\"darkblue\",\"female\"=\"darkred\")) +\n  theme(panel.grid.minor.x=element_blank(),\n        legend.position=c(0.8,0.2),\n        legend.title=element_blank())\n\n\n\n\n\n\n\nFigure 3: Fit of typical von Bertalanffy growth function to male and female Lake Erie Walleye in 2014. The dashed lines show the model fits outside the range of observed ages for each sex.\n\n\n\n\n\n \n\n\nModel Fit with Confidence Band\nCreating a graphic that has the model fit with confidence (or prediction) bands for multiple groups again requires some preparatory work before using ggplot().\nFor simplicity and completeness of presentation, the code below is repeated from above.\n\nvb &lt;- vbFuns(param=\"Typical\")     ## Typical VBGF\ngrps &lt;- unique(w14T$sex)          ## Names of groups\nngrps &lt;- length(grps)             ## Number of groups\nobsagerng &lt;- w14T |&gt;              ## Range of observed ages by group\n  group_by(sex) |&gt;\n  summarize(min=min(age),\n            max=max(age))\nsv0 &lt;- vbStarts(tl~age,data=w14T) ## Starting values ignoring groups\n\nAs before, starting values are needed for each group. Here, however, I place the starting values into a data frame where each row corresponds to one group and a variable that identifies the groups is included.15\n15 For simplicity name this variable the same as it is in the main data frame.\n( svLKt &lt;- data.frame(sex=grps,\n                      Map(rep,sv0,c(ngrps,ngrps,ngrps))) )\n\n#R|       sex     Linf         K        t0\n#R|  1 female 591.1587 0.3930388 -1.544479\n#R|  2   male 591.1587 0.3930388 -1.544479\n\n\nThe starting values created here are the same for each group. This will likely work in most cases (of fish growth data) but likely not all. Starting values can be entered into the data frame manually as shown below.16\n16 Ogle (2016) and Ogle et al. (2017) discuss alternative methods of finding starting values. But note that this may take some work if there are many groups.\n## Not run: Demo of manually entering starting values; not needed in this case\n( svLKt &lt;- data.frame(sex=grps,\n                      Linf=c(630,580),\n                      K=c(0.34,0.34),\n                      t0=c(-1,-2)) )\n\nThe method used here basically loops through the groups, uses the procedures of the previous post (for one group) and above to find fitted values and confidence (or prediction) bands for each group, and then (row) binds those results together to produce a synthetic data frame for all groups. The process for a single group is wrapped into a function called vbLOOP1() below. The ordered arguments to vbLOOP1() are a group name, the original data frame, the data frame of starting values for all groups, the data frame of observed age ranges, and a vector that identifies the overall range over which to predict mean lengths at age. vbLOOP1 will default to returning confidence intervals, but prediction intervals can be returned by including interval=\"prediction\".17\n17 See code comments for the parts of this function that are specific to this case and, thus, might have to be changed for other data.\nvbLOOP1 &lt;- function(grp,dat,svs,oagerng,eagerng,interval=\"confidence\") {\n  ## !! This requires a 'sex', 'tl', and 'age' variable in data frames !!\n  ## !!   Otherwise need to change 'sex', 'tl', and 'age' below\n  ## !!   Everything else can stay as is\n  \n  ## Loop notification (for peace of mind)\n  cat(grp,\"Loop\\n\")\n  ## Isolate group's data, starting values, and age range\n  dat1 &lt;- dplyr::filter(dat,sex==grp)\n  sv1 &lt;- svs |&gt;\n    filter(sex==grp) |&gt;\n    select(-sex) |&gt;\n    as.list()\n  oagerng1 &lt;- filter(oagerng,sex==grp)\n  ## Make ages for predictions\n  ages &lt;- seq(min(eagerng),max(eagerng),length.out=101)\n  ## Fit von B to that group\n  fit1 &lt;- nls(tl~vb(age,Linf,K,t0),data=dat1,start=sv1)\n  ## Make data frame of predicted mean lengths at age with CIs\n  preds1 &lt;- data.frame(sex=grp,\n                       age=ages,\n                       fit=investr::predFit(fit1,data.frame(age=ages),\n                                            interval=interval)) |&gt;\n    mutate(inrng=age&gt;=oagerng1$min & age&lt;=oagerng1$max) |&gt;\n    as.data.frame()\n  ## Rename variables\n  names(preds1) &lt;- c(\"sex\",\"age\",\"fit\",\"lwr\",\"upr\",\"inrng\")\n  ## Return data frame\n  preds1\n}\n\n \nThe code below initiates an object called preds that will hold the resulting data frame and then loops through the values in grps, running vbLOOP1() for each group with the w14T data frame of observed lengths and ages, svLKt data frame of starting values, obsagerng data frame of observed age ranges, and ages for making predictions that are between -1 and 12. The results for each group are row bound (i.e., using rbind()) together to produce the stacked results partially shown with peek().\n\npreds &lt;-  NULL\nfor (i in grps) preds &lt;- rbind(preds,vbLOOP1(i,w14T,svLKt,obsagerng,c(-1,12)))\n\n#R|  female Loop\n#R|  male Loop\n\npeek(preds)\n\n#R|         sex   age       fit       lwr       upr inrng\n#R|  1   female -1.00  63.17589  32.40811  93.94366 FALSE\n#R|  11  female  0.30 282.56223 275.45559 289.66887  TRUE\n#R|  21  female  1.60 419.67905 417.10638 422.25173  TRUE\n#R|  32  female  3.03 511.93506 508.44481 515.42531  TRUE\n#R|  43  female  4.46 566.94788 562.17160 571.72416  TRUE\n#R|  53  female  5.76 597.42054 589.97092 604.87016  TRUE\n#R|  64  female  7.19 617.92339 607.37308 628.47370  TRUE\n#R|  74  female  8.49 629.28032 616.34116 642.21948  TRUE\n#R|  85  female  9.92 636.92157 621.94832 651.89482  TRUE\n#R|  96  female 11.35 641.47810 625.02853 657.92767 FALSE\n#R|  106   male -0.48 225.85896 206.22610 245.49182 FALSE\n#R|  117   male  0.95 358.55434 354.06691 363.04177 FALSE\n#R|  128   male  2.38 440.69430 436.91620 444.47240  TRUE\n#R|  138   male  3.68 487.85809 483.57573 492.14046  TRUE\n#R|  149   male  5.11 520.73480 516.13704 525.33256  TRUE\n#R|  159   male  6.41 539.61222 533.94616 545.27827  TRUE\n#R|  170   male  7.84 552.77120 545.45179 560.09060  TRUE\n#R|  181   male  9.27 560.91675 552.00041 569.83310  TRUE\n#R|  191   male 10.57 565.59384 555.46575 575.72192  TRUE\n#R|  202   male 12.00 568.85412 557.68844 580.01980 FALSE\n\n\n \nFigure 4 is then constructed almost identically to Figure 3 except that geom_ribbon() is used to produce the confidence bands, with the lower part of the ribbon at the lower confidence values (i.e., ymin=lwr) and the upper part at the upper confidence value (i.e., ymax=upr) produced from the loop above.18 fill= gives the color of the enclosed ribbon. Further note that aesthetics= must be defined in scale_color_manual() because geom_ribbon() used fill=, whereas the other geoms used color=.\n18 Add geom_ribbon() first so that it is behind the points and model lines.\nggplot() +\n  geom_ribbon(data=preds,aes(x=age,ymin=lwr,ymax=upr,fill=sex),alpha=0.25) +\n  geom_point(data=w14T,aes(x=age,y=tl,color=sex),\n             size=2,alpha=0.3) +\n  geom_line(data=preds,aes(x=age,y=fit,color=sex),\n              linewidth=1,linetype=\"dashed\") +\n  geom_line(data=filter(preds,inrng),aes(x=age,y=fit,color=sex),\n              linewidth=1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  scale_color_manual(values=c(\"male\"=\"darkblue\",\"female\"=\"darkred\"),\n                     aesthetics=c(\"color\",\"fill\")) +\n  theme(panel.grid.minor.x=element_blank(),\n        legend.position=c(0.8,0.2),\n        legend.title=element_blank())\n\n\n\n\n\n\n\nFigure 4: Fit of typical von Bertalanffy growth function to male and female Lake Erie Walleye in 2014. The dashed lines show the model fits outside the range of observed ages for each sex.\n\n\n\n\n\n\n\n\n \nOf course, the graph can also be separated by the grouping variable (Figure 5).\n\nggplot() +\n  geom_ribbon(data=preds,aes(x=age,ymin=lwr,ymax=upr,fill=sex),alpha=0.25) +\n  geom_point(data=w14T,aes(x=age,y=tl,color=sex),\n             size=2,alpha=0.3) +\n  geom_line(data=preds,aes(x=age,y=fit,color=sex),\n              linewidth=1,linetype=\"dashed\") +\n  geom_line(data=filter(preds,inrng),aes(x=age,y=fit,color=sex),\n              linewidth=1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  scale_color_manual(values=c(\"male\"=\"darkblue\",\"female\"=\"darkred\"),\n                     aesthetics=c(\"color\",\"fill\")) +\n  theme(panel.grid.minor.x=element_blank(),\n        legend.position=\"none\") +\n  facet_wrap(vars(sex))\n\n\n\n\n\n\n\nFigure 5: Fit of typical von Bertalanffy growth function to male and female Lake Erie Walleye in 2014. The dashed lines show the model fits outside the range of observed ages for each sex.\n\n\n\n\n\n \n\n\nMore than Two Groups\nThe process defined above can be extended to more than two groups. Here I examine just female walleye captured in four years, as an example. Note here that year must be explicitly turned to a factor so that it will be treated as a group identifier rather than a numeric value.\n\nwfT &lt;- WalleyeErie2 |&gt;\n  filter(sex==\"female\",loc==1,year %in% c(2005,2008,2011,2014)) |&gt;\n  mutate(year=factor(year)) |&gt;\n  select(-loc,-setID,-grid,-sex)\n\nThe code below is repeated from above, but everywhere where sex was in the code has been changed to year (our new grouping variable). Once again note that using the same starting values for all groups may not always work (but it did here).\n\ngrps &lt;- unique(wfT$year)          ## Names of groups\nngrps &lt;- length(grps)             ## Number of groups\nobsagerng &lt;- wfT |&gt;               ## Range of observed ages by group\n  group_by(year) |&gt;\n  summarize(min=min(age),\n            max=max(age))\n\nsv0 &lt;- vbStarts(tl~age,data=wfT)  ## Starting values ignoring groups\n( svLKt &lt;- data.frame(year=grps,\n                      Map(rep,sv0,c(ngrps,ngrps,ngrps))) )\n\n#R|    year     Linf         K        t0\n#R|  1 2005 703.7264 0.2529961 -1.903874\n#R|  2 2008 703.7264 0.2529961 -1.903874\n#R|  3 2011 703.7264 0.2529961 -1.903874\n#R|  4 2014 703.7264 0.2529961 -1.903874\n\n\nThe vbLOOP1() function needs to be modified by replacing all instances of sex to year. Be careful to change all instances here.\n\nvbLOOP1 &lt;- function(grp,dat,svs,oagerng,eagerng,interval=\"confidence\") {\n  ## !! This requires a 'year', 'tl', and 'age' variable in data frames !!\n  ## !!   Otherwise need to change 'year', 'tl', and 'age' below\n  ## !!   Everything else can stay as is\n  \n  ## Loop notification (for peace of mind)\n  cat(grp,\"Loop\\n\")\n  ## Isolate group's data, starting values, and age range\n  dat1 &lt;- dplyr::filter(dat,year==grp)\n  sv1 &lt;- svs |&gt;\n    filter(year==grp) |&gt;\n    select(-year) |&gt;\n    as.list()\n  oagerng1 &lt;- filter(oagerng,year==grp)\n  ## Make ages for predictions\n  ages &lt;- seq(min(eagerng),max(eagerng),length.out=101)\n  ## Fit von B to that group\n  fit1 &lt;- nls(tl~vb(age,Linf,K,t0),data=dat1,start=sv1)\n  ## Make data frame of predicted mean lengths at age with CIs\n  preds1 &lt;- data.frame(year=grp,\n                       age=ages,\n                       fit=investr::predFit(fit1,data.frame(age=ages),\n                                            interval=interval)) |&gt;\n    mutate(inrng=age&gt;=oagerng1$min & age&lt;=oagerng1$max) |&gt;\n    as.data.frame()\n  ## Rename variables\n  names(preds1) &lt;- c(\"year\",\"age\",\"fit\",\"lwr\",\"upr\",\"inrng\")\n  ## Return data frame\n  preds1\n}\n\nThis code is largerly the same as in the previous example except that the name of the observed data is changed as is the range over which the model will be evaluated.\n\npreds &lt;-  NULL\nfor (i in grps) preds &lt;- rbind(preds,vbLOOP1(i,wfT,svLKt,obsagerng,c(0,14)))\n\n#R|  2005 Loop\n#R|  2008 Loop\n#R|  2011 Loop\n#R|  2014 Loop\n\npeek(preds)\n\n#R|      year   age      fit      lwr      upr inrng\n#R|  1   2005  0.00 240.2624 210.7322 269.7926 FALSE\n#R|  21  2005  2.80 453.7806 447.9004 459.6609  TRUE\n#R|  43  2005  5.88 585.6538 570.6736 600.6340  TRUE\n#R|  64  2005  8.82 655.0832 622.4730 687.6935  TRUE\n#R|  85  2005 11.76 693.9627 642.5513 745.3741 FALSE\n#R|  106 2008  0.56 308.2649 300.5394 315.9905 FALSE\n#R|  128 2008  3.64 534.2380 528.2457 540.2303  TRUE\n#R|  149 2008  6.58 599.0074 589.8620 608.1528  TRUE\n#R|  170 2008  9.52 619.1141 602.5247 635.7036 FALSE\n#R|  191 2008 12.46 625.3559 605.0004 645.7115 FALSE\n#R|  213 2011  1.40 399.5019 396.3923 402.6116  TRUE\n#R|  234 2011  4.34 574.1318 568.6675 579.5961  TRUE\n#R|  255 2011  7.28 634.0238 625.8571 642.1905  TRUE\n#R|  276 2011 10.22 654.5647 642.6946 666.4349  TRUE\n#R|  298 2011 13.30 661.7922 647.7538 675.8307 FALSE\n#R|  319 2014  2.10 457.4716 454.3409 460.6023  TRUE\n#R|  340 2014  5.04 582.3197 576.4395 588.1998  TRUE\n#R|  361 2014  7.98 625.4476 613.3847 637.5105  TRUE\n#R|  383 2014 11.06 640.7339 624.5441 656.9237 FALSE\n#R|  404 2014 14.00 645.6264 627.5198 663.7329 FALSE\n\n\nOnce again, change all instances of sex to year below. I also need to modify the limits= of the y-axis and the breaks= of the x-axis. Also note that I removed scale_color_manual() so that the default colors would be used. If you don’t want to use the default colors you would need to make sure that you identify as many colors as you have groups (four in this case).\n\nggplot() +\n  geom_ribbon(data=preds,aes(x=age,ymin=lwr,ymax=upr,fill=year),alpha=0.25) +\n  geom_point(data=wfT,aes(x=age,y=tl,color=year),\n             size=2,alpha=0.3) +\n  geom_line(data=preds,aes(x=age,y=fit,color=year),\n              linewidth=1,linetype=\"dashed\") +\n  geom_line(data=filter(preds,inrng),aes(x=age,y=fit,color=year),\n              linewidth=1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,800)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:16) +\n  theme(panel.grid.minor.x=element_blank(),\n        legend.position=\"none\") +\n  facet_wrap(vars(year))\n\n\n\n\n\n\n\nFigure 6: Fit of typical von Bertalanffy growth function to female Lake Erie Walleye in 2005, 2008, 2011, and 2014. The dashed lines show the model fits outside the range of observed ages for each sex.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\n\nOgle, D. H., T. O. Brenden, and J. L. McCormick. 2017. Growth Estimation: Growth Models and Statistical Inference. Pages 265–359 in M. C. Quist and D. A. Isermann, editors. Age and Growth of Fishes: Principles and Techniques. American Fisheries Society, Bethesda, MD.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2019,\n  author = {H. Ogle, Derek},\n  title = {Von {Bertalanffy} {Growth} {Plots} {II}},\n  date = {2019-01-02},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2020-1-2_vonB_plots_2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2019, January 2. von Bertalanffy Growth Plots II. https://fishr-core-team.github.io/fishR//blog/posts/2020-1-2_vonB_plots_2."
  },
  {
    "objectID": "blog/posts/2019-12-28_Length_Frequency_Histograms/index.html",
    "href": "blog/posts/2019-12-28_Length_Frequency_Histograms/index.html",
    "title": "Length Frequency Histograms in ggplot2",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\n \n\nIntroduction\nA common graphic produced by many fisheries scientists is a length frequency histogram. As it turns out there are a few “tricks” to make a length frequency histogram appear as I expect most fisheries folks would want it to appear – primarily, left-inclusive (i.e., 100 would be in the 100-110 bin and not the 90-100 bin). In this post, I show how to construct a length frequency histograms with ggplot2 using my preferences.\nThe data are lengths of Lake Erie Walleye (Sander vitreus) captured during October-November, 2003-2014.1 My primary interest is in the tl (total length in mm), sex, and loc variables2 and I will focus on 2014 (as an example).\n1 These data are available in the FSAdata package and formed many of the examples in Chapter 12 of the Age and Growth of Fishes: Principles and Techniques book.2 See here for more details\ndata(WalleyeErie2,package=\"FSAdata\")\nWE &lt;- dplyr::filter(WalleyeErie2,year==2014)\n\n \n\n\nBasic Length Frequency\nMaking the histogram (Figure 1) begins by identifying the data frame to use in data= and the tl variable to use for the x-axis as an aes()thetic in ggplot(). The histogram is then constructed with geom_histogram(), which I customize as follows:\n\nSet the width of the length bins with binwidth=.\nBy default the bins are centered on breaks created from binwidth=. The bins can be changed to begin on these breaks by using boundary= set to the desired value of a first break, regardless of whether that break is in the data or not. I use boundary=0 so that bins will start on breaks that make sense relative to binwidth= (e.g., 0, 25, 50, 75, etc.).\nBins are left-exclusive and right-inclusive by default. Use closed=\"left\" to make the bins the desired left-inclusive and right-exclusive.\nThe fill color of the bins is set with fill= (I prefer a slight gray).\nThe outline color of the bins is set with color= (defaults to the same as fill=; I prefer a dark boundary to make the bins obvious).\n\nThe scale_y_continuous() and scale_x_continuous() are primarily used to provide labels (i.e., names) for the y- and x-axes, respectively. By default, the bins of the histogram will “hover” slightly above the x-axis, which I find annoying. The expand= in scale_y_continuous() is used to expand the lower limit of the y-axis by a multiple of 0 (thus, not expand the lower-limit) and expand the upper limit of the y-axis by a multiple of 0.05 (thus, the upper-limit will by 5% higher than the tallest bin so that the top frame of the plot will not touch the tallest bin).3\n3 The resultant plot was assigned to an object so that name must be typed and run to see the plot.\nlenfreq1 &lt;- ggplot(data=WE,aes(x=tl)) +\n  geom_histogram(binwidth=25,boundary=0,closed=\"left\",\n                 fill=\"gray80\",color=\"black\") +\n  scale_y_continuous(name=\"Number of Fish\",expand=expansion(mult=c(0,0.05))) +\n  scale_x_continuous(name=\"Total Length (mm)\")\nlenfreq1\n\n\n\n\n\n\n\nFigure 1: Length frequency of histogram of Lake Erie Walleye, 2014.\n\n\n\n\n\nThis base object/plot can be modified by adding (using +) to it as demonstrated later.\n \n\n\nStacked by Other Variable\nIt may be useful to see the distribution of categories of fish (e.g., sex) within the length frequency bins (Figure 2). To do this, add fill= in the aes()thetic in ggplot() and set it equal to the variable that identifies the separation within each bin (e.g., sex). The bins will be stacked by this variable if position=\"stack\" in geom_histogram().4 The fill colors for each group can be set in a number of ways, but they are set manually below with scale_fill_manual().\n4 This is the default and would not need to be explicitly set below).\nlenfreq2 &lt;- ggplot(data=WE,aes(x=tl,fill=sex)) +\n  geom_histogram(binwidth=25,boundary=0,closed=\"left\",\n                 color=\"black\",position=\"stack\") +\n  scale_fill_manual(values=c(\"gray80\",\"gray40\")) +\n  scale_y_continuous(name=\"Number of Fish\",expand=expansion(mult=c(0,0.05))) +\n  scale_x_continuous(name=\"Total Length (mm)\")\nlenfreq2\n\n\n\n\n\n\n\nFigure 2: Length frequency histogram of Lake Erie Walleye 2014 stacked by sex.\n\n\n\n\n\nStacked histograms are difficult to interpret in my opinion. In a future post, I will show how to use empirical density functions to examine distributions among categories. For the time being, see below.\n \n\n\nSeparated by Other Variable(s)\nA strength of ggplot2 is that it can easily make the same plot for several different levels of another variable; e.g., separate length frequency histograms by sex (Figure 3). The plot (e.g., lenfreq from above) can be separated into different “facets” with facet_wrap(), which takes the variable to separate by within vars() as the first argument.\n\nlenfreq1 + facet_wrap(vars(sex))\n\n\n\n\n\n\n\nFigure 3: Length frequency histogram of Lake Erie Walleye 2014 separated by sex.\n\n\n\n\n\nIf the faceted groups have very different sample sizes then it may be useful to use a potentially different y-axis scale for each facet by including scales=\"free_y\" in facet_wrap() (Figure 4). Similarly, a potentially different scale can be used for each x-axis with scales=\"free_x\" or for both axes with scales=\"free\".\n\nlenfreq1 + facet_wrap(vars(sex),scales=\"free_y\")\n\n\n\n\n\n\n\nFigure 4: Length frequency histogram of Lake Erie Walleye 2014 separated by sex with different frequency scales.\n\n\n\n\n\nPlots may be faceted over multiple variables with facet_grid() (Figure 5), where the variables that identify the rows and variables for a grid of facets are included (within vars()) in rows= and cols=, respectively. Both scales can not be “free” with facet_grid(), the scale can only be “free” within a row or column.\n\nlenfreq1 + facet_grid(rows=vars(loc),cols=vars(sex),scales=\"free_y\")\n\n\n\n\n\n\n\nFigure 5: Length frequency histogram of Lake Erie Walleye 2014 separated by sex and location of capture with different frequency scales.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2019,\n  author = {H. Ogle, Derek},\n  title = {Length {Frequency} {Histograms} in Ggplot2},\n  date = {2019-12-28},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2019-12-28_Length_Frequency_Histograms},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2019, December 28. Length Frequency Histograms in ggplot2.\nhttps://fishr-core-team.github.io/fishR//blog/posts/2019-12-28_Length_Frequency_Histograms."
  },
  {
    "objectID": "blog/posts/2017-8-9_Mean_Weights/index.html",
    "href": "blog/posts/2017-8-9_Mean_Weights/index.html",
    "title": "Mean Weights at Ages From Lengths",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background) and set the random number seed for reproducibility of the randomly generated sample data below.\n\n\n\nlibrary(FSA)           # for headtail()\nlibrary(dplyr)         # for mutate(), group_by(), summarize(), \nlibrary(ggplot2)\nlibrary(patchwork)     # for combining plots\ntheme_set(theme_bw())\nset.seed(678394)  # for reproducibility of randomly created data\n\n \n\nIntroduction\nRecently I was tasked with estimating mean weights-at-age for data that contained no weights, but did contain lengths and ages (ages were from applying an age-length key). A weight-length relationship was available (derived from a smaller sample from the same population). A question arose about whether the weight-length relationship should be used to predict weights for individual fish and then summarized to estimate mean weights at age or whether the weight-length relationship should be applied to summarized mean lengths-at-age to estimate mean weights-at-age.\nThis issue has been addressed in the literature. Ricker (1975) states that the true mean weight is always greater, “on the order of 5%,” than the mean weight computed from the weight-length relationship with the mean length. Tesch (1971) suggested that the error in predicting mean weight-at-age with the weight-length relationship using mean length-at-age would be about 5-10%. In a simulation study, Nielsen and Schoch (1980) found that Tesch’s suggestion was too general and that error was less than 10% when the coefficient of variation (CV; standard deviation divided by mean) in lengths was less than 10%, but could be substantially higher when the CV was higher, with the specific result dependent on the weight-length regression exponent (b).1 Pienaar and Ricker (1968) and Beyer (1991) both suggested corrections to reduce the bias in the mean weight-at-age produced by the weight-length regression using the mean length.\n1 Nielsen and Schoch (1980) provided a nice geometric description of why this bias occurs.In this post, I explore the bias in using the weight-length regression to estimate mean weight-at-age from mean length-at-age and the correction factors suggested by Beyer (1991).\n \n\n\nSample Data\nI created a very simple population that consisted of lengths, weights, and ages of fish. Ages were derived from a population with roughly a 30% annual mortality rate. Mean lengths-at-age were modeled with a von Bertalanffy growth curve using a \\(L_{\\infty}\\)=650 mm, \\(K\\)=0.3, and \\(t_{0}\\)=0. Individual lengths were modeled from normal distributions using the modeled mean length and a constant standard deviation of 30 mm) within each age-class. Weights were predicted directly from a known weight-length relationship without any error (in wt1) and with a small amount of error (in wt2).\n\n# Generate some lengths\nages &lt;- 3:8\nN3 &lt;- 100      ## number at age-3\nA &lt;- 0.3       ## annual morality rate\nns &lt;- round(N3*(1-A)^(ages-min(ages)),0)  ## numbers at age\nLinf &lt;- 650    ## von B parameters\nK &lt;- 0.3\nt0 &lt;- 0\nmns &lt;- Linf*(1-exp(-K*(ages-t0)))         ## mean length-at-age\nsds &lt;- rep(30,length(mns))\nlens &lt;- NULL\nfor (i in 1:length(ages))                 ## individual lengths-at-age\n  lens &lt;- c(lens,rnorm(ns[i],mean=mns[i],sd=sds[i]))\nloga &lt;- -13.5  ## weight-length relationship parameters\nb &lt;- 3.2\n# Compute weights from the W-L relationship, w/ & w/o error\ndf &lt;- data.frame(age=rep(ages,ns),len=round(lens,0)) |&gt;\n  mutate(wt1=round(exp(loga+b*log(len)),0),\n         wt2=round(exp(loga+b*log(len)+rnorm(length(lens),mean=0,sd=0.1)),0))\nheadtail(df)\n\n#R|      age len  wt1  wt2\n#R|  1     3 374  235  197\n#R|  2     3 401  293  313\n#R|  3     3 369  225  210\n#R|  292   8 580  955  923\n#R|  293   8 605 1093 1081\n#R|  294   8 589 1003 1126\n\n\nModeling data in this way is simple, but at least somewhat realistic (Figure 1).\n\nlenh &lt;- ggplot(data=df,aes(x=len)) +\n  geom_histogram(binwidth=10,boundary=0,color=\"black\",fill=\"gray70\") +\n  scale_x_continuous(name=\"Length (mm)\") +\n  scale_y_continuous(name=\"frequency\",expand=expansion(mult=c(0,0.05)))\nlenw &lt;- ggplot(data=df,aes(x=wt2)) +\n  geom_histogram(binwidth=25,boundary=0,color=\"black\",fill=\"gray70\") +\n  scale_x_continuous(name=\"Weight (g)\") +\n  scale_y_continuous(name=\"frequency\",expand=expansion(mult=c(0,0.05)))\nwl &lt;- ggplot(data=df,aes(x=len,y=wt2)) +\n  geom_point(pch=21,fill=\"gray70\",color=\"black\")  +\n  scale_x_continuous(name=\"Length (mm)\") +\n  scale_y_continuous(name=\"Weight (g)\")\nal &lt;- ggplot(data=df,aes(x=age,y=len)) +\n  geom_jitter(pch=21,fill=\"gray70\",color=\"black\",width=0.05,height=0)  +\n  scale_x_continuous(name=\"Age (years)\") +\n  scale_y_continuous(name=\"Length (mm)\")\n(lenh + lenw) / (wl + al)\n\n\n\n\n\n\n\nFigure 1: Histograms of lengths (upper left) and weights (upper right) and scatterplots of weight versus length (lower left) and length versus age (lower right).\n\n\n\n\n\n \n\n\nExplore the Bias\nBelow the mean length (mnlen) and mean weight without any random erro (i.e., the “true” mean weight for individuals modeled above; true.mnwt) are calculated (using group_by() and summarize()). The predicted mean weight at each age is then computed from the mean length at each age and the weight-length relationship (i.e., pr.mnwt using mutate() and the regression coefficient values from above). Finally a percentage error between the predicted and true mean weights is computed for each age (in dif.minwt).2\n2 The as.data.frame() is used here because I don’t like the tibble format that is returned by summarize().\nsum1 &lt;- group_by(df,age) |&gt;\n  summarize(mnlen=mean(len),\n            true.mnwt=mean(wt1)) |&gt;\n  mutate(pr.mnwt=exp(loga+b*log(mnlen)),\n         dif.mnwt=(pr.mnwt-true.mnwt)/true.mnwt*100) |&gt;\n  as.data.frame()\n\nThe results from this one sample show that the mean weights computed from the mean lengths and the weight-length regression (i.e., pr.mnwt) are lower at each age then the “true” mean weights (true.mnwt; Table 1).\n\n\n\n\n\n\n\n\nage\nmnlen\ntrue.mnwt\npr.mnwt\ndif.mnwt\n\n\n\n\n3\n391\n276\n270\n-2.15\n\n\n4\n454\n448\n438\n-2.23\n\n\n5\n511\n644\n636\n-1.29\n\n\n6\n550\n817\n806\n-1.35\n\n\n7\n570\n912\n906\n-0.72\n\n\n8\n594\n1034\n1028\n-0.58\n\n\n\n\n\n\nTable 1: Summary table using weights without any error and no correction for the predicted mean weights.\n\n\n\n\n \nOf course, weight-length relationships are not without error, so the weights with a small amount of random error were used to determine if the pattern of a negative bias when predicting mean weights from mean lengths persists with more realistic data.3 Results from this one set of more realistic data showed a similar, though not as consistent, degree of negative bias when predicting mean weights from mean lengths (Table 2).\n3 Only a small error was added because the relationship between weight and length is very strong for most fishes. The r-squared for this relationship was a realistic 0.969.\n\n\n\n\n\n\n\nage\nmnlen\ntrue.mnwt\npr.mnwt\ndif.mnwt\n\n\n\n\n3\n391\n277\n270\n-2.55\n\n\n4\n454\n448\n438\n-2.24\n\n\n5\n511\n643\n636\n-1.12\n\n\n6\n550\n830\n806\n-2.87\n\n\n7\n570\n906\n906\n-0.02\n\n\n8\n594\n1054\n1028\n-2.50\n\n\n\n\n\n\nTable 2: Summary table using weights with a small amount of error and no correction for the predicted mean weights.\n\n\n\n\n \nI then performed the analysis above 1000 times, keeping track of the percent error between the predicted weight and the true mean weight for each age for each sample.4\n4 The code here also uses the correction factors discussed above; those results are presented further below.\n\n\nThe results of this simulation suggest an average negative bias near 4% for age-3 fish and between about 1 and 2% for all older fish (Table 3). Note that the CV in length for each age varied between 2.7% and 9.8% in these simulations.\n\n\n\n\n\n\n\n\nage3\nage4\nage5\nage6\nage7\nage8\n\n\n\n\n-2.49\n-1.97\n-1.74\n-1.5\n-1.32\n-1.39\n\n\n\n\n\n\nTable 3: Mean percentage difference in uncorrected predicted mean and true mean weights by age class for 1000 simulations.\n\n\n\n\n \nSimilar patterns were found using different values of the weight-length relationship exponent \\(b\\) (see Appendix). However, larger negative biases were observed as the standard deviation in lengths increased (see Appendix).\n\n\nA Possible Correction\nAs noted above, Pienaar and Ricker (1968) and Beyer (1991) offered methods to reduce or eliminate the bias from using the weight-length regression to estimate mean weight-at-age from mean length-at-age. Beyer’s corrections were simple as they were based on the CV of lengths and the \\(b\\) coefficient from the weight-length regression. Beyer specifically offered three possible bias correcting factors for isometric growth, allometric growth assuming a lognormal distribution, and allometric growth assuming a normal distributions for lengths. Here I will only consider Beyer’s corrections for allometric growth with lognormal (i.e, Beyer’s equation 16) and normal (i.e., Beyer’s equation 18) distributions of lengths within age classes.\nBeyer’s formulae are implemented by modifying group_by() and summarize() used previously. Specifically the standard deviation of lengths is calculated (in sdlen) so that the CV of lengths can be calculated (in cvlen). The correction factors are then computed (in cfn for the normal distribution and cfl for the lognormal distribution).\n\nsum2a &lt;- group_by(df,age) |&gt;\n  summarize(mnlen=mean(len),sdlen=sd(len),\n            true.mnwt=mean(wt2)) |&gt;\n  mutate(cvlen=sdlen/mnlen,\n         cfn=1+b*(b-1)/2*(cvlen^2),      # eqn 18\n         cfl=(1+cvlen^2)^(b*(b-1)/2),    # eqn 16\n         pr.mnwt=exp(loga)*mnlen^b,\n         pr.mnwt.n=pr.mnwt*cfn,\n         pr.mnwt.l=pr.mnwt*cfl,\n         dif.mnwt=(pr.mnwt-true.mnwt)/true.mnwt*100,\n         dif.mnwt.n=(pr.mnwt.n-true.mnwt)/true.mnwt*100,\n         dif.mnwt.l=(pr.mnwt.l-true.mnwt)/true.mnwt*100) |&gt;\n  as.data.frame()\n\nThese calculations were again repeated 1000 times and summarized in the bottom two rows of Table 4. These results suggest that the mean bias in predicted weights at age when corrected with a correction factor appear to only be on the order of a quarter to a half a percent. These corrections seems to perform fairly consistently across a few different values of the weight-length regression exponent \\(b\\) and standard deviations in lengths (see Appendix).\n\n\n\n\n\n\n\n\n\nage3\nage4\nage5\nage6\nage7\nage8\n\n\n\n\nNo correction\n-2.49\n-1.97\n-1.74\n-1.50\n-1.32\n-1.39\n\n\nNormal (eqn 18)\n-0.43\n-0.45\n-0.52\n-0.45\n-0.35\n-0.49\n\n\nLognormal (eqn 16)\n-0.41\n-0.45\n-0.51\n-0.44\n-0.35\n-0.48\n\n\n\n\n\n\nTable 4: Mean percentage difference in two types of corrected predicted mean and true mean weights by age class for 1000 simulations.\n\n\n\n\n \n\n\nWhy Worry About This?\nWhy am I worried about this if the bias is on the order of 4% or less? First, for my application, we are estimating mean weight so that we can expand to total biomass. While a 4% error on an individual fish may seem inconsequential, that error can become quite important when expanded to represent total biomass, especially when it is a consistent negative bias.\nSo, why worry about correction factors when I can easily predict the weight for individual fish with the weight-length regression and then summarize these fish to get mean weight at age? In my situation, it appears that some of our mean lengths at age, and by extension mean weights at age, are poorly estimated because of small sample sizes at some ages. I am considering fitting a growth model (e.g., von Bertalanffy growth model) to the length-at-age data such that the fitted model can be used to predict mean lengths at age. The advantage of this is that information at other ages can be used to inform the calculation of mean length at an age. [The potential downside, of course, is that I would be prescribing a smooth curve to the growth trajectory.] If I can then estimate mean weights at age with minimal bias from the mean lengths at age from the growth model, then this could (I would need to test this) be beneficial in my situation.\n \n\n\nSummary\nMean weights at age appear to be estimated with bias when a weight-length relationship is applied to mean lengths at age without any correction factor. The correction factors suggested by Beyer (1991) are easy to implement and seem to reduce the bias in predicted mean weights-at-age to near negligible levels. Thus, if mean weights at age cannot be predicted from individual fish, then it may be possible to get reasonable estimates from the weight-length relationship and mean lengths at age.\n \n\n\nReferences\n\n\nBeyer, J. E. 1991. On length-weight relationships. Part II. Computing mean weights from length statistics. FishByte 9:50–54.\n\n\nNielsen, L. A., and W. F. Schoch. 1980. Errors in estimating mean weight and other statistics from mean length. Transactions of the American Fisheries Society 109(3):319–322.\n\n\nPienaar, J. V., and W. K. Ricker. 1968. Estimating mean weight from length statistics. Journal of the Fisheries Research Board of Canada 25(12):2743–2747.\n\n\nRicker, W. E. 1975. Computation and Interpretation of Biological Statistics of Fish Populations. Bulletin of the Fisheries Research Board of Canada 191.\n\n\nTesch, F. W. 1971. Age and Growth. Pages 97–130 in W. E. Ricker, editor. Methods for {}Assessment{} of {}Fish{} {}Populations{}, 2nd edition. Blackwell Scientific Publications, Oxford, England.\n\n\n \n\n\nAppendix: Summaries Using Different Values of b and SDs\n\n\n\n\n\n\n\nage3\nage4\nage5\nage6\nage7\nage8\n\n\n\n\nNo correction\n-1.55\n-1.28\n-1.06\n-0.91\n-0.84\n-0.90\n\n\nNormal (eqn 18)\n-0.43\n-0.47\n-0.39\n-0.34\n-0.33\n-0.43\n\n\nLognormal (eqn 16)\n-0.43\n-0.47\n-0.39\n-0.34\n-0.33\n-0.43\n\n\n\nSummary table with everything the same but b=2.5.\n\n \n\n\n\n\n\n\n\nage3\nage4\nage5\nage6\nage7\nage8\n\n\n\n\nNo correction\n-2.21\n-1.74\n-1.48\n-1.38\n-1.16\n-1.17\n\n\nNormal (eqn 18)\n-0.44\n-0.46\n-0.42\n-0.48\n-0.34\n-0.39\n\n\nLognormal (eqn 16)\n-0.42\n-0.46\n-0.42\n-0.48\n-0.33\n-0.39\n\n\n\nSummary table with everything the same but b=3.0.\n\n \n\n\n\n\n\n\n\nage3\nage4\nage5\nage6\nage7\nage8\n\n\n\n\nNo correction\n-3.07\n-2.29\n-2.04\n-1.73\n-1.59\n-1.50\n\n\nNormal (eqn 18)\n-0.47\n-0.42\n-0.52\n-0.41\n-0.40\n-0.40\n\n\nLognormal (eqn 16)\n-0.45\n-0.41\n-0.51\n-0.40\n-0.39\n-0.39\n\n\n\nSummary table with everything the same but b=3.5.\n\n \n\n\n\n\n\n\n\nage3\nage4\nage5\nage6\nage7\nage8\n\n\n\n\nNo correction\n-5.98\n-4.50\n-3.82\n-3.32\n-2.93\n-2.75\n\n\nNormal (eqn 18)\n-0.38\n-0.46\n-0.48\n-0.42\n-0.28\n-0.28\n\n\nLognormal (eqn 16)\n-0.26\n-0.40\n-0.44\n-0.39\n-0.25\n-0.26\n\n\n\nSummary table with everything the same but SDs at 50.\n\n \n\n\n\n\n\n\n\nage3\nage4\nage5\nage6\nage7\nage8\n\n\n\n\nNo correction\n-10.66\n-8.03\n-6.71\n-5.84\n-5.20\n-4.96\n\n\nNormal (eqn 18)\n-0.36\n-0.36\n-0.44\n-0.30\n-0.23\n-0.27\n\n\nLognormal (eqn 16)\n0.09\n-0.12\n-0.28\n-0.17\n-0.13\n-0.18\n\n\n\nSummary table with everything the same but SDs at 70.\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2017,\n  author = {H. Ogle, Derek},\n  title = {Mean {Weights} at {Ages} {From} {Lengths}},\n  date = {2017-08-09},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2017-8-9_Mean_Weights},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2017, August 9. Mean Weights at Ages From Lengths. https://fishr-core-team.github.io/fishR//blog/posts/2017-8-9_Mean_Weights."
  },
  {
    "objectID": "blog/posts/2017-4-20_BlandAltman/index.html",
    "href": "blog/posts/2017-4-20_BlandAltman/index.html",
    "title": "Bland-Altman Plot for Age Comparisons?",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)            ## for WhitefishLC, headtail()\nlibrary(dplyr)          ## for mutate()\nlibrary(ggplot2)\nlibrary(BlandAltmanLeh) ## for bland.altman.plot()\nlibrary(mgcv)\ntheme_set(theme_bw())\n\n \n\nIntroduction\nIn this post, I describe the Bland-Altman plot, demonstrate how to construct it in R, give a mild critique of its use for comparing fish age estimates, and develop an alternative that is meant to address my critique. This is largely a “thinking out loud” exercise so I am open to suggestions you may have.\nThe WhitefishLC data frame from FSA.1 contains age readings made by two readers on scales, fin rays, and otoliths, along with consensus readings for each structure. These data will be used throughout this post.\n1 These data are loaded automatically with library(FSA).\nheadtail(WhitefishLC)\n\n#R|      fishID  tl scale1 scale2 scaleC finray1 finray2 finrayC otolith1 otolith2\n#R|  1        1 345      3      3      3       3       3       3        3        3\n#R|  2        2 334      4      3      4       3       3       3        3        3\n#R|  3        3 348      7      5      6       3       3       3        3        3\n#R|  149    214 599     14     16     15      14      14      14       16       16\n#R|  150    216 636      8     10      9      10       8      10       15       15\n#R|  151    218 551      5      7      6       6       9       8       13       13\n#R|      otolithC\n#R|  1          3\n#R|  2          3\n#R|  3          3\n#R|  149       16\n#R|  150       15\n#R|  151       13\n\n\n\n\nBland-Altman Plot\nThe Bland-Altman plot (Bland and Altman 1986) is commonly used in medical and chemistry research to assess agreement between two measurement or assay methods (Giavarina 2015). McBride (2015) used the Bland-Altman plot in his simulation study of the effects of accuracy and precision on the ability to diagnose agreement between sets of fish age estimates. McBride (2015) noted that Bland-Altman plots “readily depict both bias and imprecision” and that this was summarized for “the entire sample, rather than specific age classes.” Despite this, I am aware of only two entries in the fisheries literature that used the Bland-Altman plot to compare fish age estimates (one in the grey literature, one in a thesis). Below, I describe the Bland-Altman plot and then offer a modified version for comparing estimates of fish age.\nThe Bland-Altman plot is a scatterplot where the differences in two age estimates are on the y-axis and means of the two age estimates are on the x-axis. The plot may be augmented with several horizontal lines at the following locations (Figure 1):\n\nZero,\nMean difference in ages (heavy red dashed line),\nUpper and lower 95% confidence limits for the mean difference in ages (dark red dashed lines),\nUpper and lower 95% “agreement limits” (usually 1.96 times the standard deviation for the difference in ages above and below the mean difference in ages; heavy dashed blue lines), and\nUpper and lower 95% confidence limits for the upper and lower “agreement limits” (dashed dark blue lines).\n\n\n\n\n\n\n\n\n\nFigure 1: Bland-Altman plot for comparing scale to otolith age estimates of Lake Champlain Lake Whitefish. Thiw was constructed with BlandAltmanLeh.\n\n\n\n\n\n \nAs a general rule, a 95% confidence interval for the mean difference that does not contain zero suggests a difference (or a bias) between the two age estimates. For example, a bias is evident in Figure 1. In addition, one would expect 95% of the points to fall within the “agreement limits.” Points that fall outside this range may be considered further as possible outliers. Log differences have been used if the differences are not normally distributed and the percentage difference (where the difference is divided by the mean age) have also been used (Giavarina 2015).\nThe Bland-Altman plot in Figure 1 was created with bland.altman.plot() from the BlandAltmanLeh package (Lehnert 2015b). Other R functions exist for creating Bland-Altman plots (or the equivalent “Tukey’s Mean Difference Plot”). However, I demonstrate below how to construct this plot from “scratch.”\n \n\n\nConstructing a Bland-Altman Plot\nIn this example, a Bland-Altman plot is created to compare consensus (between two readers) scale (scaleC) and otolith (otolithC) age estimates for Lake Champlain Lake Whitefish. For simplicity, I restrict (using select()) the data frame below to just these two variables plus fishID and tl. In addition, for making the plot the mean and differences between the two age estimates are added the data frame (using mutate()).\n\nwfSO &lt;- WhitefishLC |&gt;\n  select(fishID,tl,scaleC,otolithC) |&gt;\n  mutate(meanSO=(scaleC+otolithC)/2,\n         diffSO=scaleC-otolithC)\nheadtail(wfSO)\n\n#R|      fishID  tl scaleC otolithC meanSO diffSO\n#R|  1        1 345      3        3    3.0      0\n#R|  2        2 334      4        3    3.5      1\n#R|  3        3 348      6        3    4.5      3\n#R|  149    214 599     15       16   15.5     -1\n#R|  150    216 636      9       15   12.0     -6\n#R|  151    218 551      6       13    9.5     -7\n\n\nA data frame of summary statistics is also needed. Below the sample, mean, standard deviation, and standard error for the differences is ages are calculated and then approximate 95% confidence limits for the differences (i.e., using the SE) and agreement limits (i.e., using the SD) are added.\n\nsumSO &lt;- wfSO |&gt;\n  summarize(n=n(),\n            mean=mean(diffSO,na.rm=TRUE),\n            sd=sd(diffSO,na.rm=TRUE),\n            se=se(diffSO,na.rm=TRUE)) |&gt;\n  mutate(lwrci=mean-1.96*se,\n         uprci=mean+1.96*se,\n         lwrag=mean-1.96*sd,\n         uprag=mean+1.96*sd)\nsumSO\n\n#R|      n      mean       sd        se     lwrci     uprci     lwrag    uprag\n#R|  1 151 -1.781457 2.553152 0.2077726 -2.188691 -1.374223 -6.785636 3.222722\n\n\nThe Bland-Altman plot can then be constructed in layers using ggplot() (Figure 2). Horizontal lines at zero, the mean difference, confidence limits for the mean difference, and the agreement limits are added with geom_hline() using yintercept= to denoted the location of the line and varying colors (with color=), line widths (with linewidth=), and line types (with linetype=). geom_point() is then used to add the observed differences in ages at the mean age as points that are slightly transparent such that as more points are plotted on top of each other the apparent point becomes darker. Finally, labels are provided for the x- and y-axes.\n\nggplot(data=wfSO,mapping=aes(x=meanSO,y=diffSO)) +\n  geom_hline(yintercept=0,color=\"gray70\",linewidth=1) +\n  geom_hline(yintercept=sumSO$mean,\n             color=\"red\",linewidth=1,linetype=\"dashed\") +\n  geom_hline(yintercept=c(sumSO$lwrci,sumSO$uprci),\n             color=\"darkred\",linewidth=1,linetype=\"dashed\") +\n  geom_hline(yintercept=c(sumSO$lwrag,sumSO$uprag),\n             color=\"blue\",linewidth=1,linetype=\"dashed\") +\n  geom_point(col=col2rgbt(\"black\",1/5),size=2) +\n  scale_x_continuous(name=\"Mean Age\") +\n  scale_y_continuous(name=\"Scale-Otolith Age\")\n\n\n\n\n\n\n\nFigure 2: Bland-Altman plot for comparing scale to otolith age estimates of Lake Champlain Lake Whitefish constructed using ggplot.\n\n\n\n\n\n \n\n\nA Critique of the Bland-Altman Plot for Age Comparisons\nI like that Bland-Altman plots (relative to age bias plots) do not require that one of the variables be designated as the “reference” group. This may be more useful when comparing age estimates where one set of estimates is not clearly a priori considered to be more accurate (e.g., comparing readers with similar levels of experience).\nHowever, I don’t like the following characteristics of (default) Bland-Altman plots.\n\nThere may be considerable overlap of the plotted points because of the discrete nature of most age data. Various authors have dealt with this by adding a “petal” to the point for each overplotted point to make a so-called “sunflower plot” (Lehnert 2015a) or using bubbles that are proportional to the number of overplotted points (McBride 2015). However, I find the “sunflowers” and “bubbles” to be distracting. I addressed this issue with transparent points above.\nThe “agreement lines” are not particularly useful. They may be useful for identifying outliers, but an egregious outlier will likely stand out without these lines.\nThe single confidence interval for the mean difference suggests that any bias between the sets of estimates is “constant” across the range of mean ages. This can be relaxed somewhat if the percentage difference is plotted on the y-axis. However, neither of these allows for more complex situations where the bias is nonlinear with age. For example, a common situation of little difference between the estimates at young ages, but increasing differences with increasing ages (e.g., Figure 2) is not well-represented by this single confidence interval.\n\n \n\n\nA Modified Bland-Altman Plot for Age Comparisons\nThe third issue above has been addressed with some Bland-Altman plots by fitting a linear regression that describes the difference in age estimates as a function of mean age (Giavarina 2015). However, this only allows for a linear relationship, which may not represent or reveal more interesting nonlinear relationships. A “smoother” (a loess or a generalized additive model (GAM)) could be used to estimate a “smoothed” potentially nonlinear relationship between the differences in ages and the means of the ages. For example, a GAM smoother is added to Figure 2 with geom_smooth() below.2\n2 geom_smooth() defaults to a loess smoother when n&lt;1000 points. Use method=\"gam\" to force using a GAM smoother. I use the GAM smoother for the hypothesis testing shown at the bottom of this section.\nggplot(data=wfSO,mapping=aes(x=meanSO,y=diffSO)) +\n  geom_hline(yintercept=0,color=\"gray70\",linewidth=1) +\n  geom_hline(yintercept=c(sumSO$lwrci,sumSO$uprci),\n             color=\"red\",linewidth=1,linetype=\"dashed\") +\n  geom_hline(yintercept=c(sumSO$lwrag,sumSO$uprag),\n             color=\"blue\",linewidth=1,linetype=\"dashed\") +\n  geom_point(col=col2rgbt(\"black\",1/5),size=2) +\n  scale_x_continuous(name=\"Mean Age\") +\n  scale_y_continuous(name=\"Scale-Otolith Age\") +\n  geom_smooth(method=\"gam\",color=\"gray30\")\n\n\n\n\n\n\n\nFigure 3: Bland-Altman plot for comparing scale to otolith age estimates of Lake Champlain Lake Whitefish with a loess smoother.\n\n\n\n\n\nThese results suggest that there is little difference between scale and otolith age estimates up to a mean age estimate of approximately five, after which age estimates from scales are less than age estimates from otoliths, with the difference between the two generally increasing with increasing mean age (Figure 3).\nA similar plot is shown for the comparison of otolith age estimates between two readers in Figure 4. Also note (see below) that the smoother term is not significant for the between-reader comparison of otolith age estimates, which suggests no relationship between the differences in ages and the mean age. In addition, the intercept term is not significantly different from zero, which indicates that there is not a constant bias between the two readers.\n\n\n\n\n\n\n\n\nFigure 4: Bland-Altman plot for comparing age estimates among two readers of Lake Champlain Lake Whitefish otoliths.\n\n\n\n\n\n \nHypotheses about the relationship between the difference in ages and the mean age can be tested with the GAM smoother model. The first line below fits the GAM smoother model used by geom_smooth(). First note that gam() is from the mgcv package and is preceded by mgcv:: so as not to load the whole package. The formula in gam has the difference in ages on the left side and a smoother term (denoted with the s() on the right side. The smoother funtion contains the mean aes and bs=\"cs\" (to force using the same algorithm as geom_smoother()). The saved object is the submitted to summary() to get, among other things, hypothesis test results about the significance of the smoother term.\n\ngamSO &lt;- mgcv::gam(diffSO~s(meanSO,bs=\"cs\"),data=wfSO)\nsummary(gamSO)\n\n#R|  \n#R|  Family: gaussian \n#R|  Link function: identity \n#R|  \n#R|  Formula:\n#R|  diffSO ~ s(meanSO, bs = \"cs\")\n#R|  \n#R|  Parametric coefficients:\n#R|              Estimate Std. Error t value Pr(&gt;|t|)\n#R|  (Intercept)  -1.7815     0.1473   -12.1   &lt;2e-16\n#R|  \n#R|  Approximate significance of smooth terms:\n#R|              edf Ref.df     F p-value\n#R|  s(meanSO) 8.044      9 17.14  &lt;2e-16\n#R|  \n#R|  R-sq.(adj) =  0.498   Deviance explained = 52.4%\n#R|  GCV = 3.4842  Scale est. = 3.2755    n = 151\n\n\nThe p-value for the significance of the smoother term (i.e., whether it adds significantly to the model or not) is &lt;2e-16 as found under “Approximate significance of smooth term:”. This indicates that the smoother term is important to the model and that mean difference in age depends on the mean age. Thus, the difference in ages is not the same for all mean ages.\n\n\n\nA similar table for the comparison of otolith ages between two readers shows that the smoother term is not significant (0.958); thus, the difference in otolith ages does not depend on the mean age. In addition, the intercept term for the model is not significantly different from 0 (1.000), which along with the result about the smoother, suggests that the mean difference in ages is 0 regardless of the mean otolith age.\n\n\n#R|  \n#R|  Family: gaussian \n#R|  Link function: identity \n#R|  \n#R|  Formula:\n#R|  diffOO ~ s(meanOO, bs = \"cs\")\n#R|  \n#R|  Parametric coefficients:\n#R|                Estimate Std. Error t value Pr(&gt;|t|)\n#R|  (Intercept) -2.443e-18  6.090e-02       0        1\n#R|  \n#R|  Approximate significance of smooth terms:\n#R|                  edf Ref.df F p-value\n#R|  s(meanOO) 4.368e-10      9 0   0.958\n#R|  \n#R|  R-sq.(adj) =  -2.79e-12   Deviance explained = 1.26e-11%\n#R|  GCV = 0.56373  Scale est. = 0.56      n = 151\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nBland, J. M., and D. G. Altman. 1986. Statistical methods for assessing agreement between two methods of clinical measurement. Lancet i:307–317.\n\n\nGiavarina, D. 2015. Understanding Bland Altman analysis. Biochemia Medica 25(2):141–151.\n\n\nLehnert, B. 2015a. What’s the main idea behind Bland-Altman plots?\n\n\nLehnert, B. 2015b, December 23. BlandAltmanLeh: Plots (Slightly Extended) Bland-Altman Plots.\n\n\nMcBride, R. S. 2015. Diagnosis of paired age agreement: A simulation of accuracy and precision effects. ICES Journal of Marine Science 72(7):2149–2167.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2017,\n  author = {H. Ogle, Derek},\n  title = {Bland-Altman {Plot} for {Age} {Comparisons?}},\n  date = {2017-04-20},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2017-4-20_BlandAltman},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2017, April 20. Bland-Altman Plot for Age Comparisons? https://fishr-core-team.github.io/fishR//blog/posts/2017-4-20_BlandAltman."
  },
  {
    "objectID": "blog/posts/2017-11-7_BackCalculationPlot/index.html",
    "href": "blog/posts/2017-11-7_BackCalculationPlot/index.html",
    "title": "Plots of Back-Calculated Lengths-At-Age",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)      # for filterD(), headtail()\nlibrary(dplyr)    # for select(), arrange(), mutate(), group_by(), et al.\nlibrary(nlme)     # for nlme()\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\n \n\nIntroduction\nOne reader commented on a past post via Twitter …\n\nNow that you solved the age-bias plot, how about the ‘best’ display of back-calculated length-at-age data, with VonB growth curve overlay?\n\nIn addition, I recently received a question related to the non-convergence of a hierarchical (mixed) model applied to fitting a von Bertalanffy growth function (VBGF) to back-calculated lengths at age. In exploring that question, I realized that a “good” plot of back-calculated lengths-at-age was needed to understand why the VBGF may (or may not) fit.\nThis post represents my initial attempts to visualize back-calculated lengths at age with what are basically spaghetti plots. Spaghetti plots show individual longitudinal traces for each subject (e.g., one example). Recently “spaghetti plots” were in the news to show modeled paths of hurricanes (e.g., I particularly enjoyed this critique).\n \n\n\nData\nIn this post, I examine back-calculated lengths (mm) at age for Walleye (Sander vitreus) captured from Lake Mille Lacs, Minnesota in late fall (September-October).1 These data were kindly provided by the Minnesota Department of Natural Resources, are available in FSAData, and were used extensively in Ogle et al. (2017). For simplicity of presentation here, these data were reduced (with filter()) to a single year and sex, several unneeded variables were removed (with select(), a new factored version of the estimated age variable was created (with mutate()), and the data were ordered by estimated, fish identification number, and back-calculated age.\n1 More details are here.\n## Get data\ndata(WalleyeML,package=\"FSAdata\")\n## Wrangle data\ndf &lt;- WalleyeML |&gt;\n  filter(Year==2002,Sex==\"F\") |&gt;\n  select(-Year,-Sex,-Scale.Rad,-Dist.Ann) |&gt;\n  mutate(fEst.Age=factor(Est.Age)) |&gt;\n  arrange(Est.Age,ID,BC.Age)\n## Examine results\nheadtail(df,n=5)\n\n#R|                 ID Est.Age  TL BC.Age BC.Len fEst.Age\n#R|  1    2002.10416.F       1 232      1  81.21        1\n#R|  2    2002.10493.F       1 248      1 114.60        1\n#R|  3    2002.10606.F       1 287      1 112.21        1\n#R|  4     2002.1153.F       1 273      1 116.29        1\n#R|  5     2002.1154.F       1 244      1 157.55        1\n#R|  1864 2002.20742.F      10 592      6 523.52       10\n#R|  1865 2002.20742.F      10 592      7 539.77       10\n#R|  1866 2002.20742.F      10 592      8 554.83       10\n#R|  1867 2002.20742.F      10 592      9 567.56       10\n#R|  1868 2002.20742.F      10 592     10 576.20       10\n\n\nThese fish were captured in late fall such that the observed length includes current year’s growth. However, the observed age does not account for time since the fish’s “birthday.” In other words, the observed age at capture should be a “fractional age” such that it represents completed years of growth plus the fraction of the current year’s growth season completed (i.e., the “current age” should be something like 10.9 rather than 10). An example of this is seen by comparing the observed length at capture (in TL) and the back-calculated length (in BC.Len) to age-1 for the first fish in the data.frame (first line in data shown above).\nSome of the plots below require a data frame where the length and age for the oldest age match in time. In other words, this data.frame should contain the length of the fish on the fish’s last “birthday.” With these data, that length is the back-calculated length at the age (in BC.Age) that matches the age of the fish at the time of capture (in Est.Age).2 An example of this data frame is below (especially compare the last five lines below to the last five lines in the previous data frame snippet above).\n2 With other data, that length may simply be the length of the fish at the time of capture.\n# data.frame of just lengths at last full age\ndf2 &lt;- df |&gt;\n  filter(BC.Age==Est.Age)\nheadtail(df2,n=5)\n\n#R|                ID Est.Age  TL BC.Age BC.Len fEst.Age\n#R|  1   2002.10416.F       1 232      1  81.21        1\n#R|  2   2002.10493.F       1 248      1 114.60        1\n#R|  3   2002.10606.F       1 287      1 112.21        1\n#R|  4    2002.1153.F       1 273      1 116.29        1\n#R|  5    2002.1154.F       1 244      1 157.55        1\n#R|  311 2002.20381.F      10 568     10 563.73       10\n#R|  312 2002.20483.F      10 594     10 580.81       10\n#R|  313 2002.20511.F      10 628     10 618.89       10\n#R|  314 2002.20688.F      10 620     10 611.08       10\n#R|  315 2002.20742.F      10 592     10 576.20       10\n\n\nFinally, in some plots below the mean back-calculated length at age is included.3\n3 as.data.frame() removes the tibble class and the remaining grouping level from this data frame.\n# data.frame of mean lengths at back-calculated ages\ndf3 &lt;- df |&gt;\n  group_by(fEst.Age,BC.Age) |&gt;\n  summarize(mnBC.Len=mean(BC.Len)) |&gt;\n  as.data.frame()\nheadtail(df3,n=5)\n\n#R|     fEst.Age BC.Age mnBC.Len\n#R|  1         1      1 111.3464\n#R|  2         2      1 113.4633\n#R|  3         2      2 225.3060\n#R|  4         3      1  98.5338\n#R|  5         3      2 211.4096\n#R|  51       10      6 505.5437\n#R|  52       10      7 538.0147\n#R|  53       10      8 558.6757\n#R|  54       10      9 571.4900\n#R|  55       10     10 579.8577\n\n\n\n\nPlots for Exploratory Data Analysis\nWhen modeling fish growth, I explore the data to make observations about (i) variability in length at each age and (ii) “shape” of growth (i.e., whether or not there is evidence for an horizontal asymptote or inflection point). When using repeated-measures data, for example from back-calculated lengths-at-age, I observe the “shape” of growth for each individual and (iii) identify how the back-calculated lengths at age from older fish compare to the back-calculated lengths at age from younger fish.4 In this section, I describe two plots (with some augmentations to the first type) that could be useful during this exploratory stage. In the last section, I describe a plot that could be used for publication.\n4 As major differences could suggest “Lee’s Phenomenon”, substantial changes in growth between year-classes or over time, or problems with the back-calculation model. \nFigure 1 shows longitudinal traces of back-calculated lengths at age for each fish, with separate colors for fish with different observed ages at capture. From this I see variability of approximately 100 mm at each age, individual fish that generally follow the typical shape of a VBGF, and some evidence that back-calculated lengths at earlier ages from “older” fish at capture are somewhat lower than the back-calculated lengths at earlier ages for “younger” fish at capture (this is most evident with the pinkish lines).\n\nsp &lt;- ggplot(data=df,aes(x=BC.Age,y=BC.Len,color=fEst.Age,group=ID)) +\n  geom_line(alpha=1/8) +\n  scale_x_continuous(\"Back-Calculated Age\") +\n  scale_y_continuous(\"Back-Calculated Length (mm)\") +\n  theme(legend.position=\"none\")\nsp\n\n\n\n\n\n\n\nFigure 1: Traces of back-calculated lengths at age for each fish. Traces with the same color are fish with the same observed age at capture.\n\n\n\n\n\n \nFigure 2 is the same as Figure 1 except that heavy lines have been added for the mean back-calculated lengths at age for fish from each age-at-capture. Here the evidence that back-calculated lengths at earlier ages from “older” fish at capture are somewhat lower than the back-calculated lengths at earlier ages for “younger” fish at capture is a little more obvious.\n\nsp +\n  geom_line(data=df3,\n            aes(x=BC.Age,y=mnBC.Len,group=fEst.Age,color=fEst.Age))\n\n\n\n\n\n\n\nFigure 2: Traces of back-calculated lengths at age for each fish with mean back-calculated lengths at age shown by the heavier lines. Traces with the same color are fish with the same observed age at capture.\n\n\n\n\n\n \nFigure 3 is the same as Figure 1 but also has points for the length and age of each fish at the last completed year of growth. These points are most near to the observed lengths and ages at capture5 and, thus, most nearly represent the data that would be used to fit a growth model if back-calculations had not been made. With this I observe that most traces of back-calculated lengths-at-age pass near these points, which suggests that “growth” has not changed dramatically over the time represented in these data and that the model used to back-calculate lengths and ages is not dramatically incorrect.\n5 They will be the observed lengths and ages at capture for datasets where the fish were captured prior to when the current season’s growth had commenced.\nsp + \n  geom_point(data=df2,aes(color=fEst.Age),alpha=1/5)\n\n\n\n\n\n\n\nFigure 3: Traces of back-calculated lengths at age for each fish. Traces with the same color are fish with the same observed age at capture.\n\n\n\n\n\n \nThe previous plots are cluttered because of the number of individual fish. This clutter can be somewhat reduced by creating separate spaghetti plots for each observed age at capture (Figure 4). From this, I observe the clear start of an asymptote at about age 5, an indication of a slight inflection around age 2 (most evident for fish that were older at capture), and that a good portion of the variability in length at early ages may be attributable to fish from different year-classes (i.e., of different observed ages-at-capture). It is, however, more difficult to see that back-calculated lengths at earlier ages from “older” fish at capture are somewhat lower than the back-calculated lengths at earlier ages for “younger” fish at capture.6\n6 I left the facet for age-1 fish in this plot to remind me that there were age-1 fish in these data, even though they do not show a trace. Also, the color here is superfluous and could be removed. I left the color here for comparison with previous figures.\n## Make facet labels for the plot below\nlbls &lt;- paste(\"Age =\",levels(df$fEst.Age))\nnames(lbls) &lt;- levels(df$fEst.Age)\n\n## Spaghetti plot separated by age at capture (with means)\nsp + \n  facet_wrap(~fEst.Age,labeller=labeller(fEst.Age=lbls)) +\n  geom_line(data=df3,aes(x=BC.Age,y=mnBC.Len,group=1),color=\"black\")\n\n\n\n\n\n\n\nFigure 4: Traces of back-calculated lengths at age for each fish separated by observed age at capture. Black lines in each facet are the mean back-calculated lengths at age for fish shown in that facet.\n\n\n\n\n\n \n\n\nPublication Graphic with Model Overlaid\nFor publication I would include traces for individual fish, but without color coding by estimated age-at-capture, and overlay the population-average growth model (i.e., the growth model expressed from using the “fixed effects” for each model parameter; Figure 5).7\n7 The model fitting code below is from Ogle et al. (2017).\n## Create the Von B Function using logged version of Linf (to ease scale issue)\nvbT &lt;- function(T,logLinf,K=NULL,t0=NULL) {\n  if (length(logLinf)==3) {\n    t0 &lt;- logLinf[[3]]\n    K &lt;- logLinf[[2]]\n    logLinf &lt;- logLinf[[1]]\n  }\n  exp(logLinf)*(1-exp(-K*(T-t0)))\n}\n\n## Generate starting values from last completed length and age data\nvbStarts(BC.Len~BC.Age,data=df2)\n\n#R|  $Linf\n#R|  [1] 624.3608\n#R|  \n#R|  $K\n#R|  [1] 0.2767802\n#R|  \n#R|  $t0\n#R|  [1] 0.2903228\n\nsvVB &lt;- list(fixed=c(logLinf=log(624.361),K=0.276,t0=0.290))\n\n## Fit hierarchical von B to back-calcd data (BE PATIENT)\n##   Will estimate population-average parameters (fixed-effect values)\n##   and parameters for each individual (in coefficients).\nfitVB &lt;- nlme(BC.Len~vbT(BC.Age,logLinf,K,t0),data=df,\n              fixed=list(logLinf~1,K~1,t0~1),\n              random=logLinf+K+t0~1|ID,\n              start=svVB)\n\n\n## von B equation for the plot\n( tmp &lt;- fixef(fitVB) )\n\n#R|    logLinf         K        t0 \n#R|  6.4662516 0.2784915 0.3922489\n\nlbl &lt;- paste(\"L==\",round(exp(tmp[1]),0),\n             \"*bgroup('(',1-e^-\",round(tmp[2],3),\n             \"(Age-\",round(tmp[3],2),\"),')')\")\n\nggplot(data=df,aes(x=BC.Age,y=BC.Len,group=ID)) +\n  geom_line(alpha=1/15) +\n  stat_function(data=data.frame(T=seq(1,10,0.1)),aes(x=T,y=NULL,group=NULL),\n                fun=vbT,args=list(logLinf=fixef(fitVB)),linewidth=1.1) +\n  geom_text(data=data.frame(x=7,y=120),aes(x=x,y=y,group=NULL,\n                            label=lbl),parse=TRUE,size=4) +\n  scale_x_continuous(\"Back-Calculated Age\") +\n  scale_y_continuous(\"Back-Calculated Length (mm)\") +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nFigure 5: Traces of back-calculated lengths at age for each fish (lighter black lines) with the population-averaged von Bertalanffy growth function (dark black line) overlaid. The equation for the best-fit von Bertalanffy growth function is shown.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nOgle, D. H., T. O. Brenden, and J. L. McCormick. 2017. Growth Estimation: Growth Models and Statistical Inference. Pages 265–359 in M. C. Quist and D. A. Isermann, editors. Age and Growth of Fishes: Principles and Techniques. American Fisheries Society, Bethesda, MD.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2017,\n  author = {H. Ogle, Derek},\n  title = {Plots of {Back-Calculated} {Lengths-At-Age}},\n  date = {2017-11-07},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2017-11-7_BackCalculationPlot},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2017, November 7. Plots of Back-Calculated Lengths-At-Age.\nhttps://fishr-core-team.github.io/fishR//blog/posts/2017-11-7_BackCalculationPlot."
  },
  {
    "objectID": "blog/posts/2016-11-11_Pauly-VBModel/index.html",
    "href": "blog/posts/2016-11-11_Pauly-VBModel/index.html",
    "title": "Pauly et al. (1992) Seasonal Cessation Growth Model",
    "section": "",
    "text": "Ogle (2017) described an algorithm for implementing the “seasonal cessation in growth model” of Pauly et al. (1992). Describing an algorithm for a growth model does not seem like a useful contribution to the literature, as fitting growth models is pretty standard fare for fisheries scientists. However, fitting the Pauly et al. (1992) model is complicated by the fact that the age data is modified by model parameters. This too may not seem particularly interesting given that \\(t_0\\) in the typical von Bertalanffy growth function (VBGF) also modifies the age variable (i.e., age-\\(t_0\\) appears in the VBGF equation). However, the modification of age in the Pauly et al. (1992) model is a shift AND a compression of the age variable, AND the degree of shift and compression is a function of age. This is different, and more complicated, than the simple shift along the age axis in the typical VBGF.\nAs I demonstrate in my note, the Pauly et al. (1992) model has, for all practical purposes, not been used in the fisheries literature. I believe this lack of use is largely attributable to lack of clarity in how to implement the model in computer code.1 My hope is that the algorithm in my note will clarify the implementation of this model. Fisheries scientists can then rigorously test whether this model provides any advantage over other growth models with a seasonal component.\n1 Pauly et al. (1992) did provide a brief description of their model, but not a detailed description of how the age variable was adjusted, and computer software for fitting their model, though that software, and its source code, have long been lost.Full R code implementin the algorithm is available here. Implementation of the algorithm depends on the FSA package, which is available on CRAN. Please let me know what you think.\n \n\n\n\n\n\nExample fitting of the Pauly et al. (1992) seasonal cessation growth model to Bonito length-at-age data.\n\n\n\n\n \n\n\n\n\n\n\n\n\nReferences\n\nOgle, D. H. 2017. An algorithm for the von Bertalanffy seasonal cessation in growth function of Pauly et al. (1992). Fisheries Research 185:1–5.\n\n\nPauly, D., M. Soriano-Bartz, J. Moreau, and A. Jarre-Teichmann. 1992. A new model accounting for seasonal cessation of growth in fishes. Australian Journal of Marine and Freshwater Research 43:1151–1156.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2016,\n  author = {H. Ogle, Derek},\n  title = {Pauly *Et Al.* (1992) {Seasonal} {Cessation} {Growth}\n    {Model}},\n  date = {2016-11-11},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2016-11-11_Pauly-VBModel},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2016, November 11. Pauly *et al.* (1992) Seasonal Cessation\nGrowth Model. https://fishr-core-team.github.io/fishR//blog/posts/2016-11-11_Pauly-VBModel."
  },
  {
    "objectID": "blog/posts/2015-9-5_Age_Comparison_Results/index.html",
    "href": "blog/posts/2015-9-5_Age_Comparison_Results/index.html",
    "title": "Age Comparison Results for Individual Fish",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)           # for WhitefishLC, agePrecision(), headtail()\nlibrary(dplyr)         # for arrange(), filter()\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\n \n\nIntroduction\nAn FSA user asked if there was a way to work with the age comparison results for individual fish. In particular, this user wanted to identify each fish for which the individual CV values from comparing two independent age estimates was greater than 10. This is fairly easy to do with an understanding of the results returned from agePrecision(). Below is an example using the WhitefishLC data found in the FSA package.1\n1 These data are automatically loaded with library(FSA).2 headtail() from FSA shows the first and last three rows of a data frame. It is used here primarily to save space.These data contain, among other things, ages assigned by two readers examining otoliths in (otolith1 and otolith2).2\n\nheadtail(WhitefishLC)\n\n#R|      fishID  tl scale1 scale2 scaleC finray1 finray2 finrayC otolith1 otolith2\n#R|  1        1 345      3      3      3       3       3       3        3        3\n#R|  2        2 334      4      3      4       3       3       3        3        3\n#R|  3        3 348      7      5      6       3       3       3        3        3\n#R|  149    214 599     14     16     15      14      14      14       16       16\n#R|  150    216 636      8     10      9      10       8      10       15       15\n#R|  151    218 551      5      7      6       6       9       8       13       13\n#R|      otolithC\n#R|  1          3\n#R|  2          3\n#R|  3          3\n#R|  149       16\n#R|  150       15\n#R|  151       13\n\n\n \n\n\nManipulate agePrecision() object\nA variety of computations on these paired age estimates are performed with agePrecision().3 The structure of the saved object shows several items returned in a list.\n3 See the agePrecision() documentation for full details.\nap1 &lt;- agePrecision(~otolith1+otolith2,data=WhitefishLC)\nstr(ap1)\n\n#R|  List of 14\n#R|   $ detail   :'data.frame': 151 obs. of  12 variables:\n#R|    ..$ otolith1: int [1:151] 3 3 3 3 3 6 9 11 3 1 ...\n#R|    ..$ otolith2: int [1:151] 3 3 3 3 3 5 10 12 4 1 ...\n#R|    ..$ mean    : num [1:151] 3 3 3 3 3 5.5 9.5 11.5 3.5 1 ...\n#R|    ..$ median  : num [1:151] 3 3 3 3 3 5.5 9.5 11.5 3.5 1 ...\n#R|    ..$ mode    : int [1:151] 3 3 3 3 3 NA NA NA NA 1 ...\n#R|    ..$ SD      : num [1:151] 0 0 0 0 0 ...\n#R|    ..$ CV      : num [1:151] 0 0 0 0 0 ...\n#R|    ..$ CV2     : num [1:151] 0 0 0 0 0 ...\n#R|    ..$ AD      : num [1:151] 0 0 0 0 0 0.5 0.5 0.5 0.5 0 ...\n#R|    ..$ PE      : num [1:151] 0 0 0 0 0 ...\n#R|    ..$ PE2     : num [1:151] 0 0 0 0 0 ...\n#R|    ..$ D       : num [1:151] 0 0 0 0 0 ...\n#R|   $ rawdiff  : 'table' int [1:5(1d)] 3 27 94 21 6\n#R|    ..- attr(*, \"dimnames\")=List of 1\n#R|    .. ..$ : chr [1:5] \"-2\" \"-1\" \"0\" \"1\" ...\n#R|   $ absdiff  : 'table' int [1:3(1d)] 94 48 9\n#R|    ..- attr(*, \"dimnames\")=List of 1\n#R|    .. ..$ : chr [1:3] \"0\" \"1\" \"2\"\n#R|   $ ASD      : num 0.309\n#R|   $ ACV      : num 4.72\n#R|   $ ACV2     : num 4.72\n#R|   $ AAD      : num 0.219\n#R|   $ APE      : num 3.34\n#R|   $ APE2     : num 3.34\n#R|   $ AD       : num 3.34\n#R|   $ PercAgree: num 62.3\n#R|   $ R        : int 2\n#R|   $ n        : int 151\n#R|   $ validn   : int 151\n#R|   - attr(*, \"class\")= chr \"agePrec\"\n\n\nFor this problem, the user wants to focus on the intermediate calculations for each individual fish, which, according to the the agePrecision() documentation, is in detail object.\n\nheadtail(ap1$detail)\n\n#R|      otolith1 otolith2 mean median mode SD CV CV2 AD PE PE2 D\n#R|  1          3        3    3      3    3  0  0   0  0  0   0 0\n#R|  2          3        3    3      3    3  0  0   0  0  0   0 0\n#R|  3          3        3    3      3    3  0  0   0  0  0   0 0\n#R|  149       16       16   16     16   16  0  0   0  0  0   0 0\n#R|  150       15       15   15     15   15  0  0   0  0  0   0 0\n#R|  151       13       13   13     13   13  0  0   0  0  0   0 0\n\n\nThe detail object is a data frame that can then be treated like any other data frame. For example, it can be sorted by ascending order of the CV value or filtered to find all fish that had a CV greater than 10.4\n4 Here I usearrange() from dplyr to sort and filter() from dplyr to filter.\n## Sort by CV\ntmp &lt;- ap1$detail |&gt;\n  arrange(CV)\nheadtail(tmp)\n\n#R|      otolith1 otolith2 mean median mode        SD       CV      CV2  AD PE PE2\n#R|  1          3        3  3.0    3.0    3 0.0000000  0.00000  0.00000 0.0  0   0\n#R|  2          3        3  3.0    3.0    3 0.0000000  0.00000  0.00000 0.0  0   0\n#R|  3          3        3  3.0    3.0    3 0.0000000  0.00000  0.00000 0.0  0   0\n#R|  149        3        2  2.5    2.5   NA 0.7071068 28.28427 28.28427 0.5 20  20\n#R|  150        3        2  2.5    2.5   NA 0.7071068 28.28427 28.28427 0.5 20  20\n#R|  151        6        4  5.0    5.0   NA 1.4142136 28.28427 28.28427 1.0 20  20\n#R|       D\n#R|  1    0\n#R|  2    0\n#R|  3    0\n#R|  149 20\n#R|  150 20\n#R|  151 20\n\n## Filter by CV\ntmp2 &lt;- ap1$detail |&gt;\n  filter(CV&gt;10)\nheadtail(tmp2)\n\n#R|     otolith1 otolith2 mean median mode        SD       CV      CV2  AD        PE\n#R|  1         6        5  5.5    5.5   NA 0.7071068 12.85649 12.85649 0.5  9.090909\n#R|  2         3        4  3.5    3.5   NA 0.7071068 20.20305 20.20305 0.5 14.285714\n#R|  3         2        3  2.5    2.5   NA 0.7071068 28.28427 28.28427 0.5 20.000000\n#R|  28        9       11 10.0   10.0   NA 1.4142136 14.14214 14.14214 1.0 10.000000\n#R|  29        9        7  8.0    8.0   NA 1.4142136 17.67767 17.67767 1.0 12.500000\n#R|  30        5        4  4.5    4.5   NA 0.7071068 15.71348 15.71348 0.5 11.111111\n#R|           PE2         D\n#R|  1   9.090909  9.090909\n#R|  2  14.285714 14.285714\n#R|  3  20.000000 20.000000\n#R|  28 10.000000 10.000000\n#R|  29 12.500000 12.500000\n#R|  30 11.111111 11.111111\n\n\nThe distribution of CV values can also be examined (Figure 1).\n\ncvdist &lt;- ggplot(data=ap1$detail,aes(x=CV)) +\n  geom_histogram(binwidth=2.5,boundary=0,color=\"black\",fill=\"gray50\") +\n  scale_x_continuous(name=\"Coefficient of Variation\") +\n  scale_y_continuous(name=\"Frequency\",expand=expansion(mult=c(0,0.05))) +\n  geom_vline(xintercept=10,color=\"red\",linewidth=1,linetype=\"dashed\")\ncvdist\n\n\n\n\n\n\n\nFigure 1: Distribution of CV values for comparisons of readings of Lake Whitefish otoliths by two readers.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2015,\n  author = {H. Ogle, Derek},\n  title = {Age {Comparison} {Results} for {Individual} {Fish}},\n  date = {2015-09-05},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2015-9-5_Age_Comparison_Results},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2015, September 5. Age Comparison Results for Individual\nFish. https://fishr-core-team.github.io/fishR//blog/posts/2015-9-5_Age_Comparison_Results."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "R for Fisheries Analyses and Tasks",
    "section": "",
    "text": "See r-bloggers where these and other R-related blog posts are aggregated.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Published - Oldest\n        \n         \n          Published - Newest\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nFSA Analyses for Multiple Groups\n\n\n11 min\n\n\nDemonstrate new (or improved) functionality in the FSA package for efficiently performing some analyses across multiple groups.\n\n\n\nDerek H. Ogle\n\n\nJan 8, 2025\n\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge-Length Key Plots\n\n\n13 min\n\n\nPlots similar to alkPlot() from FSA using ggplot2.\n\n\n\nDerek H. Ogle\n\n\nJan 5, 2025\n\n\n\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian LVB II - rstan\n\n\n16 min\n\n\nTutorial on how to estimate parameters of the LVB model with Bayesian inference and rstan\n\n\n\nJason Doll\n\n\nFeb 6, 2024\n\n\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian LVB I - brms\n\n\n9 min\n\n\nTutorial on how to estimate parameters of the LVB model with Bayesian inference and brms\n\n\n\nJason Doll\n\n\nFeb 5, 2024\n\n\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Multiple Catch Curves\n\n\n14 min\n\n\nComputing mortality from catch curves for multiple groups.\n\n\n\nDerek H. Ogle\n\n\nApr 24, 2023\n\n\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Multiple Age-Length Keys\n\n\n11 min\n\n\nComputing and applying age-length keys at once for multiple groups.\n\n\n\nDerek H. Ogle\n\n\nApr 23, 2023\n\n\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMurphy et al. (2021) Model Comparison Plots\n\n\n36 min\n\n\nUsing ggplot2 to recreate the model comparison plots of Murphy et al. (2021).\n\n\n\nDerek H. Ogle\n\n\nApr 11, 2023\n\n\n\n\n\nApr 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nO’Malley et al. (2021) Regression Plots\n\n\n13 min\n\n\nUsing ggplot2 to recreate the regresion plots of O’Malley et al. (2021).\n\n\n\nDerek H. Ogle\n\n\nApr 6, 2023\n\n\n\n\n\nApr 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVasquez et al. (2022) Kuskal-Wallis Results\n\n\n9 min\n\n\nUsing ggplot2 to display results from Kruskal-Wallis test.\n\n\n\nDerek H. Ogle\n\n\nApr 5, 2023\n\n\n\n\n\nApr 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Does Size Work in ggplot2\n\n\n19 min\n\n\nDemonstration of size= and linewidth= in ggplot2 objects.\n\n\n\nDerek H. Ogle\n\n\nApr 3, 2023\n\n\n\n\n\nApr 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiller et al. (2022) Catch Curve Plot\n\n\n8 min\n\n\nUsing ggplot2 to recreate the catch curve plot in Miller et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nApr 1, 2023\n\n\n\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiller et al. (2022) Size Plots\n\n\n12 min\n\n\nUsing ggplot2 to recreate the length frequency histogram and weight-length relationships plots in Miller et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 31, 2023\n\n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested X-Axis Labels\n\n\n20 min\n\n\nCreating nested labels for the x-axis in ggplot2.\n\n\n\nDerek H. Ogle\n\n\nMar 30, 2023\n\n\n\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiller et al. (2022) CPE Plot\n\n\n10 min\n\n\nUsing ggplot2 to recreate the CPE times series plot for two reservoirs in Miller et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 29, 2023\n\n\n\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcCarrick et al. (2022) Back-Calculated TL Plots\n\n\n11 min\n\n\nUsing ggplot2 to recreate the boxplot and bar chart of back-calculated lengths at age in McCarrick et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 28, 2023\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcCarrick et al. (2022) Age Composition Plot\n\n\n11 min\n\n\nUsing ggplot2 to recreate the age composition by year figure in McCarrick et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 27, 2023\n\n\n\n\n\nMar 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcCarrick et al. (2022) Wr Plot\n\n\n8 min\n\n\nUsing ggplot2 to recreate the relative weight (wr) by year figure in McCarrick et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 26, 2023\n\n\n\n\n\nMar 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcCarrick et al. (2022) PSD Plot\n\n\n12 min\n\n\nUsing ggplot2 to recreate the PSD by year figure in McCarrick et al. (2022). Extended it to include CIs.\n\n\n\nDerek H. Ogle\n\n\nMar 25, 2023\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcCarrick et al. (2022) CPE Plot\n\n\n15 min\n\n\nUsing ggplot2 to recreate the cpe by species, year, and length category figure in McCarrick et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 22, 2023\n\n\n\n\n\nMar 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRook et al. (2022) Estimated Cisco Stocking Densities\n\n\n14 min\n\n\nUsing ggplot2 to recreate the figure of estimated Cisco stocking densities in Rook et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 20, 2023\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRook et al. (2022) Cisco Harvest Figure\n\n\n14 min\n\n\nUsing ggplot2 to recreate the figure of Great Lakes Coregonid harvest in Rook et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 17, 2023\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAxis Magic\n\n\n25 min\n\n\nSome tricks with respect to axes in ggplot2.\n\n\n\nDerek H. Ogle\n\n\nMar 15, 2023\n\n\n\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing vjust= and hjust=\n\n\n11 min\n\n\nExplaining the use of vjust= and hjust= to position objects in ggplots.\n\n\n\nDerek H. Ogle\n\n\nMar 10, 2023\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nClemens (2022) Temperature Figure\n\n\n10 min\n\n\nUsing ggplot2 to recreate the temperature figure in Clemens (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 9, 2023\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMack and Cheatwood (2022) Cumulative Sums Figure\n\n\n8 min\n\n\nUsing ggplot2 to recreate the cumulative sums figures in Mack and Cheatwood (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 8, 2023\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nLandry et al. (2022) Logistic Regression Figures\n\n\n15 min\n\n\nUsing ggplot2 to recreate the logistic regression figures in Landry et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nMar 6, 2023\n\n\n\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUlaski et al. (2022) Growth-Maturity Figure\n\n\n17 min\n\n\nUsing ggplot2 to recreate the growth and maturity figure in Ulaski et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nFeb 16, 2023\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVaisvil et al. (2022) Hatching Date Figure\n\n\n10 min\n\n\nUsing ggplot2 to recreate the bass hatching date figure in Vaisvil et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nFeb 15, 2023\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuist et al. (2022) Back-Calculation Figure\n\n\n7 min\n\n\nUsing ggplot2 to recreate the back-calculated length-at-age figure in Quist et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nFeb 14, 2023\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuist et al. (2022) Age Comparison Figures\n\n\n14 min\n\n\nUsing ggplot2 to recreate the age comparison figures in Quist et al. (2022).\n\n\n\nDerek H. Ogle\n\n\nFeb 13, 2023\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAFS Style in ggplot2 Figures\n\n\n17 min\n\n\nSuggestions for the Glassic et al. (2019) process for modifying ggplot2 graphs to match the AFS Style Guide.\n\n\n\nDerek H. Ogle\n\n\nDec 22, 2022\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplace residPlot() with ggplot\n\n\n9 min\n\n\nUsing ggplot() as an alternative to residPlot() which was removed from FSA.\n\n\n\nDerek H. Ogle\n\n\nJun 1, 2021\n\n\n\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplace filterD()\n\n\n2 min\n\n\nAn alternative to filterD() which was removed from FSA.\n\n\n\nDerek H. Ogle\n\n\nMay 26, 2021\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplace fitPlot() with ggplot\n\n\n13 min\n\n\nUsing ggplot() as an alternative to fitPlot() which was removed from FSA.\n\n\n\nDerek H. Ogle\n\n\nMay 25, 2021\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplace compIntercepts() with emmeans()\n\n\n6 min\n\n\nUsing emmeans() from emmeans as an alternative to compIntercepts() which was removed from FSA.\n\n\n\nDerek H. Ogle\n\n\nMay 12, 2021\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplace compSlopes() with emtrends()\n\n\n6 min\n\n\nUsing emtrends() from emmeans as an alternative to compSlopes() which was removed from FSA.\n\n\n\nDerek H. Ogle\n\n\nMay 11, 2021\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge Bias Plots in ggplot2\n\n\n9 min\n\n\nFlexibly construct age bias and age difference plots using ggplot2\n\n\n\nMichael Lant\n\n\nMar 15, 2021\n\n\n\n\n\nDec 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nvon Bertalanffy Growth Plots I\n\n\n11 min\n\n\nHow to plot one von Bertalanffy growth model with data.\n\n\n\nDerek H. Ogle\n\n\nDec 31, 2019\n\n\n\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nLength Frequency Histograms in ggplot2\n\n\n6 min\n\n\nFlexibly construct length frequency histograms using ggplot2\n\n\n\nDerek H. Ogle\n\n\nDec 28, 2019\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nvon Bertalanffy Growth Plots II\n\n\n20 min\n\n\nHow to plot multiple von Bertalanffy growth models with data.\n\n\n\nDerek H. Ogle\n\n\nJan 2, 2019\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollapsing Categories or Values\n\n\n8 min\n\n\nHow to collapse categories or values into other categories\n\n\n\nDerek H. Ogle\n\n\nMar 30, 2018\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nStock-Recruitment Graphing Questions\n\n\n8 min\n\n\nA reply to a user’s questions with respect to stock-recruitment function graphics.\n\n\n\nDerek H. Ogle\n\n\nDec 12, 2017\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlots of Back-Calculated Lengths-At-Age\n\n\n9 min\n\n\nInitial thoughts on displaying back-calculated lengths-at-age.\n\n\n\nDerek H. Ogle\n\n\nNov 7, 2017\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean Weights at Ages From Lengths\n\n\n16 min\n\n\nDemonstrates concerns and corrections with estimating mean weight from observed lengths and a weight-length relationship.\n\n\n\nDerek H. Ogle\n\n\nAug 9, 2017\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRidgeline Length Frequency Plots\n\n\n3 min\n\n\nHow to visualize multiple length frequencies as ridgline plots.\n\n\n\nDerek H. Ogle\n\n\nJul 28, 2017\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nBland-Altman Plot for Age Comparisons?\n\n\n11 min\n\n\nExplore the use of Bland-Altman plots for comparing paired age estimates.\n\n\n\nDerek H. Ogle\n\n\nApr 20, 2017\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputing SE for PSD indices\n\n\n2 min\n\n\nDemonstrate how to compute SE for PSD indices.\n\n\n\nDerek H. Ogle\n\n\nDec 9, 2016\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPauly et al. (1992) Seasonal Cessation Growth Model\n\n\n3 min\n\n\nAnnouncement of an algorithm to fit the Pauly et al. (1992) seasonal cessation growth model\n\n\n\nDerek H. Ogle\n\n\nNov 11, 2016\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo New Cumulative Sum Functions\n\n\n2 min\n\n\nIntroducing the missing current and reverse cumulative sum functions.\n\n\n\nDerek H. Ogle\n\n\nSep 6, 2015\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge Comparison Results for Individual Fish\n\n\n2 min\n\n\nDemonstrates how to extract age comparison results for individual fish from the object returned by ageComparisons().\n\n\n\nDerek H. Ogle\n\n\nSep 5, 2015\n\n\n\n\n\nDec 7, 2022\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2015-9-6_Cumulative_Sums/index.html",
    "href": "blog/posts/2015-9-6_Cumulative_Sums/index.html",
    "title": "Two New Cumulative Sum Functions",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below.\n\n\n\nlibrary(FSA)   ## for pcumsum(), rcumsum()\n\n \n\nIntroduction\nIn fisheries analyses it is fairly common to compute the cumulative sum of values in a vector – i.e., all values before and including the current position in the vector. For example, the third value in the cumulative sum would be the sum of the first, second, and third values in the original vector. These types of cumulative sums are easily accomplished with cumsum() in base R.\n\nvec &lt;- 1:10\n( cum &lt;- cumsum(vec) )\n\n#R|   [1]  1  3  6 10 15 21 28 36 45 55\n\ncum[3]\n\n#R|  [1] 6\n\n\nSome applications in fisheries science (e.g., depletion estimators) require the cumulative sum NOT including the current value in the vector. For example, the third value in this case would be the sum of the first and second values in the original vector. These values may be computed by subtracting the original vector from the vector returned by cumsum().\n\ncum-vec\n\n#R|   [1]  0  1  3  6 10 15 21 28 36 45\n\n\nIn still other applications (e.g., proportional size distribution calculations) a cumulative sum from the RIGHT rather than the left is required. For example, the third value in this case would be the sum of the third, fourth, fifth, …, last values in the original vector. These values may be computed by reversing the order of the result from cumsum() that had been applied to the reverse order of the original vector.\n\nrev(cumsum(rev(vec)))\n\n#R|   [1] 55 54 52 49 45 40 34 27 19 10\n\n\n \n\n\nFSA Functions\nFor efficiency, these simple processes has been coded in pcumsum() and rcumsum() in FSA.\n\n## cumsum without the current value\n( pcum &lt;- pcumsum(vec) )\n\n#R|   [1]  0  1  3  6 10 15 21 28 36 45\n\npcum[3]\n\n#R|  [1] 3\n\n## \"reverse\" (from the right) cumsum\n( rcum &lt;- rcumsum(vec) )\n\n#R|   [1] 55 54 52 49 45 40 34 27 19 10\n\nrcum[3]\n\n#R|  [1] 52\n\n\nThe three types of cumulative sums are shown, along with the original vector, in the matrix below.\n\ncbind(vec,cum,pcum,rcum)\n\n#R|        vec cum pcum rcum\n#R|   [1,]   1   1    0   55\n#R|   [2,]   2   3    1   54\n#R|   [3,]   3   6    3   52\n#R|   [4,]   4  10    6   49\n#R|   [5,]   5  15   10   45\n#R|   [6,]   6  21   15   40\n#R|   [7,]   7  28   21   34\n#R|   [8,]   8  36   28   27\n#R|   [9,]   9  45   36   19\n#R|  [10,]  10  55   45   10\n\n\nThese two new functions are unlikely to change the world as we know it; however, I wanted to document them in this blog so that others could find them if needed.1\n1 The function documentation is available here. \n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2015,\n  author = {H. Ogle, Derek},\n  title = {Two {New} {Cumulative} {Sum} {Functions}},\n  date = {2015-09-06},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2015-9-6_Cumulative_Sums},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2015, September 6. Two New Cumulative Sum Functions. https://fishr-core-team.github.io/fishR//blog/posts/2015-9-6_Cumulative_Sums."
  },
  {
    "objectID": "blog/posts/2016-12-9_PSD_SE/index.html",
    "href": "blog/posts/2016-12-9_PSD_SE/index.html",
    "title": "Computing SE for PSD indices",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below.\n\n\n\nlibrary(FSA)   ## for psdAdd(), rcumsum(), headtail() \nlibrary(dplyr) ## for mutate(), filter()\n\n \nA reader of Ogle (2016) asked how to compute standard errors (SE) for the various PSD indices with the usual SE equation of a proportion (i.e., \\(\\sqrt{\\frac{p(1-p)}{n}}\\). I demonstrate this calculation in this post\n\n\n\n\n\n\nWarning\n\n\n\nI caution against using these standard errors to produce confidence intervals for PSD-X values. If confidence intervals are the end product for this analysis, then I suggest using the binomial or multinomial distribution methods described in Ogle (2016).\n\n\n \nI will use the same Inch Lake data from the Size Structure chapter of Ogle (2016), where I used psdAdd() (from FSA) to add a variable with the Gabelhouse length categories and eliminated all sub-stock size fish.1\n1 this and next step is from this script) from Ogle (2016).\ninchAlls &lt;- read.csv(\"http://derekogle.com/IFAR/scripts/InchLake1113.csv\") |&gt;\n  mutate(gcat=psdAdd(tl,species)) |&gt;\n  filter(gcat!=\"substock\") |&gt;\n  droplevels()\n\n#R|  No known Gabelhouse (PSD) lengths for: Iowa Darter\n\nheadtail(inchAlls)\n\n#R|       gearType year       species  tl      gcat\n#R|  1        fyke 2012 Black Crappie 246   quality\n#R|  2        fyke 2012 Black Crappie 165     stock\n#R|  3        fyke 2012 Black Crappie 330 memorable\n#R|  1270     fyke 2013  Yellow Perch 348 memorable\n#R|  1271     fyke 2013  Yellow Perch 135     stock\n#R|  1272     fyke 2013  Yellow Perch 137     stock\n\n\nI then computed the PSD-X values as shown in Ogle (2016). These will be the proportions (i.e., \\(p\\)) in the SE calculations.\n\n( freq &lt;- xtabs(~species+gcat,data=inchAlls) )\n\n#R|                   gcat\n#R|  species           stock quality preferred memorable\n#R|    Black Crappie      33      40         8        81\n#R|    Bluegill          571     199       223        43\n#R|    Largemouth Bass    26      36         0         0\n#R|    Pumpkinseed         5       2         0         0\n#R|    Yellow Perch        4       0         0         1\n\niPSDs &lt;- prop.table(freq,margin=1)*100\n( PSDs &lt;- t(apply(iPSDs,MARGIN=1,FUN=rcumsum)) )\n\n#R|                   gcat\n#R|  species           stock  quality preferred memorable\n#R|    Black Crappie     100 79.62963  54.93827 50.000000\n#R|    Bluegill          100 44.88417  25.67568  4.150579\n#R|    Largemouth Bass   100 58.06452   0.00000  0.000000\n#R|    Pumpkinseed       100 28.57143   0.00000  0.000000\n#R|    Yellow Perch      100 20.00000  20.00000 20.000000\n\n\nThe row sums from the freq table are the total number of fish (that are stock size or greater) and will be \\(n\\) in the SE calculations.\n\n( sums &lt;- rowSums(freq) )\n\n#R|    Black Crappie        Bluegill Largemouth Bass     Pumpkinseed    Yellow Perch \n#R|              162            1036              62               7               5\n\n\nFinally, with these two results the SE for PSD-Q may be computed.\n\np &lt;- PSDs[,\"quality\"]\nSEs &lt;- sqrt(p*(100-p)/sums)\nround(SEs,1)\n\n#R|    Black Crappie        Bluegill Largemouth Bass     Pumpkinseed    Yellow Perch \n#R|              3.2             1.5             6.3            17.1            17.9\n\n\nAnd can be repeat for other size indices; e.g., for PSD-P.\n\np &lt;- PSDs[,\"preferred\"]\nSEs &lt;- sqrt(p*(100-p)/sums)\nround(SEs,1)\n\n#R|    Black Crappie        Bluegill Largemouth Bass     Pumpkinseed    Yellow Perch \n#R|              3.9             1.4             0.0             0.0            17.9\n\n\n \n\n\n\n\n\n\nNote\n\n\n\nThe SE formula is usually used for proportions (and then multiplied by 100 to get a SE for percentages). However, it can be shown algebraically to apply to percentages as well.\n\n\n \n\n\n\n\n\n\n\n\nReferences\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2016,\n  author = {H. Ogle, Derek},\n  title = {Computing {SE} for {PSD} Indices},\n  date = {2016-12-09},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2016-12-9_PSD_SE},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2016, December 9. Computing SE for PSD indices. https://fishr-core-team.github.io/fishR//blog/posts/2016-12-9_PSD_SE."
  },
  {
    "objectID": "blog/posts/2017-12-12_StockRecruit_Graph_Questions/index.html",
    "href": "blog/posts/2017-12-12_StockRecruit_Graph_Questions/index.html",
    "title": "Stock-Recruitment Graphing Questions",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)          # for headtail(), srStarts(), srFuns()\nlibrary(dplyr)        # for filter(), mutate(), select()\nlibrary(nlstools)     # for nlsBoot()\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\n \n\nIntroduction\nA fishR user recently asked me\n\nIn the book that you published, I frequently use the stock-recruit curve code. The interface that shows both the Ricker/Beverton-Holt figure with the recruit per spawner to spawner figure (i.e., the dynamic plot for srStarts()) has not been working for quite some time. Additionally, I can get the recruits versus spawner plot for the Beverton-Holt or Ricker curve with confidence bounds around the curve, but how do you do the same for the recruit per spawner to spawner curve?\n\nIn this post I will answer the first question and provide a possible answer to the second questions. I will use the PSalmonAK data used in the book (Ogle 2016) and available in FSAdata.1 As in the book, I remove all records with missing stock (escapement) or recruit (return) data, rescale both the escapement and return variables to be 1000s of 1000s of fish (so millions of fish), create a log of returns variable (logret), create “recruits per spawner” (retperesc) and log recruits per spawner (logretperesc) variables, and, for convenience, remove the harvest and SST variables.\n1 Also documented here.\ndata(PSalmonAK,package=\"FSAdata\")\npinks &lt;- PSalmonAK |&gt;\n  filter(!is.na(escapement),!is.na(return)) |&gt;\n  mutate(escapement=escapement/1000,return=return/1000,\n         logret=log(return),\n         retperesc=return/escapement,logretperesc=log(retperesc)) |&gt;\n  select(-harvest,-SST)\nheadtail(pinks)\n\n#R|     year escapement return   logret retperesc logretperesc\n#R|  1  1960      1.418  2.446 0.894454  1.724965    0.5452066\n#R|  2  1961      2.835 14.934 2.703640  5.267725    1.6615986\n#R|  3  1962      1.957 10.031 2.305680  5.125703    1.6342676\n#R|  28 1987      4.289 18.215 2.902245  4.246911    1.4461918\n#R|  29 1988      2.892  9.461 2.247178  3.271438    1.1852298\n#R|  30 1989      4.577 23.359 3.150982  5.103561    1.6299386\n\n\n \n\n\nDynamic Plot Issue\nSince Ogle (2016) was published the dynamicPlot= argument was removed from srStarts() in FSA because the code for that argument relied on the tcltk package, which I found difficult to reliably support. A similar, though more manual, approach is accomplished with the new fixed= and plot= arguments. For example, using plot=TRUE (without fixed=) generates a plot of “recruits” versus “stock” with the chosen stock-recruitment model evaluated at the automatically chosen parameter starting values superimposed.\n\nsvR &lt;- srStarts(return~escapement,data=pinks,type=\"Ricker\",plot=TRUE)\n\n\n\n\n\n\n\nFigure 1: Recruitment versus stock plot with the Ricker function evaluated at the automatically chosen starting values superimposed.\n\n\n\n\n\n \nThe user, however, can show the stock-recruitment model evaluated at manually chosen parameter starting values by including those starting values in a named list() supplied to fixed=. These values can be iteratively changed in subsequent calls to srStarts() to manually find starting values that provide a model that reasonably fits (by eye) the stock-recruit data.\n\nsvR &lt;- srStarts(return~escapement,data=pinks,type=\"Ricker\",plot=TRUE,\n                fixed=list(a=4,b=0.15))\n\n\n\n\n\n\n\nFigure 2: Recruitment versus stock plot with the Ricker function evaluated at the manually chosen starting values superimposed.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nsrStarts() no longer, however, supports the simultaneously plotting of spawners versus recruits and recruits per spawner versus recruits.\n\n\n \n\n\nPlot of Recruits per Spawner versus Spawners\nThe first way that I imagined plotting recruits per spawners versus spawners with the fitted curve and confidence bands is to first follow the code for fitting the stock-recruit function to the stock and recruit data as described in Ogle (2016). In this case, the stock-recruit function is fit on the log scale to adjust for a multiplicative error structure (as described in the book).2\n2 The manually selected starting values from above are used here.\nrckr &lt;- srFuns(\"Ricker\")\nsrR &lt;- nls(logret~log(rckr(escapement,a,b)),data=pinks,start=svR)\nbootR &lt;- nlsBoot(srR)\ncbind(estimates=coef(srR),confint(bootR))\n\n#R|     estimates     95% LCI   95% UCI\n#R|  a 2.84924199  1.70935506 4.6711392\n#R|  b 0.05516673 -0.08391462 0.1994054\n\n\nOgle (2016) showed how to plot spawners versus recruits using base graphics. Here however I will use ggplot2. Either method first requires (i) constructing a sequence of “x” values that span the range of observed numbers of spawners,3 (ii) predicting the number of recruits at each spawner value using the best-fit stock-recruitment model, and (iii) constructing lower and upper confidence bounds for the predicted number of recruits at each spawner value with the bootstrap results. These results are assigned to a data framed called preds below.\n3 Increase the value in length.out= for a smoother curve and band.\nx &lt;- seq(0,9,length.out=199)        # many S for prediction\npR &lt;- rckr(x,a=coef(srR))           # predicted mean R\nLCI &lt;- UCI &lt;- numeric(length(x))\n\nfor(i in 1:length(x)) {             # CIs for mean R @ each S\n  tmp &lt;- apply(bootR$coefboot,MARGIN=1,FUN=rckr,S=x[i])\n  LCI[i] &lt;- quantile(tmp,0.025)\n  UCI[i] &lt;- quantile(tmp,0.975)\n}\npreds &lt;- data.frame(escapement=x,return=pR,LCI=LCI,UCI=UCI)\nheadtail(preds)\n\n#R|      escapement     return        LCI        UCI\n#R|  1   0.00000000  0.0000000 0.00000000  0.0000000\n#R|  2   0.04545455  0.1291866 0.07794713  0.2103041\n#R|  3   0.09090909  0.2577262 0.15650236  0.4166059\n#R|  197 8.90909091 15.5279219 6.50772063 34.9827333\n#R|  198 8.95454545 15.5680589 6.48296004 35.2738317\n#R|  199 9.00000000 15.6078974 6.45812729 35.5664363\n\n\n \nThe recruits versus spawners graph is then constructed in ggplot2 by adding the 95% confidence band from preds using geom_ribbon(), adding the best-fit model curve from preds using geom_line(), and adding the observed data from pinks using geom_point().4\n4 I plot the layers in this order so that the line is on top of the confidence band and the points are on top of both the line and band.\nggplot() +\n  geom_ribbon(data=preds,mapping=aes(x=escapement,ymin=LCI,ymax=UCI),\n              fill=\"gray50\",alpha=0.5) +\n  geom_line(data=preds,mapping=aes(x=escapement,y=return),linewidth=1) +\n  geom_point(data=pinks,mapping=aes(x=escapement,y=return)) +\n  scale_x_continuous(name=\"Escapement (millions)\") +\n  scale_y_continuous(name=\"Returners (millions)\")\n\n\n\n\n\n\n\nFigure 3: Returners versus escapement for Pacific Salmon with the best-fit Ricker recruitment function and bootstrapped 95% confidence band.\n\n\n\n\n\n \nThese results can be modified to plot recruits per spawner versus spawners by replacing the “recruits” in the code above with “recruits per spawner.” This is simple for the observed data as return is simply replaced with retperesc. However, the predicted number of recruits (return in preds) and the confidence bounds (LCI and UCI in preds) from above must be divided by the number of spawners (escapement in preds). The preds data frame is modified accordingly below.\n\npreds &lt;- preds |&gt;\n  mutate(retperesc=return/escapement,\n         rpeLCI=LCI/escapement,\n         rpeUCI=UCI/escapement)\n\nThe plot is then constructed with the appropriate modification of variable names and axis labels.5\n5 I used red for the confidence band here for illustrative purposes below, usually I would use a gray as in Figure 3.\nggplot() +\n  geom_ribbon(data=preds,mapping=aes(x=escapement,ymin=rpeLCI,ymax=rpeUCI),\n              fill=\"red\",alpha=0.5) +\n  geom_line(data=preds,mapping=aes(x=escapement,y=retperesc),linewidth=1) +\n  geom_point(data=pinks,mapping=aes(x=escapement,y=retperesc)) +\n  scale_x_continuous(name=\"Escapement (millions)\") +\n  scale_y_continuous(name=\"Returners/Escapement\")\n\n\n\n\n\n\n\nFigure 4: Returners per escapement versus escapement for Pacific Salmon with the best-fit Ricker recruitment function and bootstrapped 95% confidence band.\n\n\n\n\n\n \nAlternatively, the Ricker model could be reparameterized by dividing each side of the function by “spawners” such that the left-hand-side becomes “recruits per spawner.”6 This recruitment model can be put into an R function, with parameters estimated with nonlinear regression similar to above. The results below show that the parameter point estimates are identical and the bootsrapped confidence intervals are similar to what was obtained above.\n6 This is a fairly typical reparameterization of the Ricker model.\nrckr2 &lt;- function(S,a,b=NULL) {\n  if (length(a)&gt;1) { b &lt;- a[[2]]; a &lt;- a[[1]] }\n  a*exp(-b*S)\n}\nsrR2 &lt;- nls(logretperesc~log(rckr2(escapement,a,b)),data=pinks,start=svR)\nbootR2 &lt;- nlsBoot(srR2)\ncbind(estimates=coef(srR2),confint(bootR2))\n\n#R|     estimates     95% LCI   95% UCI\n#R|  a 2.84924192  1.67775104 4.9025369\n#R|  b 0.05516672 -0.08979965 0.2103898\n\n\nWith this, a second method for plotting recruits per spawner versus spawners is the same as how the main plot from the book was constructed but modified to use the results from this reparameterized function.\n\nx &lt;- seq(0,9,length.out=199)        # many S for prediction\npRperS &lt;- rckr2(x,a=coef(srR2))     # predicted mean RperS\nLCI2 &lt;- UCI2 &lt;- numeric(length(x))\n\nfor(i in 1:length(x)) {             # CIs for mean RperS @ each S\n  tmp &lt;- apply(bootR2$coefboot,MARGIN=1,FUN=rckr2,S=x[i])\n  LCI2[i] &lt;- quantile(tmp,0.025)\n  UCI2[i] &lt;- quantile(tmp,0.975)\n}\npreds2 &lt;- data.frame(escapement=x,retperesc=pRperS,rpeLCI=LCI2,rpeUCI=UCI2)\nheadtail(preds2)\n\n#R|      escapement retperesc    rpeLCI   rpeUCI\n#R|  1   0.00000000  2.849242 1.6777510 4.902537\n#R|  2   0.04545455  2.842106 1.6841248 4.846330\n#R|  3   0.09090909  2.834988 1.6905230 4.796657\n#R|  197 8.90909091  1.742930 0.6884826 4.213108\n#R|  198 8.95454545  1.738565 0.6821556 4.229946\n#R|  199 9.00000000  1.734211 0.6758867 4.246851\n\nggplot() +\n  geom_ribbon(data=preds2,mapping=aes(x=escapement,ymin=rpeLCI,ymax=rpeUCI),\n              fill=\"blue\",alpha=0.5) +\n  geom_line(data=preds2,mapping=aes(x=escapement,y=retperesc),linewidth=1) +\n  geom_point(data=pinks,mapping=aes(x=escapement,y=retperesc)) +\n  scale_x_continuous(name=\"Escapement (millions)\") +\n  scale_y_continuous(name=\"Returners/Escapement\")\n\n\n\n\n\n\n\nFigure 5: Returners per escapement versus escapement for Pacific Salmon with the best-fit Ricker recruitment function and bootstrapped 95% confidence band.\n\n\n\n\n\n \nThe two methods described above for plotting recruits per spawner versuse spawners are identical for the best-fit curve and nearly identical for the confidence bounds (slight differences likely due to the randomness inherent in bootstrapping). Thus, the two methods produce nearly the same visual.\n\nggplot() +\n  geom_ribbon(data=preds,mapping=aes(x=escapement,ymin=rpeLCI,ymax=rpeUCI),\n              fill=\"red\",alpha=0.25) +\n  geom_ribbon(data=preds2,mapping=aes(x=escapement,ymin=rpeLCI,ymax=rpeUCI),\n              fill=\"blue\",alpha=0.25) +\n  geom_line(data=preds,mapping=aes(x=escapement,y=retperesc),linewidth=1) +\n  geom_line(data=preds2,mapping=aes(x=escapement,y=retperesc),linewidth=1) +\n  geom_point(data=pinks,mapping=aes(x=escapement,y=retperesc)) +\n  scale_x_continuous(name=\"Escapement (millions)\") +\n  scale_y_continuous(name=\"Returners/Escapement\")\n\n\n\n\n\n\n\nFigure 6: Returners per escapement versus escapement for Pacific Salmon with the best-fit Ricker recruitment function and bootstrapped 95% confidence band shown for both estimation methods.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2017,\n  author = {H. Ogle, Derek},\n  title = {Stock-Recruitment {Graphing} {Questions}},\n  date = {2017-12-12},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2017-12-12_StockRecruit_Graph_Questions},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2017, December 12. Stock-Recruitment Graphing Questions. https://fishr-core-team.github.io/fishR//blog/posts/2017-12-12_StockRecruit_Graph_Questions."
  },
  {
    "objectID": "blog/posts/2017-7-28_LF_Ridgeline_Plot/index.html",
    "href": "blog/posts/2017-7-28_LF_Ridgeline_Plot/index.html",
    "title": "Ridgeline Length Frequency Plots",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)\nlibrary(dplyr)        # for filter(), mutate(), select()\nlibrary(ggplot2)\nlibrary(ggridges)     # for geom_density_ridges(), stat_density_ridges(), et al.\ntheme_set(theme_bw())\n\n \n\nLake Superior Kiyi\nWilke described ridgeline plots as “partially overlapping line plots that create the impression of a mountain range.” I would describe them as partially overlapping density plots (akin to a smoothed histogram).\nI thought that ridgeline plots might provide a nice visualization of length frequencies over time. For example, Lepak et al. (2017) examined (among other things) the lengths of Kiyi (Coregonus kiyi) captured in trawl tows in Lake Superior from 2003 to 2014. The length frequency data used in that paper is shown below (and stored in the lf object). Figure 1 is a modified version1 of the length frequency histograms we included in the paper.\n1 Stripped of code that increased fonts, changed colors, etc.\n\n#R|       year mon  tl\n#R|  1    2003 May 183\n#R|  2    2003 May 259\n#R|  3    2004 May 200\n#R|  9149 2004 Jul  77\n#R|  9150 2004 Jul  87\n#R|  9151 2004 Jul  81\n\n\n\n\n\n\n\n\n\n\nFigure 1: Histograms of the total lengths of Lake Superior Kiyi from 2003 to 2014.\n\n\n\n\n\n \nFigure 2 is a near default joyplot of the same data.\n\nggplot(lf,aes(x=tl,y=year)) +\n  geom_density_ridges() +\n  scale_x_continuous(name=\"Total Length (mm)\") +\n  scale_y_discrete(expand=expansion(mult=c(0.01,0.16))) +\n  theme(axis.title.y=element_blank())\n\n\n\n\n\n\n\nFigure 2: Ridgelinle plot of the total lengths of Lake Superior Kiyi from 2003 to 2014.\n\n\n\n\n\n \nIn my opinion, it is easier on the ridgeline plot to follow the strong year-classes that first appear in 2004 and 2010 through time and to see how fish in the strong year-classes grow and eventually merge in size with older fish. Thus, ridgeline plots look useful for displaying length (or age) data across many groups (years, locations, etc.).2\n2 Wilkeillustrates many possible modifications to the ridgeline plots including adding data points, show summary statistics, or using histograms rather than densities. \n\n\nLake Erie Walleye\nThe following code use the WalleyeErie2 data frame built-in to FSA. This provides an example with data that you can run on your own. I include some bells-and-whistles from Wilke’s demonstration.\n\n# reduce data to one location and make sure year variable is a factor\ndata(WalleyeErie2,package=\"FSAdata\")\nwe2 &lt;- WalleyeErie2 |&gt;\n  filter(loc==2) |&gt;\n  mutate(fyear=as.factor(year))\n\nggplot(we2,aes(x=tl,y=fyear,fill=0.5-abs(0.5-after_stat(ecdf)))) +\n  stat_density_ridges(geom=\"density_ridges_gradient\",calc_ecdf=TRUE,scale=1) +\n  scale_x_continuous(name=\"Total Length (mm)\") +\n  scale_y_discrete(expand=expansion(mult=c(0.01,0.05))) +\n  scale_fill_viridis_c(name=\"Tail Probability\",guide=\"none\") +\n  theme(axis.title.y=element_blank())\n\n\n\n\n\n\n\nFigure 3: Ridgelinle plot of the total lengths of Lake Erie Walleye from 2003 to 2014.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLepak, T. A., D. H. Ogle, and M. R. Vinson. 2017. Age, year-class strength variability, and partial age validation of Kiyis from Lake Superior. North American Journal of Fisheries Management 37:1151–1160.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2017,\n  author = {H. Ogle, Derek},\n  title = {Ridgeline {Length} {Frequency} {Plots}},\n  date = {2017-07-28},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2017-7-28_LF_Ridgeline_Plot},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2017, July 28. Ridgeline Length Frequency Plots. https://fishr-core-team.github.io/fishR//blog/posts/2017-7-28_LF_Ridgeline_Plot."
  },
  {
    "objectID": "blog/posts/2018-3-30_Collapsing_Categories_or_Values/index.html",
    "href": "blog/posts/2018-3-30_Collapsing_Categories_or_Values/index.html",
    "title": "Collapsing Categories or Values",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. The plyr package is also used but it is not loaded because I am only going to use one specific function from plyr (i.e., mapvalues()).]\n\n\n\nlibrary(dplyr)   # for mutate(), case_when()\nlibrary(forcats) # for fct_recode(), fct_collapse()\n\n \n\nIntroduction\nI have received a few queries recently that can be categorized as “How do I collapse a list of categories or values into a shorter list of categories or values?” For example, one user wanted to collapse species of fish into their respective families. Another user wanted to collapse years into decades. Data wrangling such as this is common in fisheries and is briefly described below.\n \n\n\nSample Data\nThe following creates a very simple sample of 250 individuals on which the species (as a short abbreviation) and year of capture were recorded. Because I am creating random example data below, I set the random number seed to make the results reproducible.\n\nset.seed(678394)  # for reproducibility of random data\nn &lt;- 250          # to allow easily changing sample size\ndat &lt;- data.frame(species=sample(c(\"BLG\",\"LMB\",\"PKS\",\"WAE\",\"YEP\",\"CRP\"),\n                                 n,replace=TRUE),\n                  year=sample(1980:2017,n,replace=TRUE))\nhead(dat)\n\n#R|    species year\n#R|  1     YEP 1996\n#R|  2     PKS 2005\n#R|  3     PKS 2013\n#R|  4     YEP 2014\n#R|  5     CRP 2016\n#R|  6     PKS 2006\n\n\n \n\n\nRecode Categories\nThis example demonstrates how to change the codes in one variable (e.g., species abbreviations) to new codes in another variable (e.g., long species names).\nBefore recoding I find it easier to create a vector that contains the original codes to convert from. For example, unique() extracts the vector of species abbreviations found in the species variable of the example data, which I then saved in short and alphabetized to make the next steps easier.\n\nshort &lt;- unique(dat$species) |&gt;\n  sort()\nshort\n\n#R|  [1] \"BLG\" \"CRP\" \"LMB\" \"PKS\" \"WAE\" \"YEP\"\n\n\nIn addition, I also create a vector of codes that these codes will be converted to. For example, the long vector below contains the long-form names for each species (in the same order as the abbreviations in short)\n\nlong &lt;- c(\"Bluegill\",\"Carp\",\"Largemouth Bass\",\n          \"Pumpkinseed\",\"Walleye\",\"Yellow Perch\")\n\nYou should “column-bind” these two vectors together to ensure that the codes align.\n\ncbind(short,long)\n\n#R|       short long             \n#R|  [1,] \"BLG\" \"Bluegill\"       \n#R|  [2,] \"CRP\" \"Carp\"           \n#R|  [3,] \"LMB\" \"Largemouth Bass\"\n#R|  [4,] \"PKS\" \"Pumpkinseed\"    \n#R|  [5,] \"WAE\" \"Walleye\"        \n#R|  [6,] \"YEP\" \"Yellow Perch\"\n\n\nThe mapvalues() function (from plyr) may be used to efficiently recode character (or factor) values.1 Because mapvalues() operates on a vector, it should be used within mutate() (from dplyr) to add a new variable with the recoded values to a data frame. Within mutate() the first argument to mapvalues() is the variable that contains the original data to be recoded. A vector of categories to code from is given in from= and a vector of new categories to code to is given in to=. For example, the combined use of mutate() and mapvalues() below demonstrates creating a new variable in the data frame with the long species names.\n1 The use of plyr:: in front of mapvalues() ensures that mapvalues() from plyr and not another package will be used and allows for not loading the entire plyr package.\ndat &lt;- dat |&gt;\n  mutate(speciesL=plyr::mapvalues(species,from=short,to=long))\nhead(dat)\n\n#R|    species year     speciesL\n#R|  1     YEP 1996 Yellow Perch\n#R|  2     PKS 2005  Pumpkinseed\n#R|  3     PKS 2013  Pumpkinseed\n#R|  4     YEP 2014 Yellow Perch\n#R|  5     CRP 2016         Carp\n#R|  6     PKS 2006  Pumpkinseed\n\n\n \n\nThis use of mapvalues() and mutate() is described in Section 2.2.7 of my book Introductory Fisheries Analyses with R.\n\n \nThe fct_recode() function (from forcats) can also be used to recode categories. Within mutate() the first argument to fct_recode() is the original factor variable. Subsequent arguments are of the form new level name equal to old level name.2 For example, the same recoding to long species name is shown below.\n2 Any levels not listed in fct_recode() will be retained with their original names.\ndat &lt;- dat |&gt;\n  mutate(speciesL2=fct_recode(species,\n                              \"Bluegill\" = \"BLG\",\n                              \"Carp\" = \"CRP\",\n                              \"Largemouth Bass\" = \"LMB\",\n                              \"Pumpkinseed\" = \"PKS\",\n                              \"Walleye\" = \"WAE\",\n                              \"Yellow Perch\" = \"YEP\"))\nhead(dat)\n\n#R|    species year     speciesL    speciesL2\n#R|  1     YEP 1996 Yellow Perch Yellow Perch\n#R|  2     PKS 2005  Pumpkinseed  Pumpkinseed\n#R|  3     PKS 2013  Pumpkinseed  Pumpkinseed\n#R|  4     YEP 2014 Yellow Perch Yellow Perch\n#R|  5     CRP 2016         Carp         Carp\n#R|  6     PKS 2006  Pumpkinseed  Pumpkinseed\n\n\n \n\n\nCollapse Categories\nIn some instances, one may want to collapse some categories into a single category (e.g., species into a family). This is easily accomplished with mapvalues() or fct_recode() by simply repeating some of the “to” categories. For example, family contains family names that correspond to the species names in the data frame. Note how multiple species have the same family name category.\n\nfam &lt;- c(\"Centrarchidae\",\"Cyprinidae\",\"Centrarchidae\",\n         \"Centrarchidae\",\"Percidae\",\"Percidae\")\ncbind(short,long,fam)\n\n#R|       short long              fam            \n#R|  [1,] \"BLG\" \"Bluegill\"        \"Centrarchidae\"\n#R|  [2,] \"CRP\" \"Carp\"            \"Cyprinidae\"   \n#R|  [3,] \"LMB\" \"Largemouth Bass\" \"Centrarchidae\"\n#R|  [4,] \"PKS\" \"Pumpkinseed\"     \"Centrarchidae\"\n#R|  [5,] \"WAE\" \"Walleye\"         \"Percidae\"     \n#R|  [6,] \"YEP\" \"Yellow Perch\"    \"Percidae\"\n\n\nThe example below shows how to convert the species name abbreviations to family names. In addition, the last use of mapvalues() shows how to change the long-form names to family names. This last example is, of course, repetitive, but it is used here to demonstrate how mutate() allows a variable that was “just created” to be immediately used.\n\ndat &lt;- dat |&gt;\n  mutate(family=plyr::mapvalues(species,from=short,to=fam),\n         family2=plyr::mapvalues(speciesL,from=long,to=fam))\nhead(dat)\n\n#R|    species year     speciesL    speciesL2        family       family2\n#R|  1     YEP 1996 Yellow Perch Yellow Perch      Percidae      Percidae\n#R|  2     PKS 2005  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|  3     PKS 2013  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|  4     YEP 2014 Yellow Perch Yellow Perch      Percidae      Percidae\n#R|  5     CRP 2016         Carp         Carp    Cyprinidae    Cyprinidae\n#R|  6     PKS 2006  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n\n\nThe “collapsing” of multiple levels into one level can also be accomplished with fct_collapse() (from forcats). The first argument to this function is again the variable containing the “old” levels. Subsequent arguments are formed by setting a new level name equal to a vector containing old level names to collapse.\n\ndat &lt;- dat |&gt;\n  mutate(family3=fct_collapse(species,\n                              \"Centarchidae\" = c(\"BLG\",\"PKS\",\"LMB\"),\n                              \"Percidae\" = c(\"WAE\",\"YEP\"),\n                              \"Cyprinidae\" = c(\"CRP\")))\nhead(dat)\n\n#R|    species year     speciesL    speciesL2        family       family2\n#R|  1     YEP 1996 Yellow Perch Yellow Perch      Percidae      Percidae\n#R|  2     PKS 2005  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|  3     PKS 2013  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|  4     YEP 2014 Yellow Perch Yellow Perch      Percidae      Percidae\n#R|  5     CRP 2016         Carp         Carp    Cyprinidae    Cyprinidae\n#R|  6     PKS 2006  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|         family3\n#R|  1     Percidae\n#R|  2 Centarchidae\n#R|  3 Centarchidae\n#R|  4     Percidae\n#R|  5   Cyprinidae\n#R|  6 Centarchidae\n\n\n \n\n\nCollapse Values into Categories\nIt is also common to categorize a numeric variable. For example, a “decade” variable is derived from the year variable in this example.\nThe case_when() function (from dplyr) may be used to efficiently collapse discrete values into categories. This function also operates on vectors and, thus, must be used with mutate() to add a variable to a data frame. The arguments to case_when() are a series of two-sided formulae where the left-side is a conditioning statement based on the original data and the right-side is the value that should appear in the new variable when that condition is TRUE. For example, the first line in case_when() below asks “if the year variable is in the values from 1980 to 1989 then the new category should be ‘1980s’.”3 For example, the code below creates a new variable called decade that identifies the decade that corresponds to the year-of-capture variable.\n3 The colon operator creates a sequence of all integers between the two numbers separated by the colon. The %in% is used on conditional statements to determine if a value is contained within a vector, returning TRUE if it is and FALSE if it is not.\ndat &lt;- dat |&gt;\n  mutate(decade=case_when(\n    year %in% 1980:1989 ~ \"1980s\",\n    year %in% 1990:1999 ~ \"1990s\",\n    year %in% 2000:2009 ~ \"2000s\",\n    year %in% 2010:2019 ~ \"2010s\"\n  ))\nhead(dat)\n\n#R|    species year     speciesL    speciesL2        family       family2\n#R|  1     YEP 1996 Yellow Perch Yellow Perch      Percidae      Percidae\n#R|  2     PKS 2005  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|  3     PKS 2013  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|  4     YEP 2014 Yellow Perch Yellow Perch      Percidae      Percidae\n#R|  5     CRP 2016         Carp         Carp    Cyprinidae    Cyprinidae\n#R|  6     PKS 2006  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|         family3 decade\n#R|  1     Percidae  1990s\n#R|  2 Centarchidae  2000s\n#R|  3 Centarchidae  2010s\n#R|  4     Percidae  2010s\n#R|  5   Cyprinidae  2010s\n#R|  6 Centarchidae  2000s\n\n\nThe lines in case_when() operate sequentially (like a series of “if” statements) such that the above operation can be more succinctly coded as below. Also note in this example that the resulting variable is numeric rather than categorical (simply as an example).\n\ndat &lt;- dat |&gt;\n  mutate(decade2=case_when(\n    year &lt;= 1989 ~ 1980,\n    year &lt;= 1999 ~ 1990,\n    year &lt;= 2009 ~ 2000,\n    year &lt;= 2019 ~ 2010,\n  ))\nhead(dat)\n\n#R|    species year     speciesL    speciesL2        family       family2\n#R|  1     YEP 1996 Yellow Perch Yellow Perch      Percidae      Percidae\n#R|  2     PKS 2005  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|  3     PKS 2013  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|  4     YEP 2014 Yellow Perch Yellow Perch      Percidae      Percidae\n#R|  5     CRP 2016         Carp         Carp    Cyprinidae    Cyprinidae\n#R|  6     PKS 2006  Pumpkinseed  Pumpkinseed Centrarchidae Centrarchidae\n#R|         family3 decade decade2\n#R|  1     Percidae  1990s    1990\n#R|  2 Centarchidae  2000s    2000\n#R|  3 Centarchidae  2010s    2010\n#R|  4     Percidae  2010s    2010\n#R|  5   Cyprinidae  2010s    2010\n#R|  6 Centarchidae  2000s    2000\n\nstr(dat)\n\n#R|  'data.frame':  250 obs. of  9 variables:\n#R|   $ species  : chr  \"YEP\" \"PKS\" \"PKS\" \"YEP\" ...\n#R|   $ year     : int  1996 2005 2013 2014 2016 2006 2002 2012 2013 2014 ...\n#R|   $ speciesL : chr  \"Yellow Perch\" \"Pumpkinseed\" \"Pumpkinseed\" \"Yellow Perch\" ...\n#R|   $ speciesL2: Factor w/ 6 levels \"Bluegill\",\"Carp\",..: 6 4 4 6 2 4 5 1 3 6 ...\n#R|   $ family   : chr  \"Percidae\" \"Centrarchidae\" \"Centrarchidae\" \"Percidae\" ...\n#R|   $ family2  : chr  \"Percidae\" \"Centrarchidae\" \"Centrarchidae\" \"Percidae\" ...\n#R|   $ family3  : Factor w/ 3 levels \"Centarchidae\",..: 3 1 1 3 2 1 3 1 1 3 ...\n#R|   $ decade   : chr  \"1990s\" \"2000s\" \"2010s\" \"2010s\" ...\n#R|   $ decade2  : num  1990 2000 2010 2010 2010 2000 2000 2010 2010 2010 ...\n\n\n \n\n\n\n\n\n\nWarning\n\n\n\nYou may be motivated from this example to use case_when() to develop a length category variable from measure lengths. While this is possible it is not efficient as you would have several conditions within case_when() (to span all measured lengths) and you would need to make sure that your conditions covered the range of measured lengths. I urge you to examine lencat() in FSA for the purpose of creating length categories (see examples here).\n\n\n \n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2018,\n  author = {H. Ogle, Derek},\n  title = {Collapsing {Categories} or {Values}},\n  date = {2018-03-30},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2018-3-30_Collapsing_Categories_or_Values},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2018, March 30. Collapsing Categories or Values. https://fishr-core-team.github.io/fishR//blog/posts/2018-3-30_Collapsing_Categories_or_Values."
  },
  {
    "objectID": "blog/posts/2019-12-31_vonB_plots_1/index.html",
    "href": "blog/posts/2019-12-31_vonB_plots_1/index.html",
    "title": "von Bertalanffy Growth Plots I",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. One function from investr is also used but the whole package is not loaded here. The data are also from FSAdata, which is not loaded below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)     # for vbFuns(), vbStarts(), headtail()\nlibrary(dplyr)   # for filter(), mutate()\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\n \n\nIntroduction\nThe most common questions that I receive through the fishR website are related to fitting a von Bertalanffy growth function (VBGF) to data and viewing the results. In this post, I briefly demonstrate how to fit a VBGF to a single group of data and then provide several options for how to view the fit of the function to those data.\nI will use lengths and ages of Lake Erie Walleye (Sander vitreus) captured during October-November, 2003-2014 available in FSAdata package. These data formed many of the examples in Ogle et al. (2017). My primary interest here is in the tl (total length in mm) and age variables1. I focus on female Walleye from location “1” captured in 2014 in this example.2\n1 See more details about the data.2 For succinctness, I removed year and sex as they only had one category after filtering and three variables related to the location of capture.\ndata(WalleyeErie2,package=\"FSAdata\")\nwf14T &lt;- WalleyeErie2 |&gt;\n  filter(year==2014,sex==\"female\",loc==1) |&gt;\n  select(-year,-sex,-setID,-loc,-grid)\nheadtail(wf14T)\n\n#R|       tl    w      mat age\n#R|  1   445  737 immature   2\n#R|  2   528 1571   mature   4\n#R|  3   380  506 immature   1\n#R|  323 488 1089 immature   2\n#R|  324 521 1408   mature   3\n#R|  325 565 1745   mature   3\n\n\n \n\n\nFitting the VBGF\nMethods for fitting a von Bertalannfy growth function (VBGF) are detailed in Ogle (2016) and Ogle et al. (2017). Thus, this methodology will only be briefly explained here.\nA function for the typical VBGF is constructed with vbFuns().3\n3 Other parameterizations of the VBGF can be used with param= in vbFuns() as described in its documentation.\n( vb &lt;- vbFuns(param=\"Typical\") )\n\n#R|  function (t, Linf, K = NULL, t0 = NULL) \n#R|  {\n#R|      if (length(Linf) == 3) {\n#R|          K &lt;- Linf[[2]]\n#R|          t0 &lt;- Linf[[3]]\n#R|          Linf &lt;- Linf[[1]]\n#R|      }\n#R|      Linf * (1 - exp(-K * (t - t0)))\n#R|  }\n#R|  &lt;bytecode: 0x000001ece829d0a8&gt;\n#R|  &lt;environment: 0x000001ece82dbdb8&gt;\n\n\nSome of the methods below use the fact that the three parameters of the typical VBGF (\\(L_{\\infty}\\), \\(K\\), \\(t_{0}\\)) can be given to this function separately (in that order) or as a vector (still in that order). For example, both lines below can be used to predict the mean length for an age-3 fish with the given VBGF parameters.4\n4 The parameters could be given in a different order but would need to be named; e.g., vb(3,t0=-0.5,K=0.3,Linf=300).\nvb(3,300,0.3,-0.5)\n\n#R|  [1] 195.0187\n\ntmp &lt;- c(300,0.3,-0.5) \nvb(3,tmp)\n\n#R|  [1] 195.0187\n\n\nReasonable starting values for the optimization algorithm may be obtained with vbStarts(), where the first argument is a formula of the form lengths~ages where lengths and ages are replaced with the actual variable names that contain the observed lengths and ages, respectively, and data= is set to the data frame that contains those variables.\n\n( sv0 &lt;- vbStarts(tl~age,data=wf14T) )\n\n#R|  $Linf\n#R|  [1] 645.2099\n#R|  \n#R|  $K\n#R|  [1] 0.3482598\n#R|  \n#R|  $t0\n#R|  [1] -1.548925\n\n\nThe nls() function is typically used to estimate parameters of the VBGF from observed data. The first argument is a formula that has lengths on the left-hand-side and the VBGF function created above on the right-hand-side. The VBGF function has the ages variable as its first argument and then Linf, K, and t0 as the remaining arguments (just as they appear here). Again, the data frame with the observed lengths and ages is given to data= and the starting values derived above are given to start=.\n\nfit0 &lt;- nls(tl~vb(age,Linf,K,t0),data=wf14T,start=sv0)\n\nThe parameter estimates and confidence intervals are extracted from the saved nls() object with coef() and confint(), respectively.5 They are column-bound together here for aesthetic reasons.\n5 This confint() requires the MASS package which is usually loaded automatically with base R.\ncbind(Est=coef(fit0),confint(fit0))\n\n#R|              Est        2.5%       97.5%\n#R|  Linf 648.208364 629.6754671 669.0341553\n#R|  K      0.361540   0.3223327   0.4043957\n#R|  t0    -1.283632  -1.4592315  -1.1207317\n\n\n \n\n\nModel Fit Using stat_function()\nHere all “layers” of the plot will use the same data; thus, data= and the aes()thetic mappings are defined in ggplot(). Observed lengths and ages are added to the plot with geom_point(). The points in Figure 1 were made slightly larger than the default (with size=) and also with a fairly low transparency value to handle considerable over-plotting. scale_y_continuous() and scale_x_continuous() control aspects of y- and x-axes, respectively – labels for axes are given in name=, minimum and maximum limits for the axis are in limits=, and specific major breaks for the axis are in breaks=.6 Finally, the fitted model line is added to the plot with stat_function() with the VBGF function created above in fun= and a list of arguments to this function in args=.7 In Figure 1 I made the model line a little wider than the default. Finally the theme() was modified to remove the minor grid lines from both axes.8\n6 seq(0,700,100) makes a vector of numbers from 0 to 700 in increments of 100 and 0:11 makes a vector of integers from 0 to 11.7 The usage here exploits the fact that all three parameters of the VBGF can be given in the first parameter argument, Linf=.8 Thus the gridlines only appear for labelled axis breaks.\nggplot(data=wf14T,aes(x=age,y=tl)) +\n  geom_point(size=2,alpha=0.1) +\n  scale_y_continuous(name=\"Total Length (mm)\",\n                     limits=c(0,700),breaks=seq(0,700,100)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  stat_function(fun=vb,args=list(Linf=coef(fit0)),linewidth=1) +\n  theme(panel.grid.minor=element_blank())\n\n\n\n\n\n\n\nFigure 1: Fit of typical von Bertalanffy growth function to female Lake Erie Walleye in 2014.\n\n\n\n\n\n \nThe model line can be displayed outside the range of observed ages by including minimum and maximum values in xlim= over which the function in fun= will be evaluated. In Figure 2 I add a dashed line for the model that includes evaluation at ages outside the observed range of ages (first stat_function()) and then plotted the model line for observed ages on top of that (second stat_function()). This gives the impression of using a dashed line only for the ages that would be extrapolated.9\n9 I would usually change the axis expansion factors here to clean this plot up a bit.\nggplot(data=wf14T,aes(x=age,y=tl)) +\n  geom_point(size=2,alpha=0.1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  stat_function(fun=vb,args=list(Linf=coef(fit0)),\n                xlim=c(-1,12),linewidth=1,linetype=\"dashed\") +\n  stat_function(fun=vb,args=list(Linf=coef(fit0)),linewidth=1) +\n  theme(panel.grid.minor=element_blank())\n\n\n\n\n\n\n\nFigure 2: Fit of typical von Bertalanffy growth function to female Lake Erie Walleye in 2014. The dashed line shows the model fit outside the range of observed ages.\n\n\n\n\n\n \n\n\nModel Fit Using geom_smooth()\ngeom_smooth() can use nls() to fit the VBGF “behind the scenes” and then add the resultant model line to the plot. For this purpose geom_smooth() requires method=\"nls\" and se=FALSE.10 In addition, arguments for fitting the VBGF required by nls() must be in a list given to methods.args=. Minimum required arguments for fitting the VBGF are the VBGF formula= and start=ing values as shown for nls() above. Figure 3 uses geom_smooth() in this way to reproduce Figure 1.\n10 se=FALSE is required because this argument is not implemented in nls().\nggplot(data=wf14T,aes(x=age,y=tl)) +\n  geom_point(size=2,alpha=0.1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  geom_smooth(method=\"nls\",se=FALSE,\n              method.args=list(formula=y~vb(x,Linf,K,t0),start=sv0),\n              color=\"black\",linewidth=1) +\n  theme(panel.grid.minor.x=element_blank())\n\n\n\n\n\n\n\nFigure 3: Fit of typical von Bertalanffy growth function to female Lake Erie Walleye in 2014.\n\n\n\n\n\n \n\n\nModel Fit from Predicted Values\nFigure 1 and Figure 2 can also be constructed from lengths predicted at a variety of ages “outside” of any ggplot() layers. I find it easier when using this method to first create a vector of ages over which the fitted model will be evaluated is then constructed. In this case the ages extend beyond the observed range of ages. The seq()uence produced here will have 101 age values between -1 and 12.11\n11 Use a larger value for length.out= to make the line produced further below more smooth.\nages &lt;- seq(-1,12,length.out=101)\n\nThe mean length at each of these ages is predicted with predict(), where the age vector just created is set equal to the name of the age variable in the nls() object inside of data.frame(). The vector of ages and predicted mean lengths are put into a data frame for plotting below.12\n12 Here the data frame is called preds and it has two variables named age and fit.\npreds &lt;- data.frame(age=ages,\n                    fit=predict(fit0,data.frame(age=ages)))\nheadtail(preds)\n\n#R|        age       fit\n#R|  1   -1.00  63.17547\n#R|  2   -0.87  90.03596\n#R|  3   -0.74 115.66322\n#R|  99  11.74 642.36300\n#R|  100 11.87 642.63138\n#R|  101 12.00 642.88743\n\n\n \nThese predicted mean lengths-at-age are then used to add a fitted model line to a plot of observed lengths-at-age with geom_line(). However, because the observed and predicted data are in different data frames, the data= and mapped aes()thetics are declared within the appropriate geoms rather than within ggplot(). For example, geom_point() is used below to add the observed data to the plot and geom_line() is used below to add the modeled line. Note below that separate geom_line()s are used to show the modeled line over extrapolated and observed ages.13 The results in Figure 4 reproduce Figure 2.\n13 Also note the use of filter() to reduce the predicted lengths-at-age to the observed ages.\nggplot() +\n  geom_point(data=wf14T,aes(x=age,y=tl),size=2,alpha=0.1) +\n  geom_line(data=preds,aes(x=age,y=fit),linewidth=1,linetype=\"dashed\") +\n  geom_line(data=filter(preds,age&gt;=0,age&lt;=11),aes(x=age,y=fit),linewidth=1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  theme(panel.grid.minor=element_blank())\n\n\n\n\n\n\n\nFigure 4: Fit of typical von Bertalanffy growth function to female Lake Erie Walleye in 2014. The dashed line shows the model fit outside the range of observed ages.\n\n\n\n\n\n \n\n\nModel Fit with Confidence Band\nThe main reason for introducing the idea of constructing a graphic from predicted values is that it allows for the opportunity to add confidence and prediction bands around the fitted model line (Figure 5).\nCreation of this plot requires modifying the data frame of predicted mean lengths at age with confidence (or prediction) intervals for the mean length at each age. As mentioned previously, constructing these intervals is not straightforward with non-linear models. However, confidence (or prediction) intervals can be estimated with Taylor series approximations as implemented in predFit() of investr.14 predFit() requires the saved nls() object as its first argument, a data frame of ages over which to make predictions as the second argument, and either interval=\"confidence\" for confidence intervals or interval=\"prediction\" for prediction intervals.\n14 Use of :: here allows predFit() from investr to be used without loading all of investr.\npreds &lt;- data.frame(age=ages,\n                    investr::predFit(fit0,data.frame(age=ages),\n                                     interval=\"confidence\"))\nheadtail(preds)\n\n#R|        age       fit       lwr       upr\n#R|  1   -1.00  63.17547  32.40761  93.94334\n#R|  2   -0.87  90.03596  62.76575 117.30618\n#R|  3   -0.74 115.66322  91.59342 139.73301\n#R|  99  11.74 642.36300 625.59243 659.13357\n#R|  100 11.87 642.63138 625.76065 659.50210\n#R|  101 12.00 642.88743 625.91982 659.85504\n\n\nA confidence band for mean lengths at age is added to the plot with geom_ribbon() where the lower part of the ribbon is at the lower confidence values (i.e., ymin=lwr) and the upper part is at the upper confidence value (i.e., ymax=upr).15 fill= gives the color of the enclosed ribbon.\n15 Add geom_ribbon() first so that it is behind the points and model lines.\nggplot() + \n  geom_ribbon(data=preds,aes(x=age,ymin=lwr,ymax=upr),fill=\"gray80\") +\n  geom_point(data=wf14T,aes(y=tl,x=age),size=2,alpha=0.1) +\n  geom_line(data=preds,aes(x=age,y=fit),linewidth=1,linetype=\"dashed\") +\n  geom_line(data=filter(preds,age&gt;=0,age&lt;=11),aes(x=age,y=fit),linewidth=1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  theme(panel.grid.minor=element_blank())\n\n\n\n\n\n\n\nFigure 5: Fit of typical von Bertalanffy growth function to female Lake Erie Walleye in 2014 with a 95% confidence band. The dashed line shows the model fit outside the range of observed ages.\n\n\n\n\n\n \n\n\nAdd Equation to Plot\nThe following function can be used to extract the model coefficients from an nls() object and place them into a “plotmath” format to be added to the ggplot graph.\n\nmakeVBEqnLabel &lt;- function(fit,digits=c(1,3,3)) {\n  # Isolate coefficients (and control decimals)\n  cfs &lt;- coef(fit)\n  Linf &lt;- formatC(cfs[[\"Linf\"]],format=\"f\",digits=digits[1])\n  K &lt;- formatC(cfs[[\"K\"]],format=\"f\",digits=digits[2])\n  # Handle t0 differently because of minus in the equation\n  t0 &lt;- cfs[[\"t0\"]]\n  sgn &lt;- ifelse(t0&lt;0,\"+\",\"-\")\n  t0 &lt;- formatC(abs(t0),format=\"f\",digits=digits[3])\n  # Put together and return\n  paste0(\"TL=='\",Linf,\"'~bgroup('[',1-e^{-'\",K,\"'~(age\",sgn,\"'\",t0,\"')},']')\")\n}\n\n \n\n\n\n\n\n\nNote\n\n\n\nThe function above was modified on 14-Jun-23 to correct an issue with plotmath dropping trailing zeroes in the rounded coefficient values. For example a K of 0.250 would be printed as 0.25 when the string returned from makeVBEqnLabel() was parsed in annotate() below. The correction follows the suggestion in this StackOverflow post.\n\n\n \nThe object returned from this function can be added to the ggplot graph with annotate() as shown below.16\n16 The x=, y=, hjust=, and vjust= arguments are used to position and justify the text and may take some trial-and-error to get what you want.\nggplot() + \n  geom_ribbon(data=preds,aes(x=age,ymin=lwr,ymax=upr),fill=\"gray80\") +\n  geom_point(data=wf14T,aes(y=tl,x=age),size=2,alpha=0.1) +\n  geom_line(data=preds,aes(x=age,y=fit),linewidth=1,linetype=\"dashed\") +\n  geom_line(data=filter(preds,age&gt;=0,age&lt;=11),aes(x=age,y=fit),linewidth=1) +\n  scale_y_continuous(name=\"Total Length (mm)\",limits=c(0,700)) +\n  scale_x_continuous(name=\"Age (years)\",breaks=0:11) +\n  theme(panel.grid.minor=element_blank()) +\n  annotate(geom=\"text\",label=makeVBEqnLabel(fit0),parse=TRUE,\n           size=4,x=Inf,y=-Inf,hjust=1.1,vjust=-0.5)\n\n\n\n\n\n\n\nFigure 6: Fit of typical von Bertalanffy growth function to female Lake Erie Walleye in 2014 with a 95% confidence band. The dashed line shows the model fit outside the range of observed ages.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe large parentheses created by bgroup in the equation will not render properly under Windows using R v4.2.2. I posted about this on StackOverlow and was told that it was a Windows-only bug in v4.2.2. It has been fixed in v4.2.3 under which this post was rendered.\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\n\nOgle, D. H., T. O. Brenden, and J. L. McCormick. 2017. Growth Estimation: Growth Models and Statistical Inference. Pages 265–359 in M. C. Quist and D. A. Isermann, editors. Age and Growth of Fishes: Principles and Techniques. American Fisheries Society, Bethesda, MD.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2019,\n  author = {H. Ogle, Derek},\n  title = {Von {Bertalanffy} {Growth} {Plots} {I}},\n  date = {2019-12-31},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2019-12-31_vonB_plots_1},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2019, December 31. von Bertalanffy Growth Plots I. https://fishr-core-team.github.io/fishR//blog/posts/2019-12-31_vonB_plots_1."
  },
  {
    "objectID": "blog/posts/2021-3-15-AgeBiasPlots/index.html",
    "href": "blog/posts/2021-3-15-AgeBiasPlots/index.html",
    "title": "Age Bias Plots in ggplot2",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. I also set the default ggplot theme to theme_bw() for a classic “black-and-white” plot (rather than the default plot with a gray background).\n\n\n\nlibrary(FSA)          # for WhitefishLC, ageBias(), plotAB()\nlibrary(dplyr)        # for mutate()\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\n \n\nIntroduction\nAge-bias and difference plots can be constructed with plot() and plotAB() in the FSA package. However, these types of plots can be more flexibly constructed using ggplot2. Below I will use ggplot2 to recreate many of the plots shown in the documentation for plot() and plotAB().\n \n\n\nData\nThe WhitefishLC data frame from FSA.1 contains age readings made by two readers on scales, fin rays, and otoliths, along with consensus readings for each structure.\n1 These data are loaded automatically with library(FSA).\nhead(WhitefishLC)\n\n#R|    fishID  tl scale1 scale2 scaleC finray1 finray2 finrayC otolith1 otolith2\n#R|  1      1 345      3      3      3       3       3       3        3        3\n#R|  2      2 334      4      3      4       3       3       3        3        3\n#R|  3      3 348      7      5      6       3       3       3        3        3\n#R|  4      4 300      4      3      4       3       2       3        3        3\n#R|  5      5 330      3      3      3       4       3       4        3        3\n#R|  6      6 316      4      4      4       2       3       3        6        5\n#R|    otolithC\n#R|  1        3\n#R|  2        3\n#R|  3        3\n#R|  4        3\n#R|  5        3\n#R|  6        6\n\n\nIntermediate and summary statistics for the comparison of paired ages (e.g., between consensus scale and otolith ages) can be extracted from the objected returned by ageBias() from FSA.2\n2 As described in the documentation.\nab1 &lt;- ageBias(scaleC~otolithC,data=WhitefishLC,\n               ref.lab=\"Otolith Age\",nref.lab=\"Scale Age\")\n\nFor example, the $data object of ab13 contains the original paired age estimates, the differences between those two estimates, and the mean of those two estimates.\n3 ab1 because that was the name assigned to the results from ageBias() in this example.\nhead(ab1$data)\n\n#R|    scaleC otolithC diff mean\n#R|  1      3        3    0  3.0\n#R|  2      4        3    1  3.5\n#R|  3      6        3    3  4.5\n#R|  4      4        3    1  3.5\n#R|  5      3        3    0  3.0\n#R|  6      4        6   -2  5.0\n\n\nIn addition, the $bias object of ab1 contains summary statistics of ages for the first structure given in the ageBias() formula by each age of the second structure given in that formula. For example, the first row below gives the number, minimum, maximum, mean, and standard error of the scale ages that were paired with an otolith age of 1. Additionally there is a t-test, adjusted p-value, and a significance statement for testing whether the mean scale age is different from the otolith age. Finally, confidence intervals (defaults to 95%) for the mean scale age at an otolith age of 1 is given, with a statement about whether a confidence interval could be calculated.4\n4 See the documentation for ageBias() for the criterion used to decide if the confidence interval can be calculated.\nhead(ab1$bias)\n\n#R|    otolithC  n min max     mean        SE          t   adj.p   sig       LCI\n#R|  1        1  9   1   2 1.444444 0.1756821  2.5298218 0.28212 FALSE 1.0393208\n#R|  2        2  7   1   5 2.000000 0.5773503  0.0000000 1.00000 FALSE 0.5872748\n#R|  3        3 17   1   6 3.352941 0.2416423  1.4605937 0.81743 FALSE 2.8406824\n#R|  4        4 18   2   6 3.833333 0.2322102 -0.7177407 1.00000 FALSE 3.3434126\n#R|  5        5  8   4   8 5.250000 0.4909902  0.5091751 1.00000 FALSE 4.0889926\n#R|  6        6 10   3   6 4.600000 0.2666667 -5.2500003 0.00686  TRUE 3.9967581\n#R|         UCI canCI\n#R|  1 1.849568  TRUE\n#R|  2 3.412725  TRUE\n#R|  3 3.865200  TRUE\n#R|  4 4.323254  TRUE\n#R|  5 6.411007  TRUE\n#R|  6 5.203242  TRUE\n\n\nThe results in $bias.diff are similar to those for $bias except that the difference in age between the two structures is summarized for each otolith age.\n\nhead(ab1$bias.diff)\n\n#R|    otolithC  n min max       mean        SE          t   adj.p   sig         LCI\n#R|  1        1  9   0   1  0.4444444 0.1756821  2.5298218 0.28212 FALSE  0.03932075\n#R|  2        2  7  -1   3  0.0000000 0.5773503  0.0000000 1.00000 FALSE -1.41272519\n#R|  3        3 17  -2   3  0.3529412 0.2416423  1.4605937 0.81743 FALSE -0.15931758\n#R|  4        4 18  -2   2 -0.1666667 0.2322102 -0.7177407 1.00000 FALSE -0.65658738\n#R|  5        5  8  -1   3  0.2500000 0.4909902  0.5091751 1.00000 FALSE -0.91100742\n#R|  6        6 10  -3   0 -1.4000000 0.2666667 -5.2500003 0.00686  TRUE -2.00324188\n#R|           UCI canCI\n#R|  1  0.8495680  TRUE\n#R|  2  1.4127252  TRUE\n#R|  3  0.8652000  TRUE\n#R|  4  0.3232540  TRUE\n#R|  5  1.4110074  TRUE\n#R|  6 -0.7967581  TRUE\n\n\nThese data frames are used in ggplot2 code below to create various versions of age-bias and difference plots.\n\n\n\n\n\n\nImportant\n\n\n\nAt times multiple data frames will be used when constructing the same plot so that layers of the plot can have different variables.\n\n\n \n\n\nBasic Age-Bias Plot\nFigure 1 is the age-bias plot created by default by plotAB() from FSA.\n\nFSA::plotAB(ab1)\n\n\n\n\n\n\n\nFigure 1: Default age-bias plot from plotAB() in FSA.\n\n\n\n\n\n \nFigure 1 is largely recreated (Figure 2) with the following ggplot2 code.\n\nggplot(data=ab1$bias) +\n  geom_abline(slope=1,intercept=0,linetype=\"dashed\",color=\"gray\") +\n  geom_errorbar(aes(x=otolithC,ymin=LCI,ymax=UCI,color=sig),width=0) +\n  geom_point(aes(x=otolithC,y=mean,color=sig,fill=sig),shape=21) +\n  scale_fill_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"white\"),guide=\"none\") +\n  scale_color_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"red3\"),guide=\"none\") +\n  scale_x_continuous(name=ab1$ref.lab,breaks=0:25) +\n  scale_y_continuous(name=ab1$nref.lab,breaks=0:25)\n\n\n\n\n\n\n\nFigure 2: Recreation of the default age-bias plot using ggplot2.\n\n\n\n\n\nThe specifics of the code above are described below.\n\nThe base data used in this plot is the $bias data.frame discussed above.\nThe 45o agreement line (i.e., slope of 1 and intercept of 0) is added with geom_abline(), using a dashed linetype= and a gray color=. This “layer” is first so that it sits behind the other results.\nError bars are added with geom_errorbar(). The aes()thetics here map the consensus otolith age to the x= axis and the lower and upper confidence interval values for the mean consensus scale age at each consensus otolith age to ymin= and ymax=. The color= of the lines are mapped to the sig variable so that points that are significantly different from the 45o agreement line will have a different color (with scale_color_manual() described below). Finally, width=0 assures that the error bars will not have “end caps.”\nPoints at the mean consensus scale age (y=) for each otolith age (x=) are added with geom_point(). Again, the color= and fill= are mapped to the sig variable so that they will appear different depending on whether the points are significantly different from the 45o agreement line or not. Finally, shape=21 represents a plotted point as an open circle that is outlined with color= and filled with fill=.\nscale_fill_manual() and scale_color_manual() are used to set the colors and fills for the levels in the sig variable. Note that guide=\"none\" is used so that a legend is not constructed for the colors and fills.\nscale_x_continuous() and scale_y_continuous() are used to set the labels (with name=) and axis breaks for the x- and y-axes, respectively. The names are drawn from labels that were given in the original call to ageBias() and stored in ab1.\n\nThe gridlines and the size of the fonts could be adjusted by modifying theme theme, which I did not do here for simplicity.\n \n\n\nMore Examples\nBelow are more examples of how ggplot2 can be used to recreate graphs from plot() in FSA. For example, Figure 3 is similar to Figure 2, but uses $bias.diff from ab1 to plot mean differences between scale and otolith ages against otolith ages. The reference for differences is a horizontal line at 0 so geom_abline() from above was replaced with geom_hline() here.\n\nggplot(data=ab1$bias.diff) +\n  geom_hline(yintercept=0,linetype=\"dashed\",color=\"gray\") +\n  geom_errorbar(aes(x=otolithC,ymin=LCI,ymax=UCI,color=sig),width=0) +\n  geom_point(aes(x=otolithC,y=mean,color=sig,fill=sig),shape=21) +\n  scale_fill_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"white\"),guide=\"none\") +\n  scale_color_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"red3\"),guide=\"none\") +\n  scale_x_continuous(name=ab1$ref.lab,breaks=0:25) +\n  scale_y_continuous(name=paste(ab1$nref.lab,\"-\",ab1$ref.lab),breaks=-15:5)\n\n\n\n\n\n\n\nFigure 3: Age difference plot using ggplot2.\n\n\n\n\n\n \nFigure 4 is similar but it includes the raw data points from $data and colors the mean (and confidence intervals) for the differences based on the significance as in Figure 2. Because data were drawn from different data frames (i.e., ab1$data and ab1$bias.diff) the data= and aes= arguments had to be moved into the specific geom_s. Also note that the raw data were made semi-transparent (with alpha=0.1) to emphasize the over-plotting of the discrete ages.\n\nggplot() +\n  geom_hline(yintercept=0,linetype=\"dashed\",color=\"gray\") +\n  geom_point(data=ab1$data,aes(x=otolithC,y=diff),alpha=0.1,size=1.75) +\n  geom_errorbar(data=ab1$bias.diff,aes(x=otolithC,ymin=LCI,ymax=UCI,color=sig),\n                width=0) +\n  geom_point(data=ab1$bias.diff,aes(x=otolithC,y=mean,color=sig,fill=sig),\n             shape=21,size=1.75) +\n  scale_fill_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"white\"),guide=\"none\") +\n  scale_color_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"red3\"),guide=\"none\") +\n  scale_x_continuous(name=ab1$ref.lab,breaks=seq(0,25,1)) +\n  scale_y_continuous(name=paste(ab1$nref.lab,\"-\",ab1$ref.lab),breaks=-15:5)\n\n\n\n\n\n\n\nFigure 4: Age difference plot using ggplot2 including points for individual observations.\n\n\n\n\n\n \nFigure 5 is the same as Figure 4 except that a loess smoother has been added with geom_smooth() to emphasize the trend in the differences in ages. The smoother should be fit to the raw data so be sure to use ab1$data in geom_smooth(). The smoother defaults to blue (as shown here) but I decreased the width of the line slightly with linewidth=0.65.\n\nggplot() +\n  geom_hline(yintercept=0,linetype=\"dashed\",color=\"gray\") +\n  geom_point(data=ab1$data,aes(x=otolithC,y=diff),alpha=0.1,size=1.75) +\n  geom_errorbar(data=ab1$bias.diff,aes(x=otolithC,ymin=LCI,ymax=UCI,color=sig),\n                width=0) +\n  geom_point(data=ab1$bias.diff,aes(x=otolithC,y=mean,color=sig,fill=sig),\n             shape=21,size=1.75) +\n  scale_fill_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"white\"),guide=\"none\") +\n  scale_color_manual(values=c(\"FALSE\"=\"black\",\"TRUE\"=\"red3\"),guide=\"none\") +\n  scale_x_continuous(name=ab1$ref.lab,breaks=seq(0,25,1)) +\n  scale_y_continuous(name=paste(ab1$nref.lab,\"-\",ab1$ref.lab),breaks=-15:5) +\n  geom_smooth(data=ab1$data,aes(x=otolithC,y=diff),linewidth=0.65)\n\n\n\n\n\n\n\nFigure 5: Age difference plot using ggplot2 including points for individual observations and a loess smoother.\n\n\n\n\n\n \n\n\nWhat Prompted This\nGraphics made in ggplot2 are more flexible than the ones produced in FSA. For example, a user recently asked if it was possible to make an “age-bias plot” that used “error bars” based on the standard deviation rather than the standard error. While it is questionable whether this is what should be plotted, it is nevertheless up to the user and their use case. Because this cannot be done using the plots in FSA we turned to ggplot to make such a graph.\nStandard deviations are not returned in any of the ageBias() results (saved in ab1). However, the standard error and sample size are returned in the $bias data frame. The standard deviation can be “back-calculated” from these two values using SD=SE*sqrt(n). Two new variables called LSD and USD that are the means minus and plus two standard deviations can then be created. All three of these variables are added to the $bias data frame using mutate() from dplyr.\n\nab1$bias &lt;- ab1$bias |&gt;\n  mutate(SD=SE*sqrt(n),\n         LSD=mean-2*SD,\n         USD=mean+2*SD)\n\nA plot (Figure 6) like the very first plot above but using two standard deviations for the error bars is then created by mapping ymin= and ymax= to LSD and USD, respectively, in geom_errorbar(). Note that I removed the color related to the significance test as those don’t pertain to the results when using standard deviations to represent “error bars.”\n\nggplot(data = ab1$bias)+\n  geom_abline(slope=1,intercept=0,linetype=\"dashed\",color=\"gray\") +\n  geom_errorbar(aes(x=otolithC,ymin=LSD,ymax=USD),width=0) +\n  geom_point(aes(x=otolithC,y=mean)) +\n  scale_x_continuous(name =ab1$ref.lab,breaks=0:25) +\n  scale_y_continuous(name=ab1$nref.lab,breaks=0:25)\n\n\n\n\n\n\n\nFigure 6: Mean scale age for each otolith age with error bars represented by two standard deviations.\n\n\n\n\n\nFinally, to demonstrate the flexibility of using ggplot with these type of data, I used a violin plot to show the distribution of scale ages for each otolith age while also highlighting the mean scale age for each otolith age (Figure 7). The violin plots are created with geom_violin() using the raw data stored in $data. The group= must be set to the x-axis variable (i.e., otolith age) so that a separate violin will be constructed for each age on the x-axis. I filled the violins with grey to make them stand out more.\n\nggplot() +\n  geom_abline(slope=1,intercept=0,linetype=\"dashed\",color=\"gray\") +\n  geom_violin(data=WhitefishLC,aes(x=otolithC,y=scaleC,group=otolithC),\n              fill=\"grey\") +\n  geom_point(data=ab1$bias,aes(x=otolithC,y=mean),size=2) +\n  scale_x_continuous(name=ab1$ref.lab,breaks=0:25) +\n  scale_y_continuous(name=ab1$nref.lab,breaks=0:25)\n\n\n\n\n\n\n\nFigure 7: Violin plots and mean scale age at each otolith age.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{lant2021,\n  author = {Lant, Michael},\n  title = {Age {Bias} {Plots} in Ggplot2},\n  date = {2021-03-15},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2021-3-15-AgeBiasPlots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLant, M. 2021, March 15. Age Bias Plots in ggplot2. https://fishr-core-team.github.io/fishR//blog/posts/2021-3-15-AgeBiasPlots."
  },
  {
    "objectID": "blog/posts/2021-5-12_compIntercepts-replacement/index.html",
    "href": "blog/posts/2021-5-12_compIntercepts-replacement/index.html",
    "title": "Replace compIntercepts() with emmeans()",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below.\nlibrary(dplyr)    ## for filter(), mutate()\nlibrary(emmeans)  ## for emmeans()"
  },
  {
    "objectID": "blog/posts/2021-5-12_compIntercepts-replacement/index.html#conclusion",
    "href": "blog/posts/2021-5-12_compIntercepts-replacement/index.html#conclusion",
    "title": "Replace compIntercepts() with emmeans()",
    "section": "Conclusion",
    "text": "Conclusion\nemmeans() in emmeans provides a more general solution to comparing multiple slopes than what was used in compIntercepts() in FSA prior to v0.9.0. As compIntercepts() was removed from FSA in 2022, you should now use emmeans() for this purpose.\nemmeans has extensive vignettes that further explain its use. Their “Basics” vignette is also useful.\nIn a previous post I demonstrated how to use emtrends() from emmeans to replace compSlopes(), which was also removed from FSA.\n\n\n\n\n\n\nNote\n\n\n\nThis change to FSA does not affect anything in Ogle (2016)."
  },
  {
    "objectID": "blog/posts/2021-5-26_filterD-replacement/index.html",
    "href": "blog/posts/2021-5-26_filterD-replacement/index.html",
    "title": "Replace filterD()",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below.\n\n\n\nlibrary(dplyr)  ## for filter()\n\n\n\n\n\n\n\nWarning\n\n\n\nSome functions illustrated below were in the FSA package but have now been removed and put into the non-released FSAmisc package that I maintain. These functions are used below only to show what could be done in older versions of FSA but should now be done as described in this post. DO NOT USE any of the functions below that begin with FSAmisc::.\n\n\n \nWe deprecated filterD() from FSA v0.9.0 and fully removed it by the start of 2022. filterD() was an attempt to streamline the process of using filter() (from dplyr) followed by droplevels() to remove levels of a factor variable that no longer existed in the filtered data frame.\nFor example, consider the very simple data frame below.\n\nd &lt;- data.frame(tl=runif(6,min=100,max=200),\n                spec=factor(c(\"LMB\",\"LMB\",\"SMB\",\"BG\",\"BG\",\"BG\")))\nd\n\n#R|          tl spec\n#R|  1 157.5134  LMB\n#R|  2 185.0328  LMB\n#R|  3 150.1690  SMB\n#R|  4 178.2323   BG\n#R|  5 163.9159   BG\n#R|  6 174.6475   BG\n\n\nNow suppose that this data frame is reduced to just Bluegill.\n\ndbg &lt;- d |&gt;\n  filter(spec==\"BG\")\n\nA quick frequency table of species caught shows that levels for species that no longer exist in the data frame are maintained.\n\nxtabs(~spec,data=dbg)\n\n#R|  spec\n#R|   BG LMB SMB \n#R|    3   0   0\n\n\nThis same “problem” occurs when using subset() from base R.\n\ndbg &lt;- subset(d,spec==\"BG\")\nxtabs(~spec,data=dbg)\n\n#R|  spec\n#R|   BG LMB SMB \n#R|    3   0   0\n\n\nThese “problems” can be eliminated by submitting the new data frame to drop.levels().\n\ndbg2 &lt;- droplevels(dbg)\nxtabs(~spec,data=dbg2)\n\n#R|  spec\n#R|  BG \n#R|   3\n\n\nfilterD() was a simple work-around that eliminated this second step and was useful for helping students who were just getting started with R.\n\ndbg3 &lt;- FSAmisc::filterD(d,spec==\"BG\")\nxtabs(~spec,data=dbg3)\n\n#R|  spec\n#R|  BG \n#R|   3\n\n\nHowever, this was a hacky solution to a simple problem. Thus, we deprecated and subsequently removed filterD() from FSA. Thus, please use droplevels() (or fct_drop() from forcats) after using filter() to accomplish the same task of the defunct filterD().\n \n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2021,\n  author = {H. Ogle, Derek},\n  title = {Replace {filterD()}},\n  date = {2021-05-26},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2021-5-26_filterD-replacement},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2021, May 26. Replace filterD(). https://fishr-core-team.github.io/fishR//blog/posts/2021-5-26_filterD-replacement."
  },
  {
    "objectID": "blog/posts/2022-12-22_AFS_Style_Figures/index.html",
    "href": "blog/posts/2022-12-22_AFS_Style_Figures/index.html",
    "title": "AFS Style in ggplot2 Figures",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below.\nlibrary(ggplot2)\nlibrary(patchwork)  # for positioning multiple plots"
  },
  {
    "objectID": "blog/posts/2022-12-22_AFS_Style_Figures/index.html#plot-specific-elements",
    "href": "blog/posts/2022-12-22_AFS_Style_Figures/index.html#plot-specific-elements",
    "title": "AFS Style in ggplot2 Figures",
    "section": "Plot-Specific Elements",
    "text": "Plot-Specific Elements\nI suggest the following changes to the plot-specific elements. Implementing these suggestions will result in Figure 2 (A).\n\nName the second argument to ggplot() (i.e., mapping=). ggplot() is smart enough to figure this out but I think it is awkward to name the first argument (i.e., data=) but not the second.\nI moved the text for the axis labels (i.e., titles) into scale_y_continuous() and scale_x_continuous() for simplicity (and thus removed ylab() and xlab()).\nI changed expand=c(0,0) in scale_y_continuous() and scale_x_continuous() to expand=expansion(mult=c(0,0)) to follow more recent ggplot2 conventions. This still results in no scale expansion at the top or bottom of axis.\nI removed limits= in scale_fill_manual() and used a named vector in values= to accomplish the same task. I think this makes it easier to see which category gets which color.\nI removed ggtitle() as I am going to accomplish that same task with patchwork as shown below (i.e., where the two plots are place side-by-side).\nI used legend.title=element_blank() in theme() rather than labs(fill=\"\") to remove to legend title. I think(?) this handles the freed up space better.\nI included legend.position= in theme() here because Glassic et al. (2019) positioned the legend within the plot area and, thus, will need to be set specific to each plot (i.e., will need to manually find a “white area”).\n\n\nlen_wt_afs1 &lt;- ggplot(data=length_weight_data,\n                      mapping=aes(x=length,y=weight,fill=species)) +\n  # set symbol shape and size\n  geom_point(shape=21,size=2) +\n  # set the limits, tick breaks, and scale expansion for the y- and x-axis\n  scale_y_continuous(name=\"Weight (g)\",\n                      limits=c(0,2400),breaks=seq(0,2400,400),\n                      expand=expansion(mult=c(0,0))) +\n  scale_x_continuous(name=\"Length (mm)\",limits=c(100,600),breaks=seq(100,600,100),\n                      expand=expansion(mult=c(0,0))) +\n  # set the symbol colors and make new labels for each level\n  scale_fill_manual(values=c(\"lmb\"=\"black\",\"cat\"=\"white\"),\n                    labels=c(\"Largemouth Bass\",\"Channel Catfish\")) +\n  theme(\n    # remove legend title\n    legend.title=element_blank(),\n    # set legend position within the plot\n    legend.position = c(0.35,0.95)\n  )"
  },
  {
    "objectID": "blog/posts/2022-12-22_AFS_Style_Figures/index.html#custom-theme-for-all-plots",
    "href": "blog/posts/2022-12-22_AFS_Style_Figures/index.html#custom-theme-for-all-plots",
    "title": "AFS Style in ggplot2 Figures",
    "section": "Custom Theme for All Plots",
    "text": "Custom Theme for All Plots\nTheme elements that will be consistent across plots can be put into a custom theme that can then be easily applied to any plot. The code below, for example, shows the start of a new theme called theme_AFS() that has theme_classic() as its base but will have several elements replaced. theme_AFS() will use a base font size of 14 and Times New Roman as defaults, which will minimize some of the authors’ code (as described below).\n\ntheme_AFS &lt;- function(base_size=14,base_family=\"Times New Roman\") {\n  theme_classic(base_size=base_size,base_family=base_family) +\n    theme(\n      ### Change theme elements here ###\n    )\n}\n\nWithin theme() I add nearly all of the elements that Glassic et al. (2019) included, though I reordered the items in a way that makes sense to me (e.g., axis title then axis tick labels then axis ticks then axis line). Other adjustments are:\n\nI removed text= because the font family was already set with base_family=.\nI removed legend.position= because, as described above, Glassic et al. (2019) positioned the legend within the plot area.\nI removed size=14 from axis.title.y=, axis.title.x=, axis.text.y=, and axis.text.x= because that was set with base_size=.\nI replaced axis.ticks.y= and axis.ticks.x= with axis.ticks= because their elements were the same and using axis.ticks= will set both at the same time.\n\n\ntheme_AFS &lt;- function(base_size=14,base_family=\"Times New Roman\") {\n  theme_classic(base_size=base_size,base_family=base_family) +\n    theme(\n      # modify plot title,the B in this case\n      plot.title=element_text(family=\"Arial\",face=\"bold\"),\n      # margin for the plot\n      plot.margin=unit(c(0.5,0.5,0.5,0.5),\"cm\"),\n      # set axis label (i.e., title) colors and margins\n      axis.title.y=element_text(colour=\"black\",margin=margin(t=0,r=10,b=0,l=0)),\n      axis.title.x=element_text(colour=\"black\",margin=margin(t=10,r=0,b=0,l=0)),\n      # set tick label color, margin, and position and orientation\n      axis.text.y=element_text(colour=\"black\",margin=margin(t=0,r=5,b=0,l=0),\n                               vjust=0.5,hjust=1),\n      axis.text.x=element_text(colour=\"black\",margin=margin(t=5,r=0,b=0,l=0),\n                               vjust=0,hjust=0.5,),\n      # set size of the tick marks for y- and x-axis\n      axis.ticks=element_line(linewidth=0.5),\n      # adjust length of the tick marks\n      axis.ticks.length=unit(0.2,\"cm\"),\n      # set the axis size,color,and end shape\n      axis.line=element_line(colour=\"black\",linewidth=0.5,lineend=\"square\"),\n      # adjust size of text for legend\n      legend.text=element_text(size=12)\n    )\n}\n\ntheme_AFS() can then be “added” to any plot to apply its elements. For example, applying theme_AFS() to the code that produced Figure 2 (A) will produce Figure 2 (B). It is important, however, to make sure that the plot-specific theme() elements are applied after the custom theme.\n\nlen_wt_afs2 &lt;- ggplot(data=length_weight_data,\n                      mapping=aes(x=length,y=weight,fill=species)) +\n  # set symbol shape and size\n  geom_point(shape=21,size=2) +\n  # set the limits, tick breaks, and scale expansion for the y- and x-axis\n  scale_y_continuous(name=\"Weight (g)\",\n                     limits=c(0,2400),breaks=seq(0,2400,400),\n                     expand=expansion(mult=c(0,0))) +\n  scale_x_continuous(name=\"Length (mm)\",\n                     limits=c(100,600),breaks=seq(100,600,100),\n                     expand=expansion(mult=c(0,0))) +\n  # set the symbol colors and make new labels for each level\n  scale_fill_manual(values=c(\"lmb\"=\"black\",\"cat\"=\"white\"),\n                    labels=c(\"Largemouth Bass\",\"Channel Catfish\")) +\n  theme_AFS() +\n  theme(\n    # remove legend title\n    legend.title=element_blank(),\n    # set legend position within the plot\n    legend.position = c(0.35,0.95)\n  )\n\n\n\n\n\n\n\nHint\n\n\n\nSee the “Themes” chapter of Wickham et al. (2022) for an excellent description of using themes in ggplot2 graphics."
  },
  {
    "objectID": "blog/posts/2022-12-22_AFS_Style_Figures/index.html#using-patchwork-to-position-plots",
    "href": "blog/posts/2022-12-22_AFS_Style_Figures/index.html#using-patchwork-to-position-plots",
    "title": "AFS Style in ggplot2 Figures",
    "section": "Using patchwork to Position Plots",
    "text": "Using patchwork to Position Plots\nThe patchwork package provides simple but extensive tools for combining multiple plots. Two plots can simply be “added” together to place them side-by-side (Figure 2). plot_annotation() is used to add letters to the two panels.8\n8 See the excellent documentation for patchwork to better understand what this package can do.\nlen_wt_afs1 + len_wt_afs2 +\n  plot_annotation(tag_levels=\"A\")\n\n\n\n\n\n\n\nFigure 2: Figures made with ggplot2 including one with (A) only plot-specific modifications to default values and (B) with plot specific modifications and a custom theme that adheres to American Fisheries Society guidelines for authors. Panel B reproduces panel B in Figure 1 above and Figure 2 in Glassic et al. (2019).\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\nAnnotating plots with patchwork will place the annotations on the very edge of the figure panel. Use ggtitle() to move them more to the right as Glassic et al. (2019) had them."
  },
  {
    "objectID": "blog/posts/2023-2-16_Ulaskietal_GrowthMortFig/index.html",
    "href": "blog/posts/2023-2-16_Ulaskietal_GrowthMortFig/index.html",
    "title": "Ulaski et al. (2022) Growth-Maturity Figure",
    "section": "",
    "text": "Introduction\nUlaski et al. (2022) modeled the White Sturgeon (Acipenser transmontanus) population in the Sacramento-San Joaquin River Basin to suggest management goals for the population. Their Figure 1 showed a modeled von Bertalanffy growth function with shading below the curve that indicated the probability of being mature for each modeled age. I had not seen a plot like this before and set out to recreate it with ggplot2.\nThis exercise turned out to be more challenging than I thought and I ultimately reached out to Marta Ulaski, the lead author on the paper, to see if their solution was different or “easier” than mine. It was different but I learned something with both methods; thus, I will present both here.1\n1 I modified the specifics but kept the overall concept of Ulaski’s approach. \n\n\nGetting Setup\nThe following packages are loaded for use below.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\n\nThe ggplot2 theme was set to theme_bw() but with a larger base font size and gridlines removed.\n\ntheme_set(\n  theme_bw(base_size=14) +\n  theme(panel.grid=element_blank())\n)\n\n \n\nGrowth Function\nThe three parameters (\\(L_{\\infty}\\), \\(K\\), and \\(t_{0}\\)) of a “typical” von Bertalanffy growth function (VBGF) provided in the caption to Figure 1 of Ulaski et al. (2022) were entered into the vbpar named vector.\n\nvbpar &lt;- c(\"Linf\"=380,\"K\"=0.027,\"to\"=-2.36)\n\nvbFuns() from FSA was used to create a function that returns the mean length-at-age given a set of ages and “typical” VBGF parameters.\n\nvb &lt;- FSA::vbFuns('Typical')\nvb\n\n#R|  function (t, Linf, K = NULL, t0 = NULL) \n#R|  {\n#R|      if (length(Linf) == 3) {\n#R|          K &lt;- Linf[[2]]\n#R|          t0 &lt;- Linf[[3]]\n#R|          Linf &lt;- Linf[[1]]\n#R|      }\n#R|      Linf * (1 - exp(-K * (t - t0)))\n#R|  }\n#R|  &lt;bytecode: 0x000002744f2c7608&gt;\n#R|  &lt;environment: 0x000002744f244e70&gt;\n\n\nWhile this function appears overly complicated, an advantage of this function is that all three parameters of the typical VBGF can be given to the Linf= argument.\nBelow is an illustrative example for computing the mean length-at-age given the parameters in vbpar for ages 10 and 15.\n\nvb(c(10,15),Linf=vbpar)\n\n#R|  [1] 107.8234 142.1949\n\n\nvb() will be used in both approaches to making Figure 1 below.\n \n\n\nMaturity Data\nUlaski et al. (2022) provided probabilities of being mature by age in their Table 1. Here I entered those data directly into a data frame, though I multiplied the probabilities by 100 as that is how they are presented in Figure 1. I also added a much older age of 50 to illustrate how this process could be extended to older ages than what was shown in their Figure 1.\n\ndfmat &lt;- data.frame(age=c(0,10:20,50),\n                    prmat=c(0,0.025,0.086,0.143,0.291,0.543,\n                            0.622,0.788,0.849,0.942,0.966,1,1)*100)\ndfmat\n\n#R|     age prmat\n#R|  1    0   0.0\n#R|  2   10   2.5\n#R|  3   11   8.6\n#R|  4   12  14.3\n#R|  5   13  29.1\n#R|  6   14  54.3\n#R|  7   15  62.2\n#R|  8   16  78.8\n#R|  9   17  84.9\n#R|  10  18  94.2\n#R|  11  19  96.6\n#R|  12  20 100.0\n#R|  13  50 100.0\n\n\nThe probabilities presented in Figure 1 have been binned into categories. The first category is simply 0% (none mature), but each category after that has a width of 10%. Thus, the next two categories would be from 0.1 to 10%, and 10.1 to 20%. Ulaski et al. (2022) chose to label these two categories as “0-10%” and “10-20%”, respectively.\nThese categories may be created with cut(), which takes the data to categorize as its first argument, the values at which to “cut” the categories in breaks=, and labels for the categories in labels=. It is important to note that cut() makes categories right-inclusive by default. Thus, if the breaks are c(0,10,20) then the first category would be from 0 to 10, with 10 being inclusive. Thus, a value of 9 or 10 would be included in this category, but 0 would not. Thus, to have a category for just the 0 values, the breaks must start at some negative number (in this case, negative infinity was used).2 The results of cut() were added to the prcuts variable in dfmat.3\n2 seq(0,100,10) creates a sequence from 0 to 100 in steps of 10.3 Because using mutate().\ndfmat &lt;- dfmat |&gt;\n  mutate(prcuts=cut(prmat,\n                    breaks=c(-Inf,seq(0,100,10)),\n                    labels=c(\"0%\",\"0-10%\",\"10-20%\",\"20-30%\",\"30-40%\",\"40-50%\",\n                             \"50-60%\",\"60-70%\",\"70-80%\",\"80-90%\",\"90-100%\")))\ndfmat\n\n#R|     age prmat  prcuts\n#R|  1    0   0.0      0%\n#R|  2   10   2.5   0-10%\n#R|  3   11   8.6   0-10%\n#R|  4   12  14.3  10-20%\n#R|  5   13  29.1  20-30%\n#R|  6   14  54.3  50-60%\n#R|  7   15  62.2  60-70%\n#R|  8   16  78.8  70-80%\n#R|  9   17  84.9  80-90%\n#R|  10  18  94.2 90-100%\n#R|  11  19  96.6 90-100%\n#R|  12  20 100.0 90-100%\n#R|  13  50 100.0 90-100%\n\n\ndfmat will be used in both approaches to making Figure 1 below.\n \n\n\nDefine Repetitive Values\nFor simplicity, some values that will be used in both approaches were assigned to objects that can be reused.\n\n# x-axis (age) title, limits, labels\nagettl &lt;- \"Age (yrs)\"\nagelmts &lt;- c(0,20)\nagelbls &lt;- 0:20\n# y-axis (length) title, limits, labels\nlenttl &lt;- \"Length (cm)\"\nlenlmts &lt;- c(0,200)\nlenlbls &lt;- seq(0,200,50)\n# fill color (probability mature) title\nprobttl &lt;- \"Probability\"\n\n \n\n\n\nMy Recreation of Figure 1\n\nPlotting the von B function\nThe typical VBGF can be plotted over the range of ages in a data frame with stat_function().4 The function to be evaluated (i.e., vb()) is given in fun= and any arguments that it requires are given in a list to args=.5 The smoothness of the curve can be controlled with n=, which is the number of ages over the range of ages for which the function will be evaluated.6 Finally, I increased the line width slightly.\n4 Make sure to map the age variable to the x aesthetic.5 Here we can set vbpar to just Linf given that all parameters can be given to this one argument as shown above.6 n= defaults to 101, which appeared adequate for these data, though I increased it here to demonstrate its use.\nggplot(data=dfmat,mapping=aes(x=age)) +\n  scale_x_continuous(name=agettl,limits=agelmts,breaks=agelbls,\n                     expand=expansion(mult=0)) +\n  scale_y_continuous(name=lenttl,limits=lenlmts,breaks=lenlbls,\n                     expand=expansion(mult=0)) +\n  stat_function(fun=vb,args=list(Linf=vbpar),\n                n=202,linewidth=0.75)\n\n\n\n\n\n\n\n\nAs seen above, stat_function() defaults to drawing a line of the function (i.e., it uses geom_line()). However, other geoms can be used; e.g., geom=\"area\".7\n7 I used fill= here to show the effect, but this also required setting color= because the color for the line took on the fill color.\nggplot(data=data.frame(age=c(0,20)),mapping=aes(x=age)) +\n  scale_x_continuous(name=agettl,limits=agelmts,breaks=agelbls,\n                     expand=expansion(mult=0)) +\n  scale_y_continuous(name=lenttl,limits=lenlmts,breaks=lenlbls,\n                     expand=expansion(mult=0)) +\n  stat_function(fun=vb,args=list(Linf=vbpar),\n                n=202,linewidth=0.75,\n                geom=\"area\",fill=\"salmon\",color=\"black\")\n\n\n\n\n\n\n\n\n \n\n\nAdding the Maturity Scale\nMy solutions to recreating Figure 1 of Ulaski et al. (2022) generally followed the StackOverflow answer at the bottom of this question. This process uses after_stat() and after_scale(), which were introduced to ggplot2 in v3.3.0.8 I don’t yet fully understand these two functions, but will try to explain what I think they are doing.\n8 See their introduction here.9 Examine dfmat to see why.As illustrated above, stat_function() produces a smooth curve by creating n= values of x over the range of the variable mapped to the x= aesthetic. One part of the “trick” to this solution is to first realize that these “age” values created by stat_function() are not integers and, thus, they need to be “cut” into integer age categories. The second part of the “trick” to this solution is that the probability of maturity category labels should be used for the “cuts” of age rather than labels of age. For example, an age of 13.5 created by stat_function() should be categorized as an age of 13 but labeled with “20-30%”.9 This cutting of the age values comes after they have been created by stat_function() in the x= aesthetic and are thus accessed by after_stat(x) which I pipe into cut() and set equal to the fill= aesthetic in stat_function().\nThe third part of the “trick” to this solution is to realize that the different colored areas can only be plotted if the group= aesthetic is set to the same categories used in the fill= aesthetic. As the fill= aesthetic was just created and is defined in a scale (see scale_fill_viridis_d() below), the group= aesthetic must be defined with after_scale(fill).\n\nggplot(data=dfmat,mapping=aes(x=age)) +\n  scale_x_continuous(name=agettl,limits=agelmts,breaks=agelbls,\n                     expand=expansion(mult=0)) +\n  scale_y_continuous(name=lenttl,limits=lenlmts,breaks=lenlbls,\n                     expand=expansion(mult=0)) +\n  stat_function(mapping=aes(fill=after_stat(x) |&gt;\n                              cut(breaks=!!dfmat$age,\n                                  labels=!!dfmat$prcuts[-nrow(dfmat)],\n                                  include.lowest=TRUE),\n                            group=after_scale(fill)),\n                fun=vb,args=list(Linf=vbpar),n=202,\n                geom=\"area\",color=\"black\",linewidth=0.75)\n\n\n\n\n\n\n\n\nThis, obviously, is not ideal … largely due to the long name for the legend. However, it is also not the colors used in Figure 1 of Ulaski et al. (2022). A custom viridis-based color scheme was used in Ulaski et al. (2022),10 which can be used with scale_fill_viridis_d().\n10 I would not have “discovered” this color scheme on my own. This came from seeing Ulaski’s original code.\nggplot(data=dfmat,mapping=aes(x=age)) +\n  scale_x_continuous(name=agettl,limits=agelmts,breaks=agelbls,\n                     expand=expansion(mult=0)) +\n  scale_y_continuous(name=lenttl,limits=lenlmts,breaks=lenlbls,\n                     expand=expansion(mult=0)) +\n  stat_function(mapping=aes(fill=after_stat(x) |&gt;\n                              cut(breaks=!!dfmat$age,\n                                  labels=!!dfmat$prcuts[-nrow(dfmat)],\n                                  include.lowest=TRUE),\n                            group=after_scale(fill)),\n                fun=vb,args=list(Linf=vbpar),n=202,\n                geom=\"area\",color=\"black\",linewidth=0.75) +\n  scale_fill_viridis_d(name=probttl,begin=0.85,end=0)\n\n\n\n\n\n\n\n\nFinally, there are two things with this plot that I don’t like. First, the black line for the growth function appears broken at the color breaks. Second, the linewidth around the colors in the legend is too thick. To correct these issues, I removed linewidth= and color= from stat_function() and then added a second stat_function() that plots just the growth function as a line.11 This second use of stat_function() lays the function line on top of the function “area.”\n11 See the first use of stat_function() further above for how this was done.\nggplot(data=dfmat,mapping=aes(x=age)) +\n  scale_x_continuous(name=agettl,limits=agelmts,breaks=agelbls,\n                     expand=expansion(mult=0)) +\n  scale_y_continuous(name=lenttl,limits=lenlmts,breaks=lenlbls,\n                     expand=expansion(mult=0)) +\n  stat_function(mapping=aes(fill=after_stat(x) |&gt;\n                              cut(breaks=!!dfmat$age,\n                                  labels=!!dfmat$prcuts[-nrow(dfmat)],\n                                  include.lowest=TRUE),\n                            group=after_scale(fill)),\n                fun=vb,args=list(Linf=vbpar),n=202,\n                geom=\"area\") +\n  scale_fill_viridis_d(name=probttl,begin=0.85,end=0) +\n  stat_function(fun=vb,args=list(Linf=vbpar),n=202,\n                color=\"black\",linewidth=0.75)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuture Updates?\n\n\n\nI may update this post as I learn more about after_stat() and after_scale().\n\n\n \n\n\n\nAuthor’s Creation of Figure 1\n\n\n\n\n\n\nAcknowledgment\n\n\n\nThe solution in this section follows the concept of the code provided to me by Marta Ulaski, the lead author on the original paper. I have modified the specific code to better fit the rest of the post. I thank Marta for generously providing her source code and allowing me to modify it for presentation here.\n\n\n \n\nMaking the Data Frame\nUlaski’s solution to making this figure is much simpler to explain (Thank you!) but relies on explicitly creating a data frame that is largely what stat_function() did behind-the-scenes. Specifically, Ulaski created a data frame that repeated the probability of being mature for all ages in increments of 0.1. So, for example, this data frame contained an age of 12.3 with the probability of being mature being the same as that for age 12. To generalize this solution I set out to see if I could create this data frame from the simple dfmat created above.12\n12 Ulaski had manually created a CSV with these repeated data, which were read in to a data frame.Recall what the original dfmat looked like … ages and probabilities of maturity from Table 1 in the published paper, and categories of probability mature created with cut().\n\ndfmat\n\n#R|     age prmat  prcuts\n#R|  1    0   0.0      0%\n#R|  2   10   2.5   0-10%\n#R|  3   11   8.6   0-10%\n#R|  4   12  14.3  10-20%\n#R|  5   13  29.1  20-30%\n#R|  6   14  54.3  50-60%\n#R|  7   15  62.2  60-70%\n#R|  8   16  78.8  70-80%\n#R|  9   17  84.9  80-90%\n#R|  10  18  94.2 90-100%\n#R|  11  19  96.6 90-100%\n#R|  12  20 100.0 90-100%\n#R|  13  50 100.0 90-100%\n\n\nA new data frame is created that has fractional ages that begin at the minimum age in dfmat, end at the maximum age in dfmat, and uses a constant increment (0.1 in this example). lencat() from FSA is then used to create categories of ages that match the ages in dfmat. lencat() returns a factor variable by default but as.fact=FALSE causes it to return a number to match the age variable in dfmat.\n\ndfmat2 &lt;- data.frame(agef=seq(min(dfmat$age),max(dfmat$age),0.1)) |&gt;\n  mutate(age=FSA::lencat(agef,breaks=dfmat$age,as.fact=FALSE))\nFSA::headtail(dfmat2,n=5)\n\n#R|      agef age\n#R|  1    0.0   0\n#R|  2    0.1   0\n#R|  3    0.2   0\n#R|  4    0.3   0\n#R|  5    0.4   0\n#R|  497 49.6  20\n#R|  498 49.7  20\n#R|  499 49.8  20\n#R|  500 49.9  20\n#R|  501 50.0  50\n\n\nThe probability mature values and categories are added to this data frame with left_join() using the common age variable in dfmat and dfmat2.\n\ndfmat2 &lt;- dfmat2 |&gt;\n  left_join(dfmat,by=\"age\")\nFSA::headtail(dfmat2,n=5)\n\n#R|      agef age prmat  prcuts\n#R|  1    0.0   0     0      0%\n#R|  2    0.1   0     0      0%\n#R|  3    0.2   0     0      0%\n#R|  4    0.3   0     0      0%\n#R|  5    0.4   0     0      0%\n#R|  497 49.6  20   100 90-100%\n#R|  498 49.7  20   100 90-100%\n#R|  499 49.8  20   100 90-100%\n#R|  500 49.9  20   100 90-100%\n#R|  501 50.0  50   100 90-100%\n\n\nFinally, the predicted mean lengths are added using vb() as before.\n\ndfmat2 &lt;- dfmat2 |&gt;\n  mutate(len=vb(agef,Linf=vbpar))\nFSA::headtail(dfmat2)\n\n#R|      agef age prmat  prcuts       len\n#R|  1    0.0   0     0      0%  23.45828\n#R|  2    0.1   0     0      0%  24.41965\n#R|  3    0.2   0     0      0%  25.37842\n#R|  499 49.8  20   100 90-100% 287.06956\n#R|  500 49.9  20   100 90-100% 287.32013\n#R|  501 50.0  50   100 90-100% 287.57003\n\n\nThese steps can be combined into one tight set of code as shown below.\n\ndfmat2 &lt;- data.frame(agef=seq(min(dfmat$age),max(dfmat$age),0.1)) |&gt;\n  mutate(age=FSA::lencat(agef,breaks=dfmat$age,as.fact=FALSE)) |&gt;\n  left_join(dfmat,by=\"age\") |&gt;\n  mutate(len=vb(agef,Linf=vbpar))\n\n \n\n\nMaking the Figure\nFigure 1 is then created by mapping the fractional ages (i.e., agef) to x= and the lengths to y, mapping the probability cuts to fill= in geom_ares(), and including geom_line() (with a slightly larger line width).13\n13 This will produce a warning because the data frame extends to age 50 but the figure is limited to age 20.\nggplot(data=dfmat2,mapping=aes(x=agef,y=len)) + \n  geom_area(mapping=aes(fill=prcuts)) + \n  geom_line(linewidth=0.75) +\n  scale_x_continuous(name=agettl,limits=agelmts,breaks=agelbls,\n                     expand=expansion(mult=0)) +\n  scale_y_continuous(name=lenttl,limits=lenlmts,breaks=lenlbls,\n                     expand=expansion(mult=0)) +\n  scale_fill_viridis_d(name=\"Probability\",begin=0.85,end=0)\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nFurther Thoughts\nThere are a few things that I would like to see different in this figure. First, the “white” lines between the colors are too prominent. The size of these lines are a function of the number of “fractional” ages used to produce the plot. Thus, to make these “lines” thinner one could increase n= in my solution or decrease by= in the author’s solution.\nSecond, it bothers me that the colors for the probabilities seem equally spaced even though two categories are not represented in the data (e.g., 30-40% and 40-50%). In my mind, there should be a “jump” in colors at the ages where the probability jumps from 20-30% to 50-60% (i.e., between age-13 and age-14).14 I could not address this issue with my solution, but including drop=FALSE in scale_fill_viridis_d() fixed this in the author’s solution.\n14 This will be a common issue with maturity data as the probability of maturity often increases dramatically over a short range of lengths and, thus, one or very few ages.15 This is not that useful here given the shape of the VBGF for this species.Third, the “line” does not look like a typical VBGF to me as these sturgeon are long-lived and grow so slowly that very little curvature and no asymptote is evident. In some situations is may be useful to extend the x-axis to older ages to better “see” the typical asymptotic growth of the VBGF.15\nFinally, the authors started their plot at age-1. I am not sure why they did this, but that can be accomplished by filtering the data to only age-1 and older.\nAll of these changes (with by=0.05 and extending the ages to 30) were made below using the author’s solution.\n\nagelmts &lt;- c(0,30)                               # changed max\nagelbls &lt;- seq(0,30,2)                           # changed max, made sequence by 2\nlenlmts &lt;- c(0,250)                              # changed max\nlenlbls &lt;- seq(0,250,50)                         # changed max\n\nby &lt;- 0.05                                       # made smaller\n\ndfmat3 &lt;- data.frame(agef=seq(min(dfmat$age),max(dfmat$age),by)) |&gt;\n  mutate(age=FSA::lencat(agef,breaks=dfmat$age,as.fact=FALSE)) |&gt;\n  left_join(dfmat,by=\"age\") |&gt;\n  mutate(len=vb(agef,Linf=vbpar)) |&gt;\n  filter(agef&gt;=1)                                # filtered out &lt;age-1\n\nggplot(data=dfmat3,mapping=aes(x=agef,y=len)) +  # used new dfmat3\n  geom_area(mapping=aes(fill=prcuts)) + \n  geom_line(linewidth=0.75) +\n  scale_x_continuous(name=agettl,limits=agelmts,breaks=agelbls,\n                     expand=expansion(mult=0)) +\n  scale_y_continuous(name=lenttl,limits=lenlmts,breaks=lenlbls,\n                     expand=expansion(mult=0)) +\n  scale_fill_viridis_d(name=\"Probability\",begin=0.85,end=0,drop=FALSE)\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nReferences\n\nUlaski, M. E., S. E. Blackburn, Z. J. Jackson, and M. C. Quist. 2022. Management goals for conserving White Sturgeon in the Sacramento–San Joaquin River Basin. Journal of Fish and Wildlife Management 13(2):334–343.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Ulaski Et Al. (2022) {Growth-Maturity} {Figure}},\n  date = {2023-02-16},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-2-16_Ulaskietal_GrowthMortFig},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, February 16. Ulaski et al. (2022) Growth-Maturity\nFigure. https://fishr-core-team.github.io/fishR//blog/posts/2023-2-16_Ulaskietal_GrowthMortFig."
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html",
    "title": "Axis Magic",
    "section": "",
    "text": "Description\n\n\n\nThis is a collection of methods that I have used to handle various axis-related issues in ggplot2. I will continue to update this post when I discover new “fixes.” Please send me questions, thoughts, or solutions for future updates.\nThe following packages are used here. Also note that a function from lemon() is used below but :: was used so the entire package is not attached here.\nlibrary(tidyverse)  # for ggplot2 and dplyr\nlibrary(scales)     # for help with scales\nlibrary(ggh4x)      # for a variety of \"hacks\" described below\nThe default theme will be theme_classic().\ntheme_set(theme_classic())\nMost of the examples below will use generic plots that show only an x- or y-axis, without plotting any actual data. For example, an x-axis for continuous data is constructed with demo_continuous() given a range of values for the x-axis.1\ndemo_continuous(c(0,100))\nIn contrast, an x-axis for dates data is constructed with demo_datetime() given a range of dates for the x-axis.2\ndemo_date(c(\"1-Jan-2000\",\"31-Dec-2020\"))\nSome examples will have an x- and a y-axis and will include faceting.\ndemo_date(c(\"1-Jan-2000\",\"31-Dec-2020\",\"1-Jan-2000\",\"31-Dec-2020\"),\n          c(0,50,0,100),c(\"A\",\"A\",\"B\",\"B\"))\nNearly all examples will be demonstrated only for the, though similar modifications can be made to the y-axis in a straightforward manner (replacing “x” with “y” in appropriate functions and arguments.)"
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html#first-method-minor-ticks-of-different-sizes",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html#first-method-minor-ticks-of-different-sizes",
    "title": "Axis Magic",
    "section": "First Method (Minor Ticks of Different Sizes)",
    "text": "First Method (Minor Ticks of Different Sizes)\nThe ggh4x package provides a method for adding unlabeled minor ticks between the labeled major ticks. With this method breaks= is used to define the labeled major tick locations and minor_breaks= is used to define the unlabeled minor tick locations. Then use guide=\"axis_minor\"8 in scale_x_continuous() to make the minor ticks visible.\n8 This is what ggh4x provides.\ndemo_continuous(c(0,100)) + \n  scale_x_continuous(breaks=seq(0,100,by=20),\n                     minor_breaks=seq(0,100,by=1),guide=\"axis_minor\") +\n  theme(axis.ticks.length=unit(5,units=\"pt\"))\n\n\n\n\n\n\n\n\nBe careful, though, that when using breaks_width() the minor ticks will extend into the “expanded” portion of the scale.\n\ndemo_continuous(c(0,100)) + \n  scale_x_continuous(breaks=breaks_width(20),\n                     minor_breaks=breaks_width(1),guide=\"axis_minor\") +\n  theme(axis.ticks.length=unit(5,units=\"pt\"))\n\n\n\n\n\n\n\n\nMinor ticks can be set to a different length with ggh4x.axis.ticks.length.minor= for setting the lengths of the minor ticks in theme().9 The length of the minor ticks is set relative to the length of the major ticks through rel(). In the example below, rel(0.5) is used to make the minor ticks 50% of the length of the major ticks.\n9 This argument to theme() is provided by ggh4x.\ndemo_continuous(c(0,100)) + \n  scale_x_continuous(breaks=seq(0,100,by=20),\n                     minor_breaks=seq(0,100,by=1),guide=\"axis_minor\") +\n  theme(axis.ticks.length=unit(5,units=\"pt\"),\n        ggh4x.axis.ticks.length.minor=rel(0.5))"
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html#second-method",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html#second-method",
    "title": "Axis Magic",
    "section": "Second Method",
    "text": "Second Method\nThe text associated with the breaks/ticks is controlled for both axes with axis.text= or for individual axes with axis.text.(x|y)= in theme(). For example, the tick mark labels for the x-axis are changed to red below.10\n10 I also lengthened the ticks for illustrative purposes.\ndemo_continuous(c(0,100)) +\n  theme(axis.ticks.length=unit(5,units=\"pt\"),\n        axis.text.x=element_text(color=\"red\"))\n\n\n\n\n\n\n\n\nMultiple colors can also be used, which will be “recycled” if fewer colors are given then tick mark labels. Below two colors are given, but because there are five ticks, the colors are “recycled” resulting in alternating colors.11\n11 This use of a vector of colors is not an official feature of ggplot2 and, thus, a warning will be issued.\ndemo_continuous(c(0,100)) +\n  theme(axis.ticks.length=unit(5,units=\"pt\"),\n        axis.text.x=element_text(color=c(\"red\",\"blue\")))\n\n\n\n\n\n\n\n\nMore colors can be given that better demonstrate the recycling property.\n\ndemo_continuous(c(0,100)) +\n  theme(axis.ticks.length=unit(5,units=\"pt\"),\n        axis.text.x=element_text(color=c(\"red\",\"blue\",\"green\")))\n\n\n\n\n\n\n\n\nNow suppose that labels are desired at intervals of 25, as above, with unlabeled ticks at intervals of 5. The “trick”12 here is to create a sequence for every tick (major or minor), but then set the label color to black for the first label and then NA for the next four labels. The NA “color” means to use no color. Thus, the “0” label will be black, but the “5”, “10”, “15”, and “20” will have no color and, thus, will not be visible. Given the “recycling property discussed above this pattern will be repeated so that the”25” is black but then “30” to “45” will have no color, “50” will be black, etc.13\n12 This “trick” came from this StackOverflow answer.13 Make sure you note below that the breaks were set to a sequence with an interval of 5.\ndemo_continuous(c(0,100)) +\n  scale_x_continuous(breaks=seq(0,100,by=5)) +\n  theme(axis.ticks.length=unit(5,units=\"pt\"),\n        axis.text.x=element_text(color=c(\"black\",\"NA\",\"NA\",\"NA\",\"NA\")))\n\n\n\n\n\n\n\n\nThere are times when it may be easier to use rep() to repeat the required NA values. For example, suppose that ticks are placed at intervals of 1 but that labels should be placed only at intervals of 10. In this case then, the “black” color should be followed by 9 “NA”s, as demonstrated below.\n\ndemo_continuous(c(0,100)) +\n  scale_x_continuous(breaks=seq(0,100,by=1)) +\n  theme(axis.ticks.length=unit(5,units=\"pt\"),\n        axis.text.x=element_text(color=c(\"black\",rep(NA,9))))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember to initially (in scale_x_continuous()) make tick marks at all of the values that you desire a tick mark (of some sort) and then make sure to have as many NAs in the colors as unlabeled ticks between labeled ticks.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI don’t know of a way to have differently sized major and minor tick marks using this method."
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html#digits",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html#digits",
    "title": "Axis Magic",
    "section": "Digits",
    "text": "Digits\nThere will be times when the axis displays more or fewer digits than you would prefer.\n\ndemo_continuous(c(0,100)) +\n  scale_x_continuous(breaks=breaks_width(100/6))\n\n\n\n\n\n\n\n\nlabel_number() from the scales package18 provides accuracy= which can be used to control the number of decimals shown. This argument simply takes a value that illustrates the number of decimal places to display. For example, 0.01 and 0.07 would both result in two decimals places being shown, whereas 0.003 would show 3 and 1 would show none.\n18 Make sure the package is attached with library(scales) as was done at the beginning of this post.\ndemo_continuous(c(0,100)) +\n  scale_x_continuous(breaks=breaks_width(100/6),\n                     label=label_number(accuracy=0.01))\n\n\n\n\n\n\n\n\n\ndemo_continuous(c(0,100)) +\n  scale_x_continuous(breaks=breaks_width(100/6),\n                     label=label_number(accuracy=1))"
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html#adornments-comma-dollar-signs-etc.",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html#adornments-comma-dollar-signs-etc.",
    "title": "Axis Magic",
    "section": "Adornments (Comma, Dollar Signs, etc.)",
    "text": "Adornments (Comma, Dollar Signs, etc.)\nscales also provides label_comma() to produce labels with commas at the appropriate multiples of thousands.\n\ndemo_continuous(c(0,10000)) +\n  scale_x_continuous(label=label_comma())\n\n\n\n\n\n\n\n\nThis same effect may be obtained with label_number(), with big.mark=\",\".\n\ndemo_continuous(c(0,100000)) +\n  scale_x_continuous(label=label_number(big.mark=\",\"))\n\n\n\n\n\n\n\n\nA dollar sign may be added as a prefix to the labels with label_dollar() (also noting that 0 or 2 decimals and commas will be shown)\n\ndemo_continuous(c(0,10000)) +\n  scale_x_continuous(label=label_dollar())\n\n\n\n\n\n\n\n\nThis same effect can be obtained with label_number() with prefix=\"$\" (and, if necessary, adding commas with big.mark= and controlling decimals with accuracy=).\n\ndemo_continuous(c(0,10000)) +\n  scale_x_continuous(label=label_number(prefix=\"$\",big.mark=\",\"))\n\n\n\n\n\n\n\n\nA percent sign may be added as a suffix to the labels with label_percent(). However, beware that the values will first be multiplied by 100 (i.e., assuming proportions so converting to percentages).\n\ndemo_continuous(c(0,1)) +\n  scale_x_continuous(label=label_percent())\n\n\n\n\n\n\n\n\nThe suffix is easily added with label_number(), but note that it does not automatically multiply the values by 100. Thus, this would be useful if your data was already percentages.\n\ndemo_continuous(c(0,1)) +\n  scale_x_continuous(label=label_number(suffix=\"%\"))\n\n\n\n\n\n\n\n\nHowever, the label values can be multiplied by 100 with scale=100, thus using label_number() to reproduce the label_percent() result.\n\ndemo_continuous(c(0,1)) +\n  scale_x_continuous(label=label_number(suffix=\"%\",scale=100))"
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html#handling-large-or-small-numbers",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html#handling-large-or-small-numbers",
    "title": "Axis Magic",
    "section": "Handling Large or Small Numbers",
    "text": "Handling Large or Small Numbers\nIt is worth noting that label_number() forces all labels to be numeric not in scientific notation. As an example, the default below shows labels in scientific notation.\n\ndemo_continuous(c(0,1e7))\n\n\n\n\n\n\n\n\nHowever, label_number() forces these labels out of scientific notation.\n\ndemo_continuous(c(0,1e7)) +\n  scale_x_continuous(label=label_number(big.mark=\",\"))\n\n\n\n\n\n\n\n\nYou can better control the scientific notation with label_scientific(). For example, three significant digits is shown below.\n\ndemo_continuous(c(0,1e7)) +\n  scale_x_continuous(breaks=breaks_width(1e7/6),\n                     label=label_scientific(digits=3))\n\n\n\n\n\n\n\n\nAnother way to display larger numbers is to rescale them and denote the rescaling in the axis label. For example, labels from 0 to 1,000,000 might be rescaled to “millions”. This “rescaling” can be performed on the labels rather than on the data with scale= in scale_x_contiuous(). In the example below, each label will be multiplied by 1/1e6 (i.e., divided by 1e6 or a million) before being displayed. The axis “name” was modified to explain this rescaling.\n\ndemo_continuous(c(0,1e6)) +\n  scale_x_continuous(label=label_number(scale=1/1e6),\n                     name=(\"X (millions)\"))\n\n\n\n\n\n\n\n\nSuppose that a variable had been recorded in kilograms …\n\ndemo_continuous(c(0,5)) +\n  scale_x_continuous(name=(\"X (kg)\"))\n\n\n\n\n\n\n\n\n… but you want to display it in pounds.19\n19 Note that 1 kg = 2.20462 lbs.\ndemo_continuous(c(0,5)) +\n  scale_x_continuous(label=label_number(big.mark=\",\",scale=2.20462),\n                     name=(\"X (lbs)\"))\n\n\n\n\n\n\n\n\nNote that the rescaling comes AFTER the tick mark positions have been defined on the original scale. Thus, in the last example, tick marks at places defined by pounds, rather than kilograms, will need to use data that was converted to pounds before making the plot."
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html#brief-primer-on-dates-in-r",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html#brief-primer-on-dates-in-r",
    "title": "Axis Magic",
    "section": "Brief Primer on Dates in R",
    "text": "Brief Primer on Dates in R\nDealing with dates in R will force you to use some special codes defining aspects of the dates. These codes are described in ?strptime. The codes that I most often use with dates are shown in the table below. Note that there are other codes to deal with day names, weeks in the year, etc., along with codes to deal with time.\n\n\n\n\n\n\n\n\nGroup\nCode\nDescription\n\n\n\n\nDay\n%d\nDay of month as number (01-31)\n\n\n\n%e\nDay of month as number with leading space (” 1”, not “01”)\n\n\nMonth\n%b\nAbbreviated month name (Jan-Dec)\n\n\n\n%B\nFull month name (January-December)\n\n\n\n%m\nMonth as a decimal number (01-12)\n\n\nYear\n%Y\nFour-digit year (i.e., with century; e.g., 1999, 2017)\n\n\n\n%y\nTwo-digit year (i.e., without century; e.g., 99, 17)\n\n\n\nThese codes can be combined into strings that describe how dates are formatted. For example, %d-%b-%Y indicates that the dates are in day, abbreviated month name, and four-digit year all separated with hyphens format (e.g., “9-Jul-1967”). Alternatively, %m/%d/%Y indicates that the dates are in numeric month, numeric day, and four-digit year all separated by forward slashes format (e.g., “07/09/1967”)\nStrings can be converted to dates with as.Date(), with the string as the first argument and a string that describes the date format in format=. For example, the following converts two strings in %d-%b-%Y format to “Date” objects.\n\ntmp &lt;- as.Date(c(\"1-May-2020\",\"30-Jul-2020\"),format=\"%d-%b-%Y\")\nstr(tmp)\nDate[1:2], format: “2020-05-01” “2020-07-30”\n\n\n\n\n\n\n\nTip\n\n\n\nThe lubridate packages provides other functions for dealing with dates."
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html#setting-axis-limits",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html#setting-axis-limits",
    "title": "Axis Magic",
    "section": "Setting Axis Limits",
    "text": "Setting Axis Limits\nThe default axis labels for date data depends on the range of the dates. The next three examples, show date ranges of approximately a month, a year, and five years.\n\ndemo_date(c(\"1-May-2020\",\"31-May-2020\"))\n\n\n\n\n\n\n\n\n\ndemo_date(c(\"1-May-2020\",\"30-Apr-2021\"))\n\n\n\n\n\n\n\n\n\ndemo_date(c(\"1-Nov-2015\",\"30-Jul-2020\"))\n\n\n\n\n\n\n\n\nDate axes are modified with scale_x_date(). The limits of the axis can be controlled with limits=, though the vector given here must be valid dates so you will need as.Date(). For example, the axis is extended beyond the month of May in the example below. Note how this changed the format of the labels (from the first example above).20\n20 Formatting the labels is discussed in the next section.\ndemo_date(c(\"1-May-2020\",\"31-May-2020\")) +\n  scale_x_date(limits=as.Date(c(\"1-Apr-2020\",\"30-Jun-2020\"),format=\"%d-%b-%Y\"))"
  },
  {
    "objectID": "blog/posts/2023-3-15-Axis_Magic/index.html#controlling-breaks",
    "href": "blog/posts/2023-3-15-Axis_Magic/index.html#controlling-breaks",
    "title": "Axis Magic",
    "section": "Controlling Breaks",
    "text": "Controlling Breaks\nTick mark labels can be controlled by giving specific dates to breaks= in scale_x_date().\n\ndemo_date(c(\"1-May-2020\",\"31-May-2020\")) +\n  scale_x_date(breaks=as.Date(c(\"1-May-2020\",\"8-May-2020\",\"15-May-2020\",\n                                \"22-May-2020\",\"29-May-2020\"),format=\"%d-%b-%Y\"))\n\n\n\n\n\n\n\n\nHowever, this may (likely?) format the dates in an unwanted way. For example, in the example above it may not be necessary to include the year, and it may be preferred to use the month abbreviations. The format of date labels is controlled with labels= using label_date() to format the date with codes as defined above. The example below shows the month abbreviation and day only.\n\ndemo_date(c(\"1-May-2020\",\"31-May-2020\")) +\n  scale_x_date(breaks=as.Date(c(\"1-May-2020\",\"8-May-2020\",\"15-May-2020\",\n                                \"22-May-2020\",\"29-May-2020\"),format=\"%d-%b-%Y\"),\n               labels=label_date(\"%b %d\"))\n\n\n\n\n\n\n\n\nSetting the breaks with a vector of all breaks can be tedious. However, breaks_width= introduced above can be used to set breaks with key words. For example, date_breaks=\"week\" sets breaks at every week.\n\ndemo_date(c(\"1-May-2020\",\"31-May-2020\")) +\n  scale_x_date(breaks=breaks_width(\"week\"),\n               labels=label_date(\"%b %d\"))\n\n\n\n\n\n\n\n\nIf these dates don’t start on your preferred date then use offset= to “move” the starting data. Here I wanted the dates to start on “May-01” rather than “May-04” so I moved the dates forward by three days with offset=-3.\n\ndemo_date(c(\"1-May-2020\",\"31-May-2020\")) +\n  scale_x_date(breaks=breaks_width(\"week\",offset=-3),\n               labels=label_date(\"%b %d\"))\n\n\n\n\n\n\n\n\n\ndemo_date(c(\"1-May-2020\",\"31-May-2020\")) +\n  scale_x_date(breaks=breaks_width(\"2 weeks\",offset=-10),\n               labels=label_date(\"%b %d\"))\n\n\n\n\n\n\n\n\nOther key words for breaks_width= include day, month, and year.21 For example, the approximate year example from above gets monthly breaks below.22\n21 And also sec, min, and hour for time data.22 And are labeled with the month abbreviation and the year without the century.\ndemo_date(c(\"1-May-2020\",\"30-Apr-2021\")) +\n  scale_x_date(breaks=breaks_width(\"month\"),\n               labels=label_date(\"%b %y\"))\n\n\n\n\n\n\n\n\nbreaks_width() is particularly powerful because any of the key words can be made plural and then a number added in front of it to make breaks at those intervals. For example, the example immediately above is modified below to have breaks every three months.\n\ndemo_date(c(\"1-May-2020\",\"30-Apr-2021\")) +\n  scale_x_date(breaks=breaks_width(\"3 months\"),\n               labels=label_date(\"%b %y\"))\n\n\n\n\n\n\n\n\nAgain, these breaks may not start where you want them to. Here offset= can be used as above but you can include keyword units. For example, the starting break is moved two months earlier below.\n\ndemo_date(c(\"1-May-2020\",\"30-Apr-2021\")) +\n  scale_x_date(breaks=breaks_width(\"3 months\",offset=\"-2 months\"),\n               labels=label_date(\"%b %y\"))\n\n\n\n\n\n\n\n\nMinor ticks can be added using these same strategies along with the methods from ggh4x shown above.\n\ndemo_date(c(\"1-May-2020\",\"30-Apr-2021\")) +\n  scale_x_date(breaks=breaks_width(\"3 months\",offset=\"-2 months\"),\n               minor_breaks=breaks_width(\"1 months\",offset=\"-2 months\"),\n               labels=label_date(\"%b %y\"),\n               guide=\"axis_minor\") +\n  theme(axis.ticks.length.x=unit(10,unit=\"pt\"),\n        ggh4x.axis.ticks.length.minor=rel(0.5))\n\n\n\n\n\n\n\n\nTick marks for a dates axis are modified in this post, this post, and this post."
  },
  {
    "objectID": "blog/posts/2023-3-20-Rooketal2022_Fig4/index.html",
    "href": "blog/posts/2023-3-20-Rooketal2022_Fig4/index.html",
    "title": "Rook et al. (2022) Estimated Cisco Stocking Densities",
    "section": "",
    "text": "Rook et al. (2022) analyzed historical data to answer the question of how many ciscoes are needed for stocking in the Laurentian Great Lakes. Their Figure 4 shows lake-specific estimates of fry, fall fingerling, and age-1 stocking densities used to determine the number of Cisco (Coregonus artedi) needed for stocking to mimic historical and contemporary age-1 and adult recruitment rates in Wisconsin waters of Lake Superior. I attempt to recreate their figure here.\nThe following packages are loaded for use below. Also one function from each of scales and ggtext is used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\nlibrary(ggh4x)      # for minor axis functionality\n\n\n\n\n\n\n\nNote\n\n\n\nThe figure recreated here is often called a “dynamite plot.” I understand that there is controversy around these types of plots (see here]. However, they appear often in the fisheries literature, for better or worse. With this in mind, I thought it worthwhile to some to show how one might be constructed with ggplot2."
  },
  {
    "objectID": "blog/posts/2023-3-20-Rooketal2022_Fig4/index.html#preliminaries",
    "href": "blog/posts/2023-3-20-Rooketal2022_Fig4/index.html#preliminaries",
    "title": "Rook et al. (2022) Estimated Cisco Stocking Densities",
    "section": "Preliminaries",
    "text": "Preliminaries\nRook et al. (2022) used various shades of gray to identify the time-stage (i.e., Stage) data in their Figure 4. The object below is a named vector with approximations to their color choices.\n\nsclrs &lt;- c(\"Hist. Fry\"=\"gray20\",\"Hist. Fingerling\"=\"gray50\",\"Hist. Age-1\"=\"gray80\",\n           \"Cont. Fry\"=\"gray35\",\"Cont. Fingerling\"=\"gray65\",\"Cont. Age-1\"=\"gray95\")\n\nFigure 4 is a side-by-side bar chart with confidence intervals on each bar. Sided-by-side bars require “dodging” in ggplot2 (see below). The confidence intervals also need to be “dodged” to match the bars, but they will only match if the “dodging” is pre-defined. Dodging is defined with position_dodge() using width= to determine how much the side-by-side objects will overlap. A width= of 0 will result in complete overlap, whereas a width= of 1 results in no overlap with a slight separation between bars. A value of 0.9 resulted in no overlap but touching bars as used in Rook et al. (2022).\n\npd &lt;- position_dodge(width=0.9)\n\nFinally, Rook et al. (2022) labeled each panel of their Figure 4 within the plot region, rather than as a “title” above the plot region. Thus, as in this previous post about Rook et al. (2022), the default facet labels cannot be used. An added challenge here though is that the labels they used contain plain text, italicized text, and a subscript. Thus, the “trick” used in that post cannot be easily used here.\nHere I create a very simple data frame with two variables. The first variable is Type which contains the two types of data found in dat. It is important that this variable is exactly as it is in dat (i.e., factored, levels the same) as this is the variable that is going to define the facets, or sub-panels, of Figure 4. The second variable, Label, contains the “long” labels for each panel. Here I use markdown code to create the italics (i.e., *) and the subscript (i.e., ~).4\n4 These will be used in geom_richtext() from ggtext below, rather than plotmath which I find exceedingly difficult to get correct.\ndatlbls &lt;- data.frame(Type=factor(c(\"Smax\",\"Rmax\"),levels=c(\"Smax\",\"Rmax\")),\n                      Label=c(\"Adult (*S~max~*) Equivalents\",\n                              \"Age-1 (*R~max~*) Equivalents\"))"
  },
  {
    "objectID": "blog/posts/2023-3-20-Rooketal2022_Fig4/index.html#putting-it-together",
    "href": "blog/posts/2023-3-20-Rooketal2022_Fig4/index.html#putting-it-together",
    "title": "Rook et al. (2022) Estimated Cisco Stocking Densities",
    "section": "Putting It Together",
    "text": "Putting It Together\nThe basic bar plot is constructed from the summarized data in dat using geom_col() with Lake mapped to the x-axis, MEDIAN mapped to the y-axis, and Stage mapped to the fill= color. The bars will not be sided-by-side by default; they need to be “dodged” by setting postion= to the dodge position value set above. color= is set to black to outline each bar as in Rook et al. (2022). facet_wrap() is used to separate the plots by Type, with only one column used. The x-axis was “freed” so that the axis and labels would be shown in both facets as in Rook et al. (2022).\n\nggplot() +\n  geom_col(data=dat,mapping=aes(x=Lake,y=MEDIAN,fill=Stage),\n           position=pd,color=\"black\") +\n  facet_wrap(vars(Type),ncol=1,scales=\"free_x\")\n\n\n\n\n\n\n\n\nConfidence intervals are added with geom_errorbar() with Lake again mapped to the x-axis, L95 and U95 mapped to the minimum and maximum y interval values, and Stage set as a group.5 To match the bars the confidence intervals must be dodged in the same way with postion= and the “cap” on the interval was made smaller with width=.\n5 The data needs a group here to match the group created by fill= in geom_col().\nggplot() +\n  geom_col(data=dat,mapping=aes(x=Lake,y=MEDIAN,fill=Stage),\n           position=pd,color=\"black\") +\n  geom_errorbar(data=dat,mapping=aes(x=Lake,ymin=L95,ymax=U95,group=Stage),\n                position=pd,width=0.5) +\n  facet_wrap(vars(Type),ncol=1,scales=\"free_x\")\n\n\n\n\n\n\n\n\nColors were then set with scale_fill_manual() using the sclrs vector defined above. In addition, the y-axis was given a better title, the limits were set, axis expansion was removed, major breaks (i.e., labelled) were set at intervals of 200, and minor breaks were set at intervals of 100.6\n6 These axis modifications were described in more detail in this post.\nggplot() +\n  geom_col(data=dat,mapping=aes(x=Lake,y=MEDIAN,fill=Stage),\n           position=pd,color=\"black\") +\n  geom_errorbar(data=dat,mapping=aes(x=Lake,ymin=L95,ymax=U95,group=Stage),\n                position=pd,width=0.5) +\n  facet_wrap(vars(Type),ncol=1,scales=\"free_x\") +\n  scale_fill_manual(values=sclrs) +\n  scale_y_continuous(name=\"Stocking Density (fish/ha)\",\n                     limits=c(0,1800),expand=expansion(mult=0),\n                     breaks=scales::breaks_width(200),\n                     minor_breaks=scales::breaks_width(100),\n                     guide=\"axis_minor\")\n\n\n\n\n\n\n\n\nLabels are now added to the facets with geom_richtext() from the ggtext package. Here data= is set to datlbls, which is why data=dat was used in geom_col() and geom_errobar() rather than being set in ggplot().7 The Label in datlbls is mapped to the labels, but x= and y= are defined outside of the data frame. Specifically, x= is set to 3.5 because each category is listed behind the scenes with sequential numbers beginning at 1. With six categories on the x-axis, the center will be at 3.5. y= is set to Inf to generically be set at the largest y value plotted. Thus, the labels will be centered from left-to-right (x-axis orientation) and at the top (y-axis orientation). vjust= is used to move the label down slightly (see this post), label.color=NA removes the default box around the label, and size= was used to reduce the default size slightly.\n7 If multiple data frames are used when constructing a figure, then those data frames are usually declared in the geoms rather than in ggplot().\nggplot() +\n  geom_col(data=dat,mapping=aes(x=Lake,y=MEDIAN,fill=Stage),\n           position=pd,color=\"black\") +\n  geom_errorbar(data=dat,mapping=aes(x=Lake,ymin=L95,ymax=U95,group=Stage),\n                position=pd,width=0.5) +\n  ggtext::geom_richtext(dat=datlbls,mapping=aes(label=Label),x=3.5,y=Inf,\n                        vjust=0.9,label.color=NA,size=3.5) +\n  facet_wrap(vars(Type),ncol=1,scales=\"free_x\") +\n  scale_fill_manual(values=sclrs) +\n  scale_y_continuous(name=\"Stocking Density (fish/ha)\",\n                     limits=c(0,1800),expand=expansion(mult=0),\n                     breaks=scales::breaks_width(200),\n                     minor_breaks=scales::breaks_width(100),\n                     guide=\"axis_minor\")\n\n\n\n\n\n\n\n\nFinally, the theme was modified to more closely match Figure 4 in Rook et al. (2022). Specifically, theme_class() was used as the base and facet labels were removed, the x-axis title was removed, major tick mark size was increased, minor tick mark size was set to 50% of the major tick mark size, the legend was moved to the upper right of the plot, the legend title was removed, the legend text was made smaller, and the legend box size was made smaller.\n\nggplot() +\n  geom_col(data=dat,mapping=aes(x=Lake,y=MEDIAN,fill=Stage),\n           position=pd,color=\"black\") +\n  geom_errorbar(data=dat,mapping=aes(x=Lake,ymin=L95,ymax=U95,group=Stage),\n                position=pd,width=0.5) +\n  ggtext::geom_richtext(dat=datlbls,mapping=aes(label=Label),x=3.5,y=Inf,\n                        vjust=0.9,label.color=NA,size=3.5) +\n  facet_wrap(vars(Type),ncol=1,scales=\"free_x\") +\n  scale_fill_manual(values=sclrs) +\n  scale_y_continuous(name=\"Stocking Density (fish/ha)\",\n                     limits=c(0,1800),expand=expansion(mult=0),\n                     breaks=scales::breaks_width(200),\n                     minor_breaks=scales::breaks_width(100),\n                     guide=\"axis_minor\") +\n  theme_classic() +\n  theme(strip.text=element_blank(),\n        axis.title.x=element_blank(),\n        axis.ticks.length=unit(5,units=\"pt\"),\n        ggh4x.axis.ticks.length.minor=rel(0.5),\n        legend.position=c(1,1),\n        legend.justification=c(1.05,0.95),\n        legend.title=element_blank(),\n        legend.text=element_text(size=7),\n        legend.key.size = unit(0.75,units=\"line\")\n        )\n\n\n\n\n\n\n\n\nThis largely reproduces Figure 4 in Rook et al. (2022) with the exceptions that (a) the tick marks don’t cross the x-axes and (b) the x-axis tick marks are centered on the group of bars rather than between the group of bars."
  },
  {
    "objectID": "blog/posts/2023-3-25_McCarricketal2022_Fig3/index.html",
    "href": "blog/posts/2023-3-25_McCarricketal2022_Fig3/index.html",
    "title": "McCarrick et al. (2022) PSD Plot",
    "section": "",
    "text": "Series Note\n\n\n\nThis is the second of several posts related to McCarrick et al. (2022).\n\n\n\nIntroduction\nMcCarrick et al. (2022) examined the population dynamics of Yellowstone Cutthroat Trout (Oncorhynchus clarkii bouvieri) in Henrys Lake, Idaho over a nearly two decade period. Their Figure 3 showed various proportional stock distribution (PSD) calculations of Cutthroat Trout across years. I use ggplot2 to recreate that figure here. I also modified their plot by adding confidence intervals to the calculations.\nThe following packages are loaded for use below. A few functions from each of lubridate, FSA, plyr, tidyr, scales, gghrx, and lemon are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\n\nMcCarrick et al. (2022) computed what I am calling an overall PSD1 and what are called “incremental” PSD indices. I assume that these are familar to most fisheries scientists, but they are described in more detail in Chapter 6 of Ogle (2016).2\n1 This is the most common PSD measure.2 This provides a decent description of the overall PSD. \n\n\nData Wrangling\n\nIndividual Fish Data Frame\nMcCarrick et al. (2022) provided raw data for Figure 2 as an Excel file in their Data Supplement S1. The same data wrangling, up to where catch-per-unit-effort was calculated, used in this previous post is used here and, thus, will not be discussed in detail.\n\ndat &lt;- readxl::read_xlsx(\"../2023-3-22_McCarricketal2022_Fig2/download.xlsx\",\n                         na=c(\"\",\"??\",\"QTY =\",\"QTY=\",\"UNK\",\"NO TAG\"),\n                         col_types=c(\"date\",\"numeric\",\"text\",\n                                     \"numeric\",\"numeric\",\"text\")) |&gt;\n  mutate(year=lubridate::year(Date),\n         year=ifelse(year==1905,2002,year)) |&gt;\n  filter(!is.na(year)) |&gt;\n  select(species=Species,year,length,weight)  |&gt;\n  mutate(species=case_when(\n    species %in% c(\"YCT\",\"Yct\") ~ \"YCT\",\n    species %in% c(\"UTC\",\"CHB\",\"CHUB\") ~ \"UTC\",\n    TRUE ~ species\n  )) |&gt;\n  filter(species %in% c(\"YCT\",\"UTC\")) |&gt;\n  mutate(species2=plyr::mapvalues(species,\n                                  from=c(\"YCT\",\"UTC\"),\n                                  to=c(\"Cutthroat Trout\",\"Utah Chub\"))) |&gt;\n  mutate(gcat=FSA::psdAdd(len=length,species=species2))\n\nFSA::headtail(dat)\n\n#R|        species year length weight        species2     gcat\n#R|  1         YCT 2002    150     NA Cutthroat Trout substock\n#R|  2         YCT 2002    160     NA Cutthroat Trout substock\n#R|  3         YCT 2002    160     NA Cutthroat Trout substock\n#R|  19900     YCT 2020    391    550 Cutthroat Trout  quality\n#R|  19901     YCT 2020    284    229 Cutthroat Trout    stock\n#R|  19902     YCT 2020    440    853 Cutthroat Trout  quality\n\n\n\n\nPSD Summary Data Frame\nThe data frame was filtered to only Cutthroat Trout (the only species shown in Figure 3) and sub-stock-sized fish were removed (PSD calculations do not consider sub-stock-sized fish).\n\npsd_dat &lt;- dat |&gt;\n  filter(species==\"YCT\",gcat!=\"substock\")\n\nThe calculation of PSD values begins by counting the number of fish in each of the remaining Gabelhouse length categories, within each year.\n\npsd_dat &lt;- psd_dat |&gt;\n  group_by(year,gcat) |&gt;\n  summarize(count=n()) |&gt;\n  ungroup()\nFSA::headtail(psd_dat)\n\n#R|     year      gcat count\n#R|  1  2002     stock    20\n#R|  2  2002   quality    28\n#R|  3  2002 preferred     1\n#R|  61 2020     stock    52\n#R|  62 2020   quality   132\n#R|  63 2020 preferred    13\n\n\nThis data frame was then made wider by creating columns with the length category names, each with the “count” in that category underneath it for each year.\n\npsd_dat &lt;- psd_dat |&gt;\n  pivot_wider(names_from=gcat,values_from=count)\nFSA::headtail(psd_dat)\n\n#R|     year stock quality preferred memorable trophy\n#R|  1  2002    20      28         1        NA     NA\n#R|  2  2003    31      57        43        NA     NA\n#R|  3  2004   141      36        27         1     NA\n#R|  17 2018    34      16        22         1     NA\n#R|  18 2019   176      20        17         3     NA\n#R|  19 2020    52     132        13        NA     NA\n\n\nFor example, in 2002 there were 20 stock- to quality-sized fish, 28 quality- to preferred-size fish, 1 preferred- to memorable-sized fish, and no fish in the other categories. Each PSD calculation requires the total number of stock-size and larger fish as the denominator; i.e., 49 fish in 2002. In addition, the overall PSD calculation requires the total number of quality-sized and larger fish as the numerator; i.e., 28 fish in 2002. These two quantities are computed in mutate() below, but note that rowwise() is used before that to force the calculations to be computed by row (i.e., by year).3\n3 If rowwise is not used then, for example, sum(stock+quality) would be the sum of both the stock and quality columns; i.e., the sum across all years.\npsd_dat &lt;- psd_dat |&gt;\n  rowwise() |&gt;\n  mutate(qualityplus=sum(quality,preferred,memorable,trophy,na.rm=TRUE),\n         stockplus=sum(stock,qualityplus,na.rm=TRUE))\nFSA::headtail(psd_dat)\n\n#R|     year stock quality preferred memorable trophy qualityplus stockplus\n#R|  1  2002    20      28         1        NA     NA          29        49\n#R|  2  2003    31      57        43        NA     NA         100       131\n#R|  3  2004   141      36        27         1     NA          64       205\n#R|  17 2018    34      16        22         1     NA          39        73\n#R|  18 2019   176      20        17         3     NA          40       216\n#R|  19 2020    52     132        13        NA     NA         145       197\n\n\nThe overall PSD is calculated as quality-sized and larger fish divided by stock-sized and larger fish multiplied by 100. The three incremental PSD values are calculated as the number in the incremental length group (e.g., stock- to quality-sized) divided by stock-sized and larger fish multiplied by 100. These calculations are made below within mutate().4\n4 The incremental PSD names are within single back-ticks because the name contains a space (and a hyphen).\npsd_dat &lt;- psd_dat |&gt;\n  mutate(PSD=qualityplus/stockplus*100,\n         `PSD S-Q`=stock/stockplus*100,\n         `PSD Q-P`=quality/stockplus*100,\n         `PSD P-M`=preferred/stockplus*100) |&gt;\n  ungroup()\nFSA::headtail(psd_dat)\n\n#R|     year stock quality preferred memorable trophy qualityplus stockplus      PSD\n#R|  1  2002    20      28         1        NA     NA          29        49 59.18367\n#R|  2  2003    31      57        43        NA     NA         100       131 76.33588\n#R|  3  2004   141      36        27         1     NA          64       205 31.21951\n#R|  17 2018    34      16        22         1     NA          39        73 53.42466\n#R|  18 2019   176      20        17         3     NA          40       216 18.51852\n#R|  19 2020    52     132        13        NA     NA         145       197 73.60406\n#R|      PSD S-Q   PSD Q-P   PSD P-M\n#R|  1  40.81633 57.142857  2.040816\n#R|  2  23.66412 43.511450 32.824427\n#R|  3  68.78049 17.560976 13.170732\n#R|  17 46.57534 21.917808 30.136986\n#R|  18 81.48148  9.259259  7.870370\n#R|  19 26.39594 67.005076  6.598985\n\n\nFinally, this data frame should be made longer such that the calculated PSD values will appear under one column (called values) and another column will be created with the name of the PSD metric. This process begins by restricting the data frame to the year and all calculated PSD values, then pivoting the values in all of the PSD columns into one column with the names of the PSD metric stored in metric, and then factoring metric with the levels controlled so that they will be plotted in the same order as in Figure 3. This new data frame has a new name, as the original psd_dat data frame is used further below.\n\npsd_dat2 &lt;- psd_dat |&gt;\n  select(year,contains(\"PSD\")) |&gt;\n  pivot_longer(cols=contains(\"PSD\"),names_to=\"metric\") |&gt;\n  mutate(metric=factor(metric,levels=c(\"PSD\",\"PSD S-Q\",\"PSD Q-P\",\"PSD P-M\")))\nFSA::headtail(psd_dat2)\n\n#R|     year  metric     value\n#R|  1  2002     PSD 59.183673\n#R|  2  2002 PSD S-Q 40.816327\n#R|  3  2002 PSD Q-P 57.142857\n#R|  74 2020 PSD S-Q 26.395939\n#R|  75 2020 PSD Q-P 67.005076\n#R|  76 2020 PSD P-M  6.598985\n\n\nThis data frame, now called psd_dat2, is ready for recreating Figure 3.\n \n\n\n\nRecreating Figure 3\nFigure 3 is a simple bar plot facetted across years similar to the CPE plot in this previous post. Thus, I don’t discuss the details further here.\n\nggplot(data=psd_dat2,mapping=aes(x=year,y=value)) +\n  geom_col(color=\"black\",fill=\"gray70\",width=1) +\n  geom_text(mapping=aes(label=metric),x=Inf,y=Inf,vjust=1.25,hjust=1.05,size=3,\n            check_overlap=TRUE) +\n  scale_y_continuous(name=\"PSD\",limits=c(0,100),expand=expansion(mult=0),\n                     breaks=scales::breaks_width(20)) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  lemon::facet_rep_wrap(vars(metric),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.background=element_blank(),\n        strip.text=element_blank())\n\n\n\n\n\n\n\n\n \n\n\nAdding Confidence Intervals\nI wanted to see if I could make Figure 3 as above, but add confidence intervals to the PSD calculations.5\n5 The authors added CIs to the relative weight calculations in their Figure 4, but did not do that here for their PSD calculations.As discussed in Ogle (2016) confidence intervals for a PSD can be made from binomial distribution theory using binCI() from FSA. This is a simple process of giving binCI() the number of “successes” (i.e., the numerator in the PSD calculation), the number of “trials”(i.e., the denominator), and the type of algorithm to use (we will use the “Wilson” algorithm here). For example, the CI for the overall PSD in 2002 is computed below.\n\nFSA::binCI(29,49,type=\"wilson\")*100\n\n#R|    95% LCI  95% UCI\n#R|   45.24732 71.78476\n\n\nThis becomes complicated here for several reasons:\n\nThe CIs are computed across multiple years.\nThe numerators differ among the PSD metrics (e.g., quality-sized and larger fish for the overall PSD but just quality-sized fish for PSD S-Q).\nbinCI() returns two values rather than 1 (thus, complicating the use of mutate()).\nbinCI() returns a matrix with column names rather than a named vector.\n\nGiven these issues, confidence intervals the PSD, PSD S-Q, etc. will each be calculated separately and then combined into a single data frame. I begin by calculating the CIs for the PSD.\nHere we return to the wide psd_dat data frame from above. The calculation will be for each year so again use rowwise(). FSA::binCI() will be used with mutate() but its result must first be converted to a vector with as.vector() (addresses last issue above) and then put in a list(). In this case the ci “variable” will be a list with two items (the lower and upper CI values) for each year. We want to get the two values out of this list and into their own variables, which is accomplished with unnest_wider() from tidyr (addressing the third issue above). The results from binCI() (after as.vector()) were unnamed, so names_sep= must be used in unnest_wider(). With this set to \"\", the unnested variables will be the original name (“ci”) followed by sequential numbers (i.e., “ci1” and “ci2” here). Finally,the data frame is reduced to the year, PSD, ci1, and ci2 variables, but PSD is renamed value along the way.6\n6 This renaming is necessary for bind_rows() further below.\ntmp1 &lt;- psd_dat |&gt;\n  rowwise() |&gt;\n  mutate(ci=list(as.vector(FSA::binCI(qualityplus,stockplus,type=\"wilson\")))) |&gt;\n  tidyr::unnest_wider(ci,names_sep=\"\") |&gt;\n  select(year,value=PSD,ci1,ci2)\nFSA::headtail(tmp1)\n\n#R|     year    value       ci1       ci2\n#R|  1  2002 59.18367 0.4524732 0.7178476\n#R|  2  2003 76.33588 0.6837273 0.8279848\n#R|  3  2004 31.21951 0.2527076 0.3785916\n#R|  17 2018 53.42466 0.4209895 0.6440796\n#R|  18 2019 18.51852 0.1390441 0.2423283\n#R|  19 2020 73.60406 0.6703995 0.7926523\n\n\nThis exact code is repeated for PSD S-Q but making sure that stock is the first argument to binCI(), the new value variable comes from PSD S-Q, and the resulting data frame is given a different name.\n\ntmp2 &lt;- psd_dat |&gt;\n  rowwise() |&gt;\n  mutate(ci=list(as.vector(FSA::binCI(stock,stockplus,type=\"wilson\")))) |&gt;\n  tidyr::unnest_wider(ci,names_sep=\"\") |&gt;\n  select(year,value=`PSD S-Q`,ci1,ci2)\n\nThis process is repeated for the other two metrics.\n\ntmp3 &lt;- psd_dat |&gt;\n  rowwise() |&gt;\n  mutate(ci=list(as.vector(FSA::binCI(quality,stockplus,type=\"wilson\")))) |&gt;\n  tidyr::unnest_wider(ci,names_sep=\"\") |&gt;\n  select(year,value=`PSD Q-P`,ci1,ci2)\n\n\ntmp4 &lt;- psd_dat |&gt;\n  rowwise() |&gt;\n  mutate(ci=list(as.vector(FSA::binCI(preferred,stockplus,type=\"wilson\")))) |&gt;\n  tidyr::unnest_wider(ci,names_sep=\"\") |&gt;\n  select(year,value=`PSD P-M`,ci1,ci2)\n\nThese four temporary data frames are bound together with a metric variable added to indicate which PSD metric appears in each row of the new data frame. Additionally, ci1 and ci2 were renamed to LCI and UCI for clarity, each CI endpoint was muliplied by 100 to put it on the same scale as the point estimates (i.e., percentages rather than proportions returned from binCI()), and metric was factored with controlled levels as above.\n\npsd_dat3 &lt;- bind_rows(list(\"PSD\"=tmp1,\"PSD S-Q\"=tmp2,\"PSD Q-P\"=tmp3,\"PSD P-M\"=tmp4),\n                      .id=\"metric\") |&gt;\n  rename(LCI=`ci1`,UCI=`ci2`) |&gt;\n  mutate(LCI=LCI*100,UCI=UCI*100,\n         metric=factor(metric,levels=c(\"PSD\",\"PSD S-Q\",\"PSD Q-P\",\"PSD P-M\")))\nFSA::headtail(psd_dat3)\n\n#R|      metric year     value       LCI      UCI\n#R|  1      PSD 2002 59.183673 45.247319 71.78476\n#R|  2      PSD 2003 76.335878 68.372730 82.79848\n#R|  3      PSD 2004 31.219512 25.270764 37.85916\n#R|  74 PSD P-M 2018 30.136986 20.822591 41.43737\n#R|  75 PSD P-M 2019  7.870370  4.971693 12.24137\n#R|  76 PSD P-M 2020  6.598985  3.896693 10.96152\n\n\nThe same code used above to recreate Figure 3 is repeated below, but geom_errorbar() is used with LCI mapped to ymin= and UCI mapped to ymax= to form the confidence intervals. geom_errorbar() is before geom_col() which gives the appearance of only showing the upper portion of the confidence interval (i.e., the lower portion is behind the bar). width=0.25 was used to narrow the “caps” on the intervals.\n\nggplot(data=psd_dat3,mapping=aes(x=year,y=value)) +\n  geom_errorbar(mapping=aes(ymin=LCI,ymax=UCI),width=0.25) +\n  geom_col(color=\"black\",fill=\"gray70\",width=1) +\n  geom_text(mapping=aes(label=metric),x=Inf,y=Inf,vjust=1.25,hjust=1.05,size=3,\n            check_overlap=TRUE) +\n  scale_y_continuous(name=\"PSD\",limits=c(0,100),expand=expansion(mult=0),\n                     breaks=scales::breaks_width(20)) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  lemon::facet_rep_wrap(vars(metric),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.background=element_blank(),\n        strip.text=element_blank())\n\n\n\n\n\n\n\n\n \n\n\nFurther Thoughts\n\nPoint-and-Lines Plot\nAs mentioned in this previous post I understand that these are the much derided “dynamite plots”. Personally, I find the bars distracting (so much gray with little purpose) and find a point-and-lines plot more appealing.\n\nggplot(data=psd_dat3,mapping=aes(x=year,y=value)) +\n  geom_errorbar(mapping=aes(ymin=LCI,ymax=UCI),linewidth=0.5,width=0.25) +\n  geom_line(linewidth=0.75,color=\"gray70\") +\n  geom_point(size=1) +\n  geom_text(mapping=aes(label=metric),x=Inf,y=Inf,vjust=1.25,hjust=1.05,size=3,\n            check_overlap=TRUE) +\n  scale_y_continuous(name=\"PSD\",limits=c(0,100),expand=expansion(mult=0),\n                     breaks=scales::breaks_width(20)) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  lemon::facet_rep_wrap(vars(metric),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.background=element_blank(),\n        strip.text=element_blank())\n\n\n\n\n\n\n\n\n\n\nLoess Smoother\nA loess smoother could also be added with geom_smooth() to highlight any trends (or lack thereof).\n\nggplot(data=psd_dat3,mapping=aes(x=year,y=value)) +\n  geom_errorbar(mapping=aes(ymin=LCI,ymax=UCI),linewidth=0.5,width=0.25) +\n  geom_line(linewidth=0.75,color=\"gray70\") +\n  geom_point(size=1) +\n  geom_smooth(se=FALSE,color=\"gray30\",linetype=\"dashed\",linewidth=0.5) +\n  geom_text(mapping=aes(label=metric),x=Inf,y=Inf,vjust=1.25,hjust=1.05,size=3,\n            check_overlap=TRUE) +\n  scale_y_continuous(name=\"PSD\",limits=c(0,100),expand=expansion(mult=0),\n                     breaks=scales::breaks_width(20)) +\n  scale_x_continuous(name=\"Year\",\n                     limits=c(2000,2022),breaks=scales::breaks_width(2),\n                     expand=expansion(mult=0)) +\n  lemon::facet_rep_wrap(vars(metric),ncol=1) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        strip.background=element_blank(),\n        strip.text=element_blank())\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nReferences\n\nMcCarrick, D. K., J. C. Dillon, B. High, and M. C. Quist. 2022. Population dynamics of Yellowstone Cutthroat Trout in Henrys Lake, Idaho. Journal of Fish and Wildlife Management 13(1):169–181.\n\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {McCarrick Et Al. (2022) {PSD} {Plot}},\n  date = {2023-03-25},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-3-25_McCarricketal2022_Fig3},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, March 25. McCarrick et al. (2022) PSD Plot. https://fishr-core-team.github.io/fishR//blog/posts/2023-3-25_McCarricketal2022_Fig3."
  },
  {
    "objectID": "blog/posts/2023-3-27_McCarricketal2022_Fig5/index.html",
    "href": "blog/posts/2023-3-27_McCarricketal2022_Fig5/index.html",
    "title": "McCarrick et al. (2022) Age Composition Plot",
    "section": "",
    "text": "Series Note\n\n\n\nThis is the fourth of several posts related to McCarrick et al. (2022).\n\n\n\nIntroduction\nMcCarrick et al. (2022) examined the population dynamics of Yellowstone Cutthroat Trout (Oncorhynchus clarkii bouvieri) in Henrys Lake, Idaho over a nearly two decade period. Their Figure 5 showed the relative age composition of Cutthroat Trout across years. I use ggplot2 to recreate that figure here.\nThe following packages are loaded for use below. A few functions from each of FSA, plyr, lemon, and forcats are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\n\n \n\n\nData Wrangling\nMcCarrick et al. (2022) provided raw data for Figure 2 in a MSWord table as a supplement. I copied the table from Word, pasted it into Excel, and saved it for loading here.\n\ndat &lt;- readxl::read_xlsx(\"YCT_AgeComp.xlsx\")\ndat\n\n#R|  # A tibble: 19 × 12\n#R|      Year `Age 1` `Age 2` `Age 3` `Age 4` `Age 5` `Age 6` `Age 7` `Age 8` `Age 9`\n#R|     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#R|   1  2002   0.113   0.46    0.394   0.034   0       0       0       0       0    \n#R|   2  2003   0.207   0.207   0.194   0.289   0.058   0.025   0       0       0.019\n#R|   3  2004   0.39    0.472   0.084   0.034   0.003   0.007   0.007   0.003   0    \n#R|   4  2005   0.368   0.527   0.078   0.015   0.013   0       0       0       0    \n#R|   5  2006   0.091   0.276   0.52    0.107   0.002   0.004   0       0       0    \n#R|   6  2007   0.025   0.708   0.13    0.128   0.009   0       0       0       0    \n#R|   7  2008   0       0.536   0.298   0.146   0.021   0       0       0       0    \n#R|   8  2009   0.051   0.27    0.502   0.177   0       0       0       0       0    \n#R|   9  2010   0.028   0.753   0.092   0.116   0.008   0.003   0       0       0    \n#R|  10  2011   0.003   0.774   0.187   0.029   0.005   0.003   0       0       0    \n#R|  11  2012   0.017   0.594   0.348   0.029   0.004   0.006   0       0       0    \n#R|  12  2013   0.004   0.61    0.22    0.14    0.02    0.002   0.004   0       0    \n#R|  13  2014   0.007   0.353   0.437   0.136   0.05    0.013   0.002   0.002   0    \n#R|  14  2015   0.021   0.399   0.239   0.271   0.031   0.04    0       0       0    \n#R|  15  2016   0       0.594   0.238   0.082   0.053   0.033   0       0       0    \n#R|  16  2017   0.02    0.518   0.197   0.197   0.027   0.035   0       0.007   0    \n#R|  17  2018   0.053   0.137   0.505   0.148   0.101   0.013   0.031   0.013   0    \n#R|  18  2019   0.013   0.765   0.127   0.037   0.027   0.014   0.004   0.013   0    \n#R|  19  2020   0.005   0.206   0.677   0.096   0.003   0.005   0.005   0.002   0    \n#R|  # … with 2 more variables: `Age 10` &lt;dbl&gt;, `Age 11` &lt;dbl&gt;\n\n\nThe data were provided in a wide format that needs to be “tidied” to a long format with the proportions at age in the Age X columns stacked on top of each other with a corresponding variable that indicates the age. Note that the columns to pivot in cols= were selected by ignoring year and, thus, leaving all of the Age X columns.1\n1 The columns also could have been selected with starts_with(\"Age\"), among other possibilities.\ndat &lt;- dat |&gt;\n  pivot_longer(cols=-Year,names_to=\"Age\",values_to=\"Proportion\")\nFSA::headtail(dat)\n\n#R|      Year    Age Proportion\n#R|  1   2002  Age 1      0.113\n#R|  2   2002  Age 2      0.460\n#R|  3   2002  Age 3      0.394\n#R|  207 2020  Age 9      0.000\n#R|  208 2020 Age 10      0.001\n#R|  209 2020 Age 11      0.000\n\n\nThe Age variable should be converted to a factor so that the levels can be controlled, otherwise “Age 10” will follow “Age 1”.\n\ndat &lt;- dat |&gt;\n  mutate(Age=factor(Age,levels=c(\"Age 1\",\"Age 2\",\"Age 3\",\"Age 4\",\"Age 5\",\"Age 6\",\n                                 \"Age 7\",\"Age 8\",\"Age 9\",\"Age 10\",\"Age 11\")))\n\nThis data frame is ready for recreating Figure 5.\n \n\n\nRecreating Figure 5\nFigure 5 is an “area plot” which is easily constructed in ggplot2 with geom_area(), with the variable that defines the colored areas (i.e., Age) mapped to fill=.2\n2 I plotted every other on the x-axis as the font needed to be so small to show every year at in McCarrick et al. (2022).\nggplot(data=dat,mapping=aes(x=Year,y=Proportion,fill=Age)) +\n  geom_area() +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  theme_bw() +\n  theme(panel.grid=element_blank())\n\n\n\n\n\n\n\n\nI won’t address color here (but see further below) as these colors are no uglier than what is shown in the published Figure 5. However, there are other adjustments that need to be made to match Figure 5.\nFirst, the ages above are stacked opposite of what is in Figure 5; i.e., age-1 is at the top, age-2 below that, etc. rather than age-1 at the bottom, age-2 on top of that, etc. The order of stacking is reversed with position= in geom_area() as shown below. Second, the authors included two digits for the values on the y-axis for some reason. Assuming this behavior is desired it can be obtained with scales::label_number() in scale_y_continuous() as shown below.3\n3 This was demonstrated in this post.\nggplot(data=dat,mapping=aes(x=Year,y=Proportion,fill=Age)) +\n  geom_area(position=position_stack(reverse=TRUE)) +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1),\n                     label=scales::label_number(0.01)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  theme_bw() +\n  theme(panel.grid=element_blank())\n\n\n\n\n\n\n\n\nThird, the legend needs to be moved to below the x-axis. This is easily accomplished with legend.position=\"bottom\" in theme(). However, doing this alone shows the legend in three rows. One row was used for the legend as shown below with guides(). Finally, other theme options were altered to remove the legend title and make the legend symbols and text smaller.\n\nggplot(data=dat,mapping=aes(x=Year,y=Proportion,fill=Age)) +\n  geom_area(position=position_stack(reverse=TRUE)) +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1),\n                     label=scales::label_number(0.01)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  guides(fill=guide_legend(nrow=1)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        legend.position=\"bottom\",\n        legend.title=element_blank(),\n        legend.key.size=unit(2,\"mm\"),\n        legend.text=element_text(size=7))\n\n\n\n\n\n\n\n\nThis largely recreates Figure 5 in McCarrick et al. (2022).\n \n\n\nPossible Modifications\nArea plots are notorious for being difficult for readers to interpret.4 Below I offer some modifications that may(?) aid interpretation.\n4 See this for example.\nLump Ages\nMany potential ages are shown in Figure 5 though ages greater than age-5 are rarely visible (or noticeable) as they never represent more than 10% in total or 4% individually of the fish in any given year.\n\n# Find maximum proportion by age, is that more than 5%\ndat |&gt; group_by(Age) |&gt; summarize(maxP=max(Proportion))\n\n#R|  # A tibble: 11 × 2\n#R|     Age     maxP\n#R|     &lt;fct&gt;  &lt;dbl&gt;\n#R|   1 Age 1  0.39 \n#R|   2 Age 2  0.774\n#R|   3 Age 3  0.677\n#R|   4 Age 4  0.289\n#R|   5 Age 5  0.101\n#R|   6 Age 6  0.04 \n#R|   7 Age 7  0.031\n#R|   8 Age 8  0.013\n#R|   9 Age 9  0.019\n#R|  10 Age 10 0.002\n#R|  11 Age 11 0.001\n\n\nThe number of colors used in the plot, and the number of areas that the reader looks for, can be reduced by lumping together ages that are not well-represented in the data. Below fct_collapse() from forcats is used to collapse the “Age 6” to “Age 11” levels in Age to one level called “Age 6+”. To preserve the original data this data frame was called dat2.\n\ndat2 &lt;- dat |&gt;\n  mutate(Age=forcats::fct_collapse(Age,`Age 6+`=c(\"Age 6\",\"Age 7\",\"Age 7\",\"Age 8\",\n                                                  \"Age 9\",\"Age 10\",\"Age 11\")))\nFSA::headtail(dat2)\n\n#R|      Year    Age Proportion\n#R|  1   2002  Age 1      0.113\n#R|  2   2002  Age 2      0.460\n#R|  3   2002  Age 3      0.394\n#R|  207 2020 Age 6+      0.000\n#R|  208 2020 Age 6+      0.001\n#R|  209 2020 Age 6+      0.000\n\n\nHowever, “Age 6+” was simply repeated for the original “Age 6”, “Age 7”, etc. The proportions for these ages should be summed, within each year, to get a proper proportion of “Age 6+” fish.5\n5 Summing the proportions for the ages that were lumped will just return the proportion for those ages.\ndat2 &lt;- dat2 |&gt;\n  group_by(Year,Age) |&gt;\n  summarize(Proportion=sum(Proportion)) |&gt;\n  ungroup()\nFSA::headtail(dat2)\n\n#R|      Year    Age Proportion\n#R|  1   2002  Age 1      0.113\n#R|  2   2002  Age 2      0.460\n#R|  3   2002  Age 3      0.394\n#R|  112 2020  Age 4      0.096\n#R|  113 2020  Age 5      0.003\n#R|  114 2020 Age 6+      0.013\n\n\nTo save some typing below, I set the theme to the theme items above with the exception that I moved the legend back to the right where I think it is more appropriate.\n\ntheme_set(\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        legend.title=element_blank(),\n        legend.key.size=unit(3,\"mm\"),\n        legend.text=element_text(size=8))\n)\n\nThe code for Figure 5 is largely repeated below with these data. However, I reversed the order of the labels in the legend with guides() so that the labels more naturally followed how the areas were plotted and I removed the unnecessary extra digit in the y-axis labels.\n\nggplot(data=dat2,mapping=aes(x=Year,y=Proportion,fill=Age)) +\n  geom_area(position=position_stack(reverse=TRUE)) +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  guides(fill=guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\n\n\nColor\nChanging the color is not necessarily going to make the figure easier to interpret. However, a gradient of color can be used to more clearly represent the gradient of age. Here, I form a gradient of grays with scale_fill_grey().\n\nggplot(data=dat2,mapping=aes(x=Year,y=Proportion,fill=Age)) +\n  geom_area(position=position_stack(reverse=TRUE)) +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  scale_fill_grey(start=0.2,end=0.8) +\n  guides(fill=guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\nA sequential palette that ranges from dark green to light blue is used below with scale_fill_brewer(). A variety of palettes may be chosen in this function as described in its documentation. Note that direction=-1 was used here to reverse the palette order as I preferred the darker colors for age-1 and the lighter colors for age-6+.\n\nggplot(data=dat2,mapping=aes(x=Year,y=Proportion,fill=Age)) +\n  geom_area(position=position_stack(reverse=TRUE)) +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  scale_fill_brewer(palette=\"BuGn\",direction=-1) +\n  guides(fill=guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\nOther palette algorithms can also be used. Below, scale_fill_viridis_d() is used as an example.\n\nggplot(data=dat2,mapping=aes(x=Year,y=Proportion,fill=Age)) +\n  geom_area(position=position_stack(reverse=TRUE)) +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  scale_fill_viridis_d(begin=0.25,end=0.9) +\n  guides(fill=guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere is both an art and a science behind choosing colors for plots. Unfortunately, I am not strong nor fully knowledgeable in either. Thus, I show how to alter colors here, but I don’t claim that these choices are ideal.\n\n\n\n\nAdd Lines\nI sometimes feel that the colors in area plots “run together” in my eyes. One way to make more clear “breaks” is to highlight the color changes. Below I also map Age to color=, use scale_color_viridis_d() with the same options as scale_fill_viridis_d(), except that for the fill I set a slight transparency with alpha=. Here color= is used for the boundaries of the areas and since the fill was made transparent these boundaries are a bit darker and, thus, appear as lines between the areas. Further note that both fill= and color= had to be “reversed” in guides().\n\nggplot(data=dat2,mapping=aes(x=Year,y=Proportion,fill=Age,color=Age)) +\n  geom_area(position=position_stack(reverse=TRUE),linewidth=0.75) +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  scale_color_viridis_d(begin=0.25,end=0.9) +\n  scale_fill_viridis_d(begin=0.25,end=0.9,alpha=0.75) +\n  guides(fill=guide_legend(reverse=TRUE),\n         color=guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\n\n\nBar Chart\nMost people that eschew the use of area plots suggests using bar charts instead. This is easily accomplished here by replacing geom_area() with geom_col(). However, the limits of the x-axis will need to be extended by one year on both sides so that the first and last year will be shown correctly.\n\nggplot(data=dat2,mapping=aes(x=Year,y=Proportion,fill=Age)) +\n  geom_col(position=position_stack(reverse=TRUE)) +\n  scale_y_continuous(expand=expansion(mult=0),\n                     breaks=scales::breaks_width(0.1)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2001,2021),breaks=scales::breaks_width(2)) +\n  scale_fill_viridis_d(begin=0.25,end=0.9) +\n  guides(fill=guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\n\n\nFacets\nFinally, I think that if the goal is to show annular trends for the specific ages, that they should be plotted as lines for each age. Here lines are used with geom_line() and faceting was used on Age. The use of color is superfluous here, but I kept it for comparing with the previous options.\n\nggplot(data=dat2,mapping=aes(x=Year,y=Proportion,color=Age)) +\n  geom_line(linewidth=0.75) +\n  scale_y_continuous(expand=expansion(mult=c(0,0.025)),\n                     breaks=scales::breaks_width(0.1)) +\n  scale_x_continuous(expand=expansion(mult=0),\n                     limits=c(2002,2020),breaks=scales::breaks_width(2)) +\n  scale_color_viridis_d(begin=0.25,end=0.9) +\n  lemon::facet_rep_wrap(vars(Age),ncol=1,strip.position=\"right\") +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nIn my view, it is much easier to decipher what is going on with age-2 to age-4 fish from this plot.\n \n\n\n\n\n\n\n\n\n\n\nReferences\n\nMcCarrick, D. K., J. C. Dillon, B. High, and M. C. Quist. 2022. Population dynamics of Yellowstone Cutthroat Trout in Henrys Lake, Idaho. Journal of Fish and Wildlife Management 13(1):169–181.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {McCarrick Et Al. (2022) {Age} {Composition} {Plot}},\n  date = {2023-03-27},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-3-27_McCarricketal2022_Fig5},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, March 27. McCarrick et al. (2022) Age Composition\nPlot. https://fishr-core-team.github.io/fishR//blog/posts/2023-3-27_McCarricketal2022_Fig5."
  },
  {
    "objectID": "blog/posts/2023-3-29_Milleretal2022_Fig1/index.html",
    "href": "blog/posts/2023-3-29_Milleretal2022_Fig1/index.html",
    "title": "Miller et al. (2022) CPE Plot",
    "section": "",
    "text": "Series Note\n\n\n\nThis is the first of several posts related to Miller et al. (2022). I thank the authors for making their data available with their publication.\n\n\n\nIntroduction\nMiller et al. (2022) examined life history characteristics of Goldeye (Hiodon alosoides) in two Kansas reservoirs. Their Figure 1 displayed a time series of catch-per-unit-effort (CPE) from 1997 to 2020 for both reservoirs, including a regression line to demonstrate the decline in abundance. I use ggplot2 here to recreate both figures.\nThe following packages are loaded for use below. A few functions from each of readxl, FSA, scales, and rstatix are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\nlibrary(ggtext)     # for use of markdown in text/labels\n\n \n\n\nData Wrangling\nMiller et al. (2022) provided the raw data for producing Figure 1 in the Data Supplement S1, which I loaded below. I converted the “reservoir” (impd) variable to a factor, reordered the levels so that the facets created below would be in the order used by the authors, and replaced the reservoir abbreviations with their longer names/labels.\n\ndat &lt;- read.csv(\"JFWM-21-090.S1.csv\") |&gt;\n  mutate(impd=factor(impd,levels=c(\"MILR\",\"LOVR\"),labels=c(\"Milford\",\"Lovewell\")))\n\nFSA::headtail(dat)\n\n#R|         impd   yr fish nets net_area_m2   catch_m2\n#R|  1  Lovewell 1997  496   16    1189.158 0.41710171\n#R|  2  Lovewell 1998  499   16    1189.158 0.41962450\n#R|  3  Lovewell 1999  324   16    1189.158 0.27246160\n#R|  45  Milford 2017   73   20     891.870 0.08185049\n#R|  46  Milford 2018   10   20     891.870 0.01121240\n#R|  47  Milford 2020   49   20     891.870 0.05494074\n\n\nI double-checked to make sure that the provided CPE variable (catch_m2) was equal to catch (fish) divided by the areal effort (net_area_m2).1\n1 all_equal() checks equality between two vectors with a tolerance for machine precision.\nall.equal(dat$fish/dat$net_area_m2,dat$catch_m2)\n\n#R|  [1] TRUE\n\n\nThese data are ready to recreate Figure 1.\n \n\n\nRecreating Figure 1\n\nBasic Plot with Regression Line\nThe basic time series plot is created with geom_line() by mapping yr to the x-axis and catch_m2 to the y-axis, and using a shade of gray and slightly thicker line to match the authors’ choices. A linear regression is added with geom_smooth() using method=\"lm\".2 Here se=FALSE was used so a confidence band is not shown, and the line was changed to dashed, made black,3 and made a little thicker than the time series line to match the authors’ choices. A facet based on impd was created with ncol=1 to stack the two plots and scales=\"free_x\" so that labeled ticks would appear on the x-axis of both facets. Finally, the y-axis was labelled (more on this below), both axis limits were controlled and were expanded in a way to try to match the authors’ choices, and the basic black-and-white theme was added.\n2 The default is to add a loess smoother, lm is for a linear model.3 It defaults to blue.\nggplot(data=dat,mapping=aes(x=yr,y=catch_m2)) +\n  geom_line(color=\"gray50\",linewidth=1) +\n  geom_smooth(method=\"lm\",se=FALSE,linetype=\"dashed\",color=\"black\",linewidth=1.2) +\n  facet_wrap(vars(impd),ncol=1,scales=\"free_x\") +\n  scale_y_continuous(name=\"Catch/m^2^\",expand=expansion(mult=c(0.02,0.10))) +\n  scale_x_continuous(limits=c(1995,2020),expand=expansion(mult=0.03)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe portion of the regression line for Lovewell that extends below 0 was cut off in the published Figure 1. It is tempting to accomplish this by limiting the extent of the y-axis with limits= in scale_y_continuous() as has been done in other posts. However, in this case, that will remove the years for which the regression extends below zero from the data used to compute the regression, effectively altering the results. The better way to handle this is to limit the extent of the y-axis with ylim= in coord_cartesian(). With this all data is used in the regression, but the axis is clipped for viewing.\n\nggplot(data=dat,mapping=aes(x=yr,y=catch_m2)) +\n  geom_line(color=\"gray50\",linewidth=1) +\n  geom_smooth(method=\"lm\",se=FALSE,linetype=\"dashed\",color=\"black\",linewidth=1.2) +\n  facet_wrap(vars(impd),ncol=1,scales=\"free_x\") +\n  scale_y_continuous(name=\"Catch/m^2^\",expand=expansion(mult=c(0.02,0.10))) +\n  scale_x_continuous(limits=c(1995,2020),expand=expansion(mult=0.03)) +\n  coord_cartesian(ylim=c(0,0.4)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe theme was then modified to try to match choices made by the authors. Specifically, the grid lines were removed; the x-axis title was removed; the axes tick mark labels were made slightly larger, bold, and black; and the facet strip labels were removed. In addition, the y-axis title was formatted with element_markdown() which forces any markdown language code in the title to be rendered appropriately. In this case, the “carets” around the “2” in name= (in scale_y_continuous()) is markup code to superscript the “2.” Other arguments to element_markdown() are treated the same as those in element_text(). Thus, the y-axis title was also made slightly larger and bold.\n\nggplot(data=dat,mapping=aes(x=yr,y=catch_m2)) +\n  geom_line(color=\"gray50\",linewidth=1) +\n  geom_smooth(method=\"lm\",se=FALSE,linetype=\"dashed\",color=\"black\",linewidth=1.2) +\n  facet_wrap(vars(impd),ncol=1,scales=\"free_x\") +\n  scale_y_continuous(name=\"Catch/m^2^\",expand=expansion(mult=c(0.02,0.10))) +\n  scale_x_continuous(limits=c(1995,2020),expand=expansion(mult=0.03)) +\n  coord_cartesian(ylim=c(0,0.4)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\"),\n        strip.text=element_blank(),\n        axis.title.y=element_markdown(size=12,face=\"bold\"))\n\n\n\n\n\n\n\n\n\n\nReservoir and Regression Result Labels\nThe last part to add to the plot is the reservoir label along with the significance (of the slope) and “r-squared” value from the regression. This takes a little bit of work.\nFirst, the regressions for each reservoir were fit “outside” of ggplot and the results returned from summary() were saved to individual objects.\n\nmreg &lt;- summary(lm(catch_m2~yr,data=filter(dat,impd==\"Milford\")))\nlreg &lt;- summary(lm(catch_m2~yr,data=filter(dat,impd==\"Lovewell\")))\n\nThese objects contain the pertinent p-value and r-squared, which will be extracted for use. The p-value is in the second row and Pr(&gt;|t|) column of the $coefficients portion of the results. These values were extracted from both regression results, saved into a vector ps, and then that vector was given to p_format() from rstatix for formatting. This particular use of p_format() will include the p= or p&lt; label, will use one significant digit, and will convert any p-value that is less than 0.001 to “p&lt;0.001”.\n\n( ps &lt;- c(mreg$coefficients[2,\"Pr(&gt;|t|)\"],lreg$coefficients[2,\"Pr(&gt;|t|)\"]) )\n\n#R|  [1] 7.148242e-03 3.053910e-08\n\n( ps &lt;-rstatix::p_format(ps,add.p=TRUE,accuracy=0.001,digits=1) )\n\n#R|  [1] \"p=0.007\" \"p&lt;0.001\"\n\n\nThe r-squared values are stored in $r.squared of the regression results and are extracted below and placed into a vector rs. They are then rounded to three decimal places and appended to r^2^ which is markdown language for r2.\n\n( rs &lt;- c(mreg$r.squared,lreg$r.squared) )\n\n#R|  [1] 0.2971323 0.7588716\n\n( rs &lt;- paste0(\"r^2^=\",round(rs,3)) )\n\n#R|  [1] \"r^2^=0.297\" \"r^2^=0.759\"\n\n\nFinally, a small data frame was created that had the reservoir names (in factor format to match the original data frame) and the ps and rs pasted together with &lt;br&gt; in between in a variable called lbls. The &lt;br&gt; will force a “line break” when the markdown language is rendered, thus forming a label with the r-squared value beneath to p-value result.\n\ndlbls &lt;- data.frame(impd=factor(c(\"Milford\",\"Lovewell\"),levels=c(\"Milford\",\"Lovewell\")),\n                    lbls=paste0(ps,\"&lt;br&gt;\",rs))\ndlbls\n\n#R|        impd                  lbls\n#R|  1  Milford p=0.007&lt;br&gt;r^2^=0.297\n#R|  2 Lovewell p&lt;0.001&lt;br&gt;r^2^=0.759\n\n\nThe reservoir names are added with geom_text() below as described in this post. Note here that size=13/.pt will use a 13 pt font and that fontface=\"bold\" will make the font bold. geom_richtext() is very similar to geom_text() except that it will render the markdown code appropriately. By default geom_richtext() places a box around the text, which is removed with label.color=NA. The reservoir label and the regression statistics were placed on the plot separately because the authors used a larger font for the reservoir label.\n\nggplot(data=dat,mapping=aes(x=yr,y=catch_m2)) +\n  geom_line(color=\"gray50\",linewidth=1) +\n  geom_smooth(method=\"lm\",se=FALSE,linetype=\"dashed\",color=\"black\",linewidth=1.2) +\n  geom_text(mapping=aes(label=impd),\n            x=2014,y=Inf,vjust=1.3,hjust=0,size=13/.pt,fontface=\"bold\",\n            check_overlap=TRUE) +\n  geom_richtext(data=dlbls,mapping=aes(label=lbls),\n                x=2014,y=Inf,vjust=1.5,hjust=0,size=10/.pt,fontface=\"bold\",\n                label.color=NA) +\n  facet_wrap(vars(impd),ncol=1,scales=\"free_x\") +\n  scale_y_continuous(name=\"Catch/m^2^\",expand=expansion(mult=c(0.02,0.10))) +\n  scale_x_continuous(limits=c(1995,2020),expand=expansion(mult=0.03)) +\n  coord_cartesian(ylim=c(0,0.4)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\"),\n        strip.text=element_blank(),\n        axis.title.y=element_markdown(size=12,face=\"bold\"))\n\n\n\n\n\n\n\n\n \n\n\n\nFurther Thoughts\n\nConfidence Band\nIn the authors’ Figure 5 they included a confidence band around the fitted von Bertalanffy growth curve. They did not include a confidence band in their Figure 1. A confidence band can be added to Figure 1 by simply removing se=FALSE from geom_smooth().4\n4 The default is se=TRUE which adds the confidence band.\nggplot(data=dat,mapping=aes(x=yr,y=catch_m2)) +\n  geom_line(color=\"gray50\",linewidth=1) +\n  geom_smooth(method=\"lm\",linetype=\"dashed\",color=\"black\",linewidth=1.2) +\n  geom_text(mapping=aes(label=impd),\n            x=2014,y=Inf,vjust=1.3,hjust=0,size=13/.pt,fontface=\"bold\",\n            check_overlap=TRUE) +\n  geom_richtext(data=dlbls,mapping=aes(label=lbls),\n                x=2014,y=Inf,vjust=1.5,hjust=0,size=10/.pt,fontface=\"bold\",\n                label.color=NA) +\n  facet_wrap(vars(impd),ncol=1,scales=\"free_x\") +\n  scale_y_continuous(name=\"Catch/m^2^\",expand=expansion(mult=c(0.02,0.10))) +\n  scale_x_continuous(limits=c(1995,2020),expand=expansion(mult=0.03)) +\n  coord_cartesian(ylim=c(0,0.4)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\"),\n        strip.text=element_blank(),\n        axis.title.y=element_markdown(size=12,face=\"bold\"))\n\n\n\n\n\n\n\n\n\n\nInformation Label\nThe process of adding the reservoir name with the regression results above was hacky because the reservoir name was in a larger font then the regression results. This, can, however be solved more elegantly, but more verbosely, using HTML code in geom_richtext().5 Here, though, the reservoir label and the regression results need to be wrapped in paired &lt;span&gt; and &lt;\\span&gt; couplets, with &lt;span&gt; including a style= that sets the font size as shown below.\n5 Markdown is flexible enough to render HTML code appropriately.\ntmp &lt;- c(\"Milford\",\"Lovewell\")\ndlbls &lt;- data.frame(impd=factor(tmp,levels=tmp),\n                    lbls=paste0(\"&lt;span style='font-size:13pt;'&gt;\",tmp,\"&lt;/span&gt;&lt;br&gt;\",\n                                \"&lt;span style='font-size:10pt;'&gt;\",ps,\"&lt;br&gt;\",\n                                rs,\"&lt;/span&gt;\"))\ndlbls\n\n#R|        impd\n#R|  1  Milford\n#R|  2 Lovewell\n#R|                                                                                                           lbls\n#R|  1  &lt;span style='font-size:13pt;'&gt;Milford&lt;/span&gt;&lt;br&gt;&lt;span style='font-size:10pt;'&gt;p=0.007&lt;br&gt;r^2^=0.297&lt;/span&gt;\n#R|  2 &lt;span style='font-size:13pt;'&gt;Lovewell&lt;/span&gt;&lt;br&gt;&lt;span style='font-size:10pt;'&gt;p&lt;0.001&lt;br&gt;r^2^=0.759&lt;/span&gt;\n\n\nWith this new dlbls data frame, the geom_text() from above that put on the reservoir labels can be removed and, in geom_richtext(), the size= should be removed as the font size is set in dlbls and vjust= should be adjusted up a little bit.\n\nggplot(data=dat,mapping=aes(x=yr,y=catch_m2)) +\n  geom_line(color=\"gray50\",linewidth=1) +\n  geom_smooth(method=\"lm\",se=FALSE,linetype=\"dashed\",color=\"black\",linewidth=1.2) +\n  geom_richtext(data=dlbls,mapping=aes(label=lbls),\n                x=2014,y=Inf,vjust=1.1,hjust=0,fontface=\"bold\",\n                label.color=NA) +\n  facet_wrap(vars(impd),ncol=1,scales=\"free_x\") +\n  scale_y_continuous(name=\"Catch/m^2^\",expand=expansion(mult=c(0.02,0.10))) +\n  scale_x_continuous(limits=c(1995,2020),expand=expansion(mult=0.03)) +\n  coord_cartesian(ylim=c(0,0.4)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title.x=element_blank(),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\"),\n        strip.text=element_blank(),\n        axis.title.y=element_markdown(size=12,face=\"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you know a little bit of markdown or HTML, then ggtext is a simple but powerful package for handling these kinds of issues. See more about ggtext here.\n\n\n \n\n\n\n\n\n\n\n\n\n\nReferences\n\nMiller, B. T., E. Flores, D. S. Waters, and B. C. Neely. 2022. An evaluation of Goldeye life history characteristics in two Kansas reservoirs. Journal of Fish and Wildlife Management 13(1):243–249.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Miller Et Al. (2022) {CPE} {Plot}},\n  date = {2023-03-29},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-3-29_Milleretal2022_Fig1},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, March 29. Miller et al. (2022) CPE Plot. https://fishr-core-team.github.io/fishR//blog/posts/2023-3-29_Milleretal2022_Fig1."
  },
  {
    "objectID": "blog/posts/2023-3-31_Milleretal2022_Fig23/index.html",
    "href": "blog/posts/2023-3-31_Milleretal2022_Fig23/index.html",
    "title": "Miller et al. (2022) Size Plots",
    "section": "",
    "text": "Series Note\n\n\n\nThis is the second of several posts related to Miller et al. (2022). I thank the authors for making their data available with their publication.\n\n\n\nIntroduction\nMiller et al. (2022) examined life history characteristics of Goldeye (Hiodon alosoides) in two Kansas reservoirs. Their Figure 2 and Figure 3 examined the length frequency and weight-length relationship of Goldeye captured in Milford Reservoir in 2020. I use ggplot2 here to recreate both figures.\nThe following packages are loaded for use below. A few functions from each of readxl, FSA, and scales are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\nlibrary(ggtext)     # for use of markdown in text/labels\n\n \n\n\nData Wrangling\nMiller et al. (2022) provided the raw data for producing Figures 2 and 3 in their Data Supplement S2, which I loaded below.\n\ndat &lt;- read.csv(\"JFWM-21-090.S2.csv\")\nhead(dat,n=8)\n\n#R|    netid  tl   w agecap ann bclen\n#R|  1     6 385 521      6   1   210\n#R|  2     6 385 521      6   2   264\n#R|  3     6 385 521      6   3   299\n#R|  4     6 385 521      6   4   329\n#R|  5     6 385 521      6   5   358\n#R|  6     6 385 521      6   6   378\n#R|  7     6 357 466      4   1   213\n#R|  8     6 357 466      4   2   262\n\n\nThe first six rows above all have the same tl, w, and age because these variables were repeated for each of the six observed annuli on this fish’s otoliths. To construct a proper length frequency and weight-length relationship, the data need to be reduced to only one observation of tl and w per fish. Thus, below, only records that corresponded to the first annulus are retained.\n\ndat2 &lt;- dat |&gt;\n  filter(ann==1)\nhead(dat2)\n\n#R|    netid  tl   w agecap ann bclen\n#R|  1     6 385 521      6   1   210\n#R|  2     6 357 466      4   1   213\n#R|  3     6 397 725      5   1   209\n#R|  4     8 393 610      8   1   202\n#R|  5     8 373 571      4   1   193\n#R|  6     8 389 656      6   1   163\n\n\nTotal length of these fish was summarized below.\n\nsumtl &lt;- dat2 |&gt;\n  summarize(n=n(),\n            mdnTL=median(tl,na.rm=TRUE),\n            minTL=min(tl,na.rm=TRUE),\n            maxTL=max(tl,na.rm=TRUE))\nsumtl\n\n#R|      n mdnTL minTL maxTL\n#R|  1 152   268   235   431\n\n\nThese results match the summary results in the paragraph above Figure 2 in Miller et al. (2022). These data are ready to recreate Figures 2 and 3.\n \n\n\nRecreating Figure 2\nFigure 2 is a histogram of total length, which can be created with geom_histogram() with tl mapped to the x-axis. Bin widths of 10 mm that started on 0 mm and were closed on the left, as is typical for most fisheries applications, were used.1 It is difficult to discern what the authors did with respect to these arguments but my result is at least close to theirs.2\n1 Making histograms in ggplot2 was discussed in detail in this post.2 Curiously all of their bars seem shifted left by about 5 mm; this is most noticeable at 350 mm.3 Adding labels such as these was discussed in more detail in this post.To try to match the authors’ other choices I named, set limits, and defined breaks for both axes; removed the lower expansion for the y-axis so the bars did not hover about the x-axis and reduced the other expansion factors for both axes; applied the black-and-white theme; increased the font size and bolded the axis titles and tick mark labels; changed the tick mark labels to black; and used annotate() to add the sample size label in the upper-left corner.3\n\nggplot(data=dat2,mapping=aes(x=tl)) +\n  geom_histogram(binwidth=10,boundary=0,closed=\"left\") +\n  scale_y_continuous(name=\"Count\",\n                     limits=c(0,50),breaks=scales::breaks_width(10),\n                     expand=expansion(mult=c(0,0.01))) +\n  scale_x_continuous(name=\"Total Length (mm)\",limits=c(200,450),\n                     breaks=scales::breaks_width(50),\n                     expand=expansion(mult=0.02)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\")) +\n  annotate(geom=\"text\",label=paste(\"N =\",sumtl$n),x=-Inf,y=Inf,vjust=1.5,hjust=-0.2,\n           size=12/.pt,fontface=\"bold\")\n\n\n\n\n\n\n\n\n\nPossible Modifications\nI find the use of a solid fill color with no delineation of the actual bars to be difficult on the eyes. I personally like to outline the bars with color=\"black\" and then fill the bars with fill= set to some version of gray in geom_histogram().\n\nggplot(data=dat2,mapping=aes(x=tl)) +\n  geom_histogram(binwidth=10,boundary=0,closed=\"left\",\n                 color=\"black\",fill=\"gray60\") +\n  scale_y_continuous(name=\"Count\",\n                     limits=c(0,50),breaks=scales::breaks_width(10),\n                     expand=expansion(mult=c(0,0.01))) +\n  scale_x_continuous(name=\"Total Length (mm)\",limits=c(200,450),\n                     breaks=scales::breaks_width(50),\n                     expand=expansion(mult=0.02)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\")) +\n  annotate(geom=\"text\",label=paste(\"N =\",sumtl$n),x=-Inf,y=Inf,vjust=1.5,hjust=-0.2,\n           size=12/.pt,fontface=\"bold\")\n\n\n\n\n\n\n\n\nIf you really like the dark fill then I would outline the bars with a lighter color.\n\nggplot(data=dat2,mapping=aes(x=tl)) +\n  geom_histogram(binwidth=10,boundary=0,closed=\"left\",\n                 color=\"gray60\",fill=\"gray20\") +\n  scale_y_continuous(name=\"Count\",\n                     limits=c(0,50),breaks=scales::breaks_width(10),\n                     expand=expansion(mult=c(0,0.01))) +\n  scale_x_continuous(name=\"Total Length (mm)\",limits=c(200,450),\n                     breaks=scales::breaks_width(50),\n                     expand=expansion(mult=0.02)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\")) +\n  annotate(geom=\"text\",label=paste(\"N =\",sumtl$n),x=-Inf,y=Inf,vjust=1.5,hjust=-0.2,\n           size=12/.pt,fontface=\"bold\")\n\n\n\n\n\n\n\n\n \n\n\n\nRecreating Figure 3\nFigure 3 is a scatterplot of weight versus length, with a best-fit regression line overlaid. The authors used a log-scale for both axes rather than plotting the log-transformed data. The basic scatterplot is constructed with geom_point() with the appropriate variables mapped to their respective axes. The authors used points that were outlined in black and semi-transparent in the middle. Circles with separate outline and fill colors are made with shape=21, with the outline color in color= and the fill color in fill=. col2rgbt() from FSA adds a transparency value (the second argument) to a named color.\n\nggplot(data=dat2,mapping=aes(x=tl,y=w)) +\n  geom_point(shape=21,color=\"black\",fill=FSA::col2rgbt(\"black\",0.5)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\"))\n\n\n\n\n\n\n\n\nLog-scales can be created in a variety of ways, but I prefer to use trans=log10 in scale_(x|y)_continuous().\n\nggplot(data=dat2,mapping=aes(x=tl,y=w)) +\n  geom_point(shape=21,color=\"black\",fill=FSA::col2rgbt(\"black\",0.5)) +\n  scale_x_continuous(name=\"Total Length (mm)\",trans=\"log10\",\n                     limits=c(220,440),breaks=scales::breaks_width(20),\n                     expand=expansion(mult=0.02)) +\n  scale_y_continuous(name=\"Mass (g)\",trans=\"log10\",\n                     limits=c(100,800),breaks=scales::breaks_width(100),\n                     expand=expansion(mult=0.02)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\"))\n\n\n\n\n\n\n\n\nThe linear regression line is added with geom_smooth() using method=\"lm\",4 se=FALSE to suppress showing the confidence band, and color=\"black\" to show the line in black.5 Because trans=\"log10\" was used for both scales, the regression will be performed with log-transformed lengths and weights.\n4 The default is to use a loess model.5 The default is blue.\nggplot(data=dat2,mapping=aes(x=tl,y=w)) +\n  geom_smooth(method=\"lm\",se=FALSE,color=\"black\") +\n  geom_point() +\n  scale_x_continuous(name=\"Total Length (mm)\",trans=\"log10\",\n                     limits=c(220,440),breaks=scales::breaks_width(20),\n                     expand=expansion(mult=0.02)) +\n  scale_y_continuous(name=\"Mass (g)\",trans=\"log10\",\n                     limits=c(100,800),breaks=scales::breaks_width(100),\n                     expand=expansion(mult=0.02)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\"))\n\n\n\n\n\n\n\n\nCreating the regression results labels is similar to that shown in this previous post. The regression must be performed on the log-log scale and, thus, variables for the log of weight and length are added to the data frame.\n\ndat2 &lt;- dat2 |&gt;\n  mutate(logtl=log10(tl),\n         logw=log10(w))\n\nThe regression is then fit and the summarized results saved to an object called tmp. The r2, intercept (stored in b), and slope (in m) are extracted and assigned to objects. Each was rounded to two decimals to match the authors’ choices.\n\ntmp &lt;- summary(lm(logw~logtl,data=dat2))\n( r2 &lt;- round(tmp$r.squared,2) )\n\n#R|  [1] 0.98\n\n( b &lt;- round(tmp$coefficients[\"(Intercept)\",\"Estimate\"],2) )\n\n#R|  [1] -4.38\n\n( m &lt;- round(tmp$coefficients[\"logtl\",\"Estimate\"],2) )\n\n#R|  [1] 2.75\n\n\nThese results are then turned into labels using markdown language code. Recall that text wrapped in * or &lt;emph&gt;will be italicized, text wrapped in ^ or &lt;sup&gt; will be superscripted, and text wrapped in ~ or &lt;sub&gt; will be subscripted.6 The &times; is specific code to produce the × symbol. paste0 pastes text together with no separation between the provided parts.\n6 I had inconsistent problems using ^ in the equation so I used &lt;sup&gt; here.\nr2 &lt;- paste0(\"*r*^2^ = \",r2)\neqn1 &lt;- paste0(\"Mass = 10&lt;sup&gt;(\",b,\"+\",m,\"&times;log~10~TL)&lt;/sup&gt;\")\n\nThese labels are then added to the plot with annotate() using geom=\"richtext\" so that the markdown language code will be rendered appropriately. Note that Inf does not work well with trans=\"log10\" so the x= and y= coordinates had to be specified.\n\nggplot(data=dat2,mapping=aes(x=tl,y=w)) +\n  geom_smooth(method=\"lm\",se=FALSE,color=\"black\") +\n  geom_point() +\n  scale_x_continuous(name=\"Total Length (mm)\",trans=\"log10\",\n                     limits=c(220,440),breaks=scales::breaks_width(20),\n                     expand=expansion(mult=0.02)) +\n  scale_y_continuous(name=\"Mass (g)\",trans=\"log10\",\n                     limits=c(100,800),breaks=scales::breaks_width(100),\n                     expand=expansion(mult=0.02)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\")) +\n  annotate(geom=\"richtext\",x=220,y=800,vjust=1,hjust=0,label=r2,\n           label.color=NA,fontface=\"bold\") +\n  annotate(geom=\"richtext\",x=220,y=800,vjust=2,hjust=0,label=eqn1,\n           label.color=NA,fontface=\"bold\")\n\n\n\n\n\n\n\n\n\nPossible Modifications\nThe equation of the regression is presented oddly in the sense that the left-hand-side is on the original mass scale but the right-hand-side shows log-transformed length but has that in a superscript of 10 suggesting a back-transformation to the original scale. If the actual values of the intercept and slope on the transformed scale are important for some reason then I suggest showing the results as a fully-transformed (linear) model. However, if the idea is to show the power function on the original scale then I suggest fully back-transforming the right-hand-side. Below I show this second option. Note that formatC() is used with format=\"f\" so that the back-transformed intercept is not presented in scientific notation.\n\n( bb &lt;- formatC(10^(tmp$coefficients[\"(Intercept)\",\"Estimate\"]),format=\"f\",digits=6))\n\n#R|  [1] \"0.000041\"\n\neqn2 &lt;- paste0(\"Mass = \",bb,\"TL&lt;sup&gt;\",m,\"&lt;/sup&gt;\")\n\nIt is not really that important, but it seems odd to me to have the r2 above the model equation. Thus, I switched these two below.\n\nggplot(data=dat2,mapping=aes(x=tl,y=w)) +\n  geom_smooth(method=\"lm\",se=FALSE,color=\"black\") +\n  geom_point() +\n  scale_x_continuous(name=\"Total Length (mm)\",trans=\"log10\",\n                     limits=c(220,440),breaks=scales::breaks_width(20),\n                     expand=expansion(mult=0.02)) +\n  scale_y_continuous(name=\"Mass (g)\",trans=\"log10\",\n                     limits=c(100,800),breaks=scales::breaks_width(100),\n                     expand=expansion(mult=0.02)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\")) +\n  annotate(geom=\"richtext\",x=220,y=800,vjust=1,hjust=0,label=eqn2,\n           label.color=NA,fontface=\"bold\") +\n  annotate(geom=\"richtext\",x=220,y=800,vjust=2,hjust=0,label=r2,\n           label.color=NA,fontface=\"bold\")\n\n\n\n\n\n\n\n\nThe authors presented a confidence band in their Figure 5 and I showed how to add one to their Figure 1 in this post. As I said there, for the purposes of consistency I would add a confidence band to this plot as well by simply removing se=FALSE from geom_smooth().7\n7 The regression line is so strong that the confidence band is barely noticeable.\nggplot(data=dat2,mapping=aes(x=tl,y=w)) +\n  geom_smooth(method=\"lm\",color=\"black\") +\n  geom_point() +\n  scale_x_continuous(name=\"Total Length (mm)\",trans=\"log10\",\n                     limits=c(220,440),breaks=scales::breaks_width(20),\n                     expand=expansion(mult=0.02)) +\n  scale_y_continuous(name=\"Mass (g)\",trans=\"log10\",\n                     limits=c(100,800),breaks=scales::breaks_width(100),\n                     expand=expansion(mult=0.02)) +\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        axis.title=element_text(size=12,face=\"bold\"),\n        axis.text=element_text(size=10,face=\"bold\",color=\"black\")) +\n  annotate(geom=\"richtext\",x=220,y=800,vjust=1,hjust=0,label=eqn2,\n           label.color=NA,fontface=\"bold\") +\n  annotate(geom=\"richtext\",x=220,y=800,vjust=2,hjust=0,label=r2,\n           label.color=NA,fontface=\"bold\")\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nReferences\n\nMiller, B. T., E. Flores, D. S. Waters, and B. C. Neely. 2022. An evaluation of Goldeye life history characteristics in two Kansas reservoirs. Journal of Fish and Wildlife Management 13(1):243–249.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Miller Et Al. (2022) {Size} {Plots}},\n  date = {2023-03-31},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-3-31_Milleretal2022_Fig23},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, March 31. Miller et al. (2022) Size Plots. https://fishr-core-team.github.io/fishR//blog/posts/2023-3-31_Milleretal2022_Fig23."
  },
  {
    "objectID": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html",
    "href": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html",
    "title": "Mack and Cheatwood (2022) Cumulative Sums Figure",
    "section": "",
    "text": "Mack and Cheatwood (2022) examined the upstream movements of American Eel (Anguilla rostrata) at four dams from North Carolina to the New York and Canada border. Their Figure 2 shows the cumulative percentage catch of eels by day at each dam for ten years. They must have used ggplot2 to produce their figure as it was fairly straightforward to reproduce. However, doing so reveals a few “tricks of the trade”, which I demonstrate below."
  },
  {
    "objectID": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html#one-location",
    "href": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html#one-location",
    "title": "Mack and Cheatwood (2022) Cumulative Sums Figure",
    "section": "One Location",
    "text": "One Location\nFor illustrative purposes I first constructed just the Roanoke Rapids subpanel of Figure 2.\n\ndatRR &lt;- dat |&gt;\n  filter(Location==\"Roanoke Rapids\")\n\nThe foundation of the figure is constructed by mapping Date2 to the x-axis and pcumsumEels to the y-axis, mapping Year to group= to get separate lines for each year, and then using geom_line(). I increased the line width slightly to better match the authors’ choice.\n\nggplot(data=datRR,mapping=aes(x=Date2,y=pcumsumEels,group=Year)) +\n  geom_line(linewidth=1)\n\n\n\n\n\n\n\n\nThe x-axis labels need to be modified to be monthly, which is accomplished with breaks=breaks_width(\"month\") within scale_x_date(). By default these labels will appear numeric with the month, day, and year. However, just the abbreviated month name can be used by including labels=label_date(\"%b%).5\n5 %b is a code the identifies the month name abbreviation. See this post.\nggplot(data=datRR,mapping=aes(x=Date2,y=pcumsumEels,group=Year)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=breaks_width(\"month\"),labels=label_date(\"%b\")) +\n  scale_y_continuous(name=\"Cumulative Percentage of Eels Captured\")"
  },
  {
    "objectID": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html#all-locations",
    "href": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html#all-locations",
    "title": "Mack and Cheatwood (2022) Cumulative Sums Figure",
    "section": "All Locations",
    "text": "All Locations\nThe plot for one location can be expanded to a plot for all locations by first changing the data to the data frame that has all locations and then “faceting” with respect to Location.\n\nggplot(data=dat,mapping=aes(x=Date2,y=pcumsumEels,group=Year)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=breaks_width(\"month\"),labels=label_date(\"%b\")) +\n  scale_y_continuous(name=\"Cumulative Percentage of Eels Captured\") +\n  facet_wrap(vars(Location))\n\n\n\n\n\n\n\n\nThe subpanels can be “stacked” by forcing the faceting to be in one column with ncol=. In addition, the “facet” (or “strip”) labels can be moved from the default position with strip.position=.\n\nggplot(data=dat,mapping=aes(x=Date2,y=pcumsumEels,group=Year)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=breaks_width(\"month\"),labels=label_date(\"%b\")) +\n  scale_y_continuous(name=\"Cumulative Percentage of Eels Captured\") +\n  facet_wrap(vars(Location),\n             ncol=1,strip.position=\"left\")\n\n\n\n\n\n\n\n\nThis is very close to Figure 2 in Mack and Cheatwood (2022) but they had an x-axis with tick marks for each facet. I initially tried to accomplish this with scales=\"free_x\" in facet_wrap() but this also included the month labels for each facet. The only way I could accomplish what the authors did was to use facet_rep_wrap() from the lemon package, which for this purpose has the same arguments as facet_wrap().\n\nggplot(data=dat,mapping=aes(x=Date2,y=pcumsumEels,group=Year)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=breaks_width(\"month\"),labels=label_date(\"%b\")) +\n  scale_y_continuous(name=\"Cumulative Percentage of Eels Captured\") +\n  lemon::facet_rep_wrap(vars(Location),\n                        ncol=1,strip.position=\"left\")"
  },
  {
    "objectID": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html#geom_step",
    "href": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html#geom_step",
    "title": "Mack and Cheatwood (2022) Cumulative Sums Figure",
    "section": "geom_step()",
    "text": "geom_step()\nIt is fairly common to show cumulative distributions with “steps” rather than lines. This is easily accomplished by replacing geom_line() with geom_step(). I don’t think that using steps is necessarily “better” with these data.\n\nggplot(data=datRR,mapping=aes(x=Date2,y=pcumsumEels,group=Year)) +\n  geom_step(linewidth=1) +\n  scale_x_date(breaks=breaks_width(\"month\"),labels=label_date(\"%b\")) +\n  scale_y_continuous(name=\"Cumulative Percentage of Eels Captured\")"
  },
  {
    "objectID": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html#explore-year-effects",
    "href": "blog/posts/2023-3-8_MackCheatwood2022_CumSums/index.html#explore-year-effects",
    "title": "Mack and Cheatwood (2022) Cumulative Sums Figure",
    "section": "Explore Year Effects",
    "text": "Explore Year Effects\nMack and Cheatwood (2022) were not interested in describing specific year-to-year differences. However, I was curious if any patterns among years were visually evident. I initially examined this by coding years with color, but the plot is pretty messy.\n\nggplot(data=dat,mapping=aes(x=Date2,y=pcumsumEels,group=Year,color=Year)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=breaks_width(\"month\"),labels=label_date(\"%b\")) +\n  scale_y_continuous(name=\"Cumulative Percentage of Eels Captured\") +\n  scale_color_viridis_c(begin=0.75,end=0.25,label=scales::label_number(1)) +\n  lemon::facet_rep_wrap(vars(Location),\n                        ncol=1,strip.position=\"left\")\n\n\n\n\n\n\n\n\nI then tried faceting by year and using color for location to see if there might be some obvious congruencies across locations within years. I don’t think this leads to a different narrative than what was evident in the authors’ Figure 2.\n\nggplot(data=dat,mapping=aes(x=Date2,y=pcumsumEels,group=Location,color=Location)) +\n  geom_line(linewidth=1) +\n  scale_x_date(breaks=breaks_width(\"2 months\"),labels=label_date(\"%b\")) +\n  scale_y_continuous(name=\"Cumulative Percentage of Eels Captured\") +\n  scale_color_viridis_d(begin=0.9,end=0.1) +\n  lemon::facet_rep_wrap(vars(Year),ncol=3)"
  },
  {
    "objectID": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html",
    "href": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html",
    "title": "Murphy et al. (2021) Model Comparison Plots",
    "section": "",
    "text": "Murphy et al. (2021) examined the ability to use Landsat thermal imagery and a variety of possible environmental data to predict water temperatures in small streams in Alaska. Their Figure 3 is a fairly complex graphic that shows hows Landsat observations and predictions from a variety of generalized additive mixed models (GAMMs) compared to in situ water temperature measurements. Here I use ggplot2 to try to recreate their figure.\nThe following packages are loaded for use below. A few functions from each of FSA, gtext, and scales are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\nlibrary(patchwork)  # for placing multiple plots\nlibrary(mgcv)       # for GAMM analyses\n\nSome of the results below include randomization. To promote reproducibility, the random seed is set here.\n\nset.seed(7834344)\n\nFinally, to help keep the ggplot2 code near the end simpler, the following custom theme was created. Its components will be described further below.\n\ntheme_murphy &lt;- function() {\n  theme_bw() +\n  theme(panel.grid=element_blank(),\n        legend.position=c(1,1),legend.justification=c(1.1,1.1),\n        legend.title=element_blank(),\n        legend.text=element_text(size=10))\n}"
  },
  {
    "objectID": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#fitting-models",
    "href": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#fitting-models",
    "title": "Murphy et al. (2021) Model Comparison Plots",
    "section": "Fitting Models",
    "text": "Fitting Models\n\n\n\n\n\n\nWarning\n\n\n\nMy goal here is not to fully demonstrate how to fit GAMMs in R, rather I want to show how Figure 3 was constructed. I am not an expert with GAMMs and am, in fact, still learning how to properly use and interpret them. Thus, be cautious and wary about following my work here.\n\n\nInputs or results for each of the four GAMMs will be appended with the corresponding panel letter. For example, objects that end with a “B” will be related to the GAMM used in panel B.\nFour objects are created below that define the GAMM “formula.” In these formulae, s() is the smoother function provided by mgcv and k=5 is the “modest maximum basis size” and bs=\"cr\" is the cubic spline smoother used in Murphy et al. (2021). The bs=\"re\" used for SiteCode causes it to be treated as a “random effect” as in Murphy et al. (2021). This code does not yet perform any analyses, it just creates a “formula” to be used next (and later).\n\nformB &lt;- In.Situ~s(AirDaily,k=5,bs=\"cr\")+\n  s(GageDaily,k=5,bs=\"cr\")+\n  s(GageDiff,k=5,bs=\"cr\")+\n  s(SiteCode,bs=\"re\")\nformC &lt;- In.Situ~s(AirDaily,k=5,bs=\"cr\")+\n  s(GageDaily,k=5,bs=\"cr\")+\n  s(GageDiff,k=5,bs=\"cr\")+\n  s(Landsat,k=5,bs=\"cr\")+\n  s(SiteCode,bs=\"re\")\nformD &lt;- In.Situ~s(AirDaily,k=5,bs=\"cr\")+\n  s(SiteCode,bs=\"re\")\nformE &lt;- In.Situ~s(Landsat,k=5,bs=\"cr\")+\n  s(SiteCode,bs=\"re\")\n\nThe actual GAMMs are fit with gam() from mgcv, taking a formula for the model as the first argument and the corresponding data frame in data=. The four lines below fit the four GAMMS of panels B-E using the formula objects created above.\n\nmdlB &lt;- gam(formB,data=dat)\nmdlC &lt;- gam(formC,data=dat)\nmdlD &lt;- gam(formD,data=dat)\nmdlE &lt;- gam(formE,data=dat)\n\nFor example, a summary() of the model for panel B shows the same r2 and smooth term p-values as described in the results of Murphy et al. (2021).\n\nsummary(mdlB)\n\n#R|  \n#R|  Family: gaussian \n#R|  Link function: identity \n#R|  \n#R|  Formula:\n#R|  In.Situ ~ s(AirDaily, k = 5, bs = \"cr\") + s(GageDaily, k = 5, \n#R|      bs = \"cr\") + s(GageDiff, k = 5, bs = \"cr\") + s(SiteCode, \n#R|      bs = \"re\")\n#R|  \n#R|  Parametric coefficients:\n#R|              Estimate Std. Error t value Pr(&gt;|t|)    \n#R|  (Intercept)  12.5004     0.8018   15.59   &lt;2e-16 ***\n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#R|  \n#R|  Approximate significance of smooth terms:\n#R|                 edf Ref.df      F  p-value    \n#R|  s(AirDaily)  1.000      1 46.477  &lt; 2e-16 ***\n#R|  s(GageDaily) 1.000      1 18.995  0.00011 ***\n#R|  s(GageDiff)  1.000      1  5.128  0.02984 *  \n#R|  s(SiteCode)  8.257      9  7.384 2.66e-06 ***\n#R|  ---\n#R|  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#R|  \n#R|  R-sq.(adj) =  0.716   Deviance explained = 78.6%\n#R|  GCV = 3.1696  Scale est. = 2.343     n = 47"
  },
  {
    "objectID": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#leave-one-out-predictions-and-mse",
    "href": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#leave-one-out-predictions-and-mse",
    "title": "Murphy et al. (2021) Model Comparison Plots",
    "section": "Leave-One-Out Predictions and MSE",
    "text": "Leave-One-Out Predictions and MSE\n\n\n\n\n\n\nAcknowledgment\n\n\n\nThe code in this section would not have been possible without help from the lead author of Murphy et al. (2021). I ultimately modified their code, but the general idea is still theirs. Of course, errors in the code presented here are my responsibility. I thank the authors for providing this code to me.\n\n\nMurphy et al. (2021) used a leave-one-out cross-validation (LOOCV) procedure to generate the root mean square error (MSE) as well as “LOOCV” predictions for each model. The general idea here is to fit the GAMM with the first observation removed (i.e., “left out”), predict the in situ temperature for the first observation from this model, compute the residual between the observed in situ and predicted in situ temperature for this point, repeat this a number of times,2 compute mean squared residual, and save the mean squared residuals and mean predicted in situ temperatures. Repeat this process for each of the other observations.3 Then compute and return the square root of the mean of the mean squared residuals (this is the LOOCV MSE) and the vector of mean predicted in situ temperatures related to each observation (i.e., for each DateTime and SiteCode combination).\n2 Murphy et al. (2021) repeated this 100 times.3 There were 47 observations in Murphy et al. (2021).4 I modified this function from what the authors provided me by including comments, changing some intermediate variable names, removing some inner loops to speed things up, and returning the results in a different way.This process is coded into the loocv.re() function shown in the folded code below.4\n\n\nCode\nloocv.re &lt;- function(form,dat,n.iters=100){\n  n &lt;- nrow(dat)\n  ## prepare matrix to received LOOCV prediction values (rows are for ith point\n  ##   left-out, columns are for random iterations)\n  predsRE &lt;- matrix(NA,nrow=n,ncol=n.iters)\n  ## perform LOOCV loop\n  for(i in 1:n) {\n    ## fit GAMM with ith row removed\n    mod &lt;- gam(form,data=dat[-i,])\n    ## get random effects SE\n    sitelevs &lt;- paste0(\"s(SiteCode).\",1:length(levels(dat[,\"SiteCode\"])))\n    re.se &lt;- sd(coef(mod)[sitelevs])\n    ## get individual SD\n    resid.se &lt;- sigma(mod)\n    ## predict In.Situ for ith left-out point, excluding SiteCode from GAMM fit\n    p &lt;- predict(mod,newdata=dat[i,],type=\"terms\",exclude=\"s(SiteCode)\")\n    ## get fixed effects portion of prediction\n    predsFE &lt;- sum(p) + attr(p,\"constant\")\n    ## add on random random effects portions\n    predsRE[i,] &lt;- rep(predsFE,n.iters) + \n      rnorm(n.iters,mean=0,sd=re.se) +\n      rnorm(n.iters,mean=0,sd=resid.se)\n  }\n  ## Compute mean of predicted values for each observation using FE+RE\n  mean.predsRE &lt;- rowMeans(predsRE)\n  ## Find all residuals from predicted values using FE+RE\n  residsRE &lt;- matrix(NA,nrow=n,ncol=n.iters)\n  for(i in 1:n) residsRE[i,] &lt;- dat[i,\"In.Situ\"] - predsRE[i,]\n  ## Find MSE from residuals from using predictions from FE+RE\n  MSERE &lt;- sqrt(mean(rowMeans(residsRE)^2))\n  ## Return all results\n  list(mean.predsRE=mean.predsRE,MSERE=MSERE,\n       resids=residsRE,formula=form)\n}\n\n\nThe process laid out above is applied to the model in panel B below.\n\nlooB &lt;- loocv.re(formB,dat=dat)\n\nFrom this result, the MSE can be extracted.5\n5 This MSE may not exactly match that in the published Figure 3 because of the randomization used in making the model predictions. Also 100 iterations is on the low side, such that this value may vary somewhat from run to run (see “Further Thoughts” section.\nlooB$MSERE\n\n#R|  [1] 2.455614\n\n\nThe mean LOOCV predicted in situ temperatures for each observation (i.e., location-date combination) may also be examined.\n\nlooB$mean.predsRE\n\n#R|   [1] 12.851791 14.149310 11.636388 14.001670 12.315976 12.552054 14.406877\n#R|   [8] 12.111865 14.576413 11.324704 13.884762 13.122397  9.594543 14.072126\n#R|  [15] 11.477815 13.830428 12.831921  9.334164 12.429613 14.532410 12.209832\n#R|  [22] 12.424465 14.553735 11.631457 13.734138 12.551352  9.853848 12.629046\n#R|  [29] 11.580924 14.031804 12.249635  9.467855 15.333607 11.544352 12.818436\n#R|  [36] 12.603421  9.896241 14.343051 13.954222 12.598558  9.358737 12.140968\n#R|  [43] 14.786858 11.550166 13.681013 12.143915  9.626715\n\n\nThe LOOCV process was repeated for the other three models below.\n\n\nCode\nlooC &lt;- loocv.re(formC,dat=dat)\nlooD &lt;- loocv.re(formD,dat=dat)\nlooE &lt;- loocv.re(formE,dat=dat)\n\n\nThe resultant MSE values were stored into a named vector for later use.\n\nMSE &lt;- c(\"B\"=looB$MSERE,\"C\"=looC$MSERE,\"D\"=looD$MSERE,\"E\"=looE$MSERE)\nMSE\n\n#R|         B        C        D        E \n#R|  2.455614 2.478938 2.499286 2.545127\n\n\nAnd the resultant mean LOOCV predicted in situ temperatures were stored into a data frame, along with the corresponding SiteCode and DateTime information for later use.\n\nmean.predsRE &lt;- data.frame(SiteCode=dat$SiteCode,\n                           DateTime=dat$DateTime,\n                           predB=looB$mean.predsRE,\n                           predC=looC$mean.predsRE,\n                           predD=looD$mean.predsRE,\n                           predE=looE$mean.predsRE)\nFSA::headtail(mean.predsRE)\n\n#R|     SiteCode   DateTime     predB     predC     predD     predE\n#R|  1      CR-1 2015-05-30 12.851791 11.210556 12.144141 12.568541\n#R|  2      CR-1 2015-06-15 14.149310 14.470042 14.437798 14.684522\n#R|  3      CR-1 2015-07-01 11.636388 12.541739 11.940564 12.973309\n#R|  45     SF-3 2015-08-11 13.681013 13.712191 13.659172 12.561452\n#R|  46     SF-3 2015-08-18 12.143915 12.791890 13.015895 12.910432\n#R|  47     SF-3 2015-09-12  9.626715  9.353639  9.350488  9.646322"
  },
  {
    "objectID": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#panel-a",
    "href": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#panel-a",
    "title": "Murphy et al. (2021) Model Comparison Plots",
    "section": "Panel A",
    "text": "Panel A\nPanels A-E have the same basic structure of layering together several geoms with both raw and summarized data. I demonstrate this layering for Panel A and, then, in the next section show how this is modified to produce Panel B.\nPanel A shows in situ temperature observations and predictions from landsat images for six dates with multiple locations at each date. In situ observations and landsat predictions paired by location are connected within each date. To do this, Murphy et al. (2021) “jittered” the in situ and landsat results slightly at each date. Jittering usually means adding a random amount to each point, which is not what was done in Figure 3. Rather, the groups were “dodged” left for the in situ observations and right for the landsat predictions. This will be accomplished here by subtracting and adding a small amount to the date (i.e., x-axis) for in situ and landsat results, respectively. Below, I set that amount in an object called ddg so that it can be held constant for multiple geoms in each panel and also across panels.7\n7 Choosing this value is largely a matter of trial-and-error until you find the look you like.\nddg &lt;- 1.5\n\nI began panel A by plotting lines at the means for both the in situ and landsat results. The color= is set to a constant string in each geom_line() because the in situ and landsat results are in separate columns in sum2 rather than stacked with a second variable describing the source of the value. As seen below, these strings are shown in the legend.\n\nggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnRL,color=\"Raw Landsat\"),\n            linewidth=2)\n\n\n\n\n\n\n\n\nPoints that demonstrate the observed in situ temperatures and the landsat predicted temperatures are added with geom_point(). ddg is used here to “dodge” the two sets of points. shape=21 is an open circle outlined in color= but filled with fill=. Again, because of the arrangement of the data in separate columns, fill= is set to a constant string that matches that used for the geom_line()s.\n\nggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnRL,color=\"Raw Landsat\"),\n            linewidth=2) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,mapping=aes(x=DateTime+ddg,y=Landsat,fill=\"Raw Landsat\"),\n             shape=21,size=3,color=\"black\")\n\n\n\n\n\n\n\n\nMurphy et al. (2021) connected in situ and landsat temperatures paired by location within each date with light gray lines. This is accopmlished with geom_segment() below. geom_segment() was placed before the geom_point()s so that the gray segments would appear behind the points. It is worth noting here that adding these segments is the reason why the data were not stacked and the in situ and landsat results were kept in separate columns.\n\nggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnRL,color=\"Raw Landsat\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                     y=In.Situ,yend=Landsat),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,mapping=aes(x=DateTime+ddg,y=Landsat,fill=\"Raw Landsat\"),\n             shape=21,size=3,color=\"black\")\n\n\n\n\n\n\n\n\nThe MSE and panel labels are added below with annotate() using geom=\"text\" as demonstrated in previous posts. Note here that Inf or -Inf cannot be used for the x-axis values because they are dates. Thus, the x-axis position must be chosen by choosing a date and making sure that it is treated as a date with as.Date().\n\nggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnRL,color=\"Raw Landsat\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                    y=In.Situ,yend=Landsat),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,mapping=aes(x=DateTime+ddg,y=Landsat,fill=\"Raw Landsat\"),\n             shape=21,size=3,color=\"black\") +\n  annotate(geom=\"text\",x=as.Date(\"2015-08-05\",format=\"%Y-%m-%d\"),y=-Inf,\n           label=MSE_lbls[[\"A\"]],hjust=0,vjust=-0.5,size=11/.pt) +\n  annotate(geom=\"text\",x=as.Date(\"2015-05-30\",format=\"%Y-%m-%d\"),y=Inf,\n           label=\"a)\",vjust=1.5,size=11/.pt)\n\n\n\n\n\n\n\n\nThe x-and y-axes were modified as described in previous posts. Note, however, the use of markdown code in the y-axis title, which will require the use of element_markdown() from ggtext later when a theme is applied.\n\nggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnRL,color=\"Raw Landsat\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                    y=In.Situ,yend=Landsat),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,mapping=aes(x=DateTime+ddg,y=Landsat,fill=\"Raw Landsat\"),\n             shape=21,size=3,color=\"black\") +\n  annotate(geom=\"text\",x=as.Date(\"2015-08-05\",format=\"%Y-%m-%d\"),y=-Inf,\n           label=MSE_lbls[[\"A\"]],hjust=0,vjust=-0.5,size=11/.pt) +\n  annotate(geom=\"text\",x=as.Date(\"2015-05-30\",format=\"%Y-%m-%d\"),y=Inf,\n           label=\"a)\",vjust=1.5,size=11/.pt) +\n  scale_y_continuous(name=\"Temperature (^o^C)\",\n                     limits=c(0,30),breaks=scales::breaks_width(10)) +\n  scale_x_date(name=\"Date\")\n\n\n\n\n\n\n\n\nThe colors of the points and lines need to be changed to match those in Murphy et al. (2021) and the issue of multiple legends also needs to be addressed. The colors of the lines are set with scale_color_manual() below and the fills of the points are set with scale_fill_manual() as shown in other posts. Note that values= is set to a named list where the names are the constant text values used in color= for geom_line() and fill= for geom_point() above. The use of name= here must be the same in both scale_fill_manual() and scale_color_manual() so that the two legends above will be merged into a common legend with this name as its title.\n\nggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnRL,color=\"Raw Landsat\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                    y=In.Situ,yend=Landsat),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,mapping=aes(x=DateTime+ddg,y=Landsat,fill=\"Raw Landsat\"),\n             shape=21,size=3,color=\"black\") +\n  annotate(geom=\"text\",x=as.Date(\"2015-08-05\",format=\"%Y-%m-%d\"),y=-Inf,\n           label=MSE_lbls[[\"A\"]],hjust=0,vjust=-0.5,size=11/.pt) +\n  annotate(geom=\"text\",x=as.Date(\"2015-05-30\",format=\"%Y-%m-%d\"),y=Inf,\n           label=\"a)\",vjust=1.5,size=11/.pt) +\n  scale_y_continuous(name=\"Temperature (^o^C)\",\n                     limits=c(0,30),breaks=scales::breaks_width(10)) +\n  scale_x_date(name=\"Date\") +\n  scale_fill_manual(name=\"legend\",\n                    values=c(\"In Situ\"=\"gray70\",\"Raw Landsat\"=\"steelblue1\")) +\n  scale_color_manual(name=\"legend\",\n                     values=c(\"In Situ\"=\"black\",\"Raw Landsat\"=\"blue4\"))\n\n\n\n\n\n\n\n\nFinally, theme_murphy() create above was applied. This theme applies the built-in theme_bw(), removes all grid lines, moves the legend into the upper-right corner of the plot layer, removes the legend title, and makes the legend text slightly larger than the default. After this the y-axis title will use element_markdown() so that the markdown code will be rendered and the x-axis title is removed. Finally, this entire plot is saved into the pA object so that it can ultimately be stitched together with the other panels to form Figure 3.\n\npA &lt;- ggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnRL,color=\"Raw Landsat\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                    y=In.Situ,yend=Landsat),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,mapping=aes(x=DateTime+ddg,y=Landsat,fill=\"Raw Landsat\"),\n             shape=21,size=3,color=\"black\") +\n  annotate(geom=\"text\",x=as.Date(\"2015-08-05\",format=\"%Y-%m-%d\"),y=-Inf,\n           label=MSE_lbls[[\"A\"]],hjust=0,vjust=-0.5,size=11/.pt) +\n  annotate(geom=\"text\",x=as.Date(\"2015-05-30\",format=\"%Y-%m-%d\"),y=Inf,\n           label=\"a)\",vjust=1.5,size=11/.pt) +\n  scale_y_continuous(name=\"Temperature (^o^C)\",\n                     limits=c(0,30),breaks=scales::breaks_width(10)) +\n  scale_x_date(name=\"Date\") +\n  scale_fill_manual(name=\"legend\",\n                    values=c(\"In Situ\"=\"gray70\",\"Raw Landsat\"=\"steelblue1\")) +\n  scale_color_manual(name=\"legend\",\n                     values=c(\"In Situ\"=\"black\",\"Raw Landsat\"=\"blue4\")) +\n  theme_murphy() +\n  theme(axis.title.y=ggtext::element_markdown(),\n        axis.title.x=element_blank())\npA"
  },
  {
    "objectID": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#panels-b-e",
    "href": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#panels-b-e",
    "title": "Murphy et al. (2021) Model Comparison Plots",
    "section": "Panels B-E",
    "text": "Panels B-E\nPanel B is constructed very similarly to Panel A. The most important difference is that all references to results related to Landsat must be changed to results related to model B. Of course, the object name and colors for the model were also changed. Finally, this panel did not have titles for either axis.\n\npB &lt;- ggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,\n            mapping=aes(x=DateTime,y=mnB,\n                        color=\"Model = AirDaily + GageDaily + GageDiff\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                    y=In.Situ,yend=predB),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,\n             mapping=aes(x=DateTime+ddg,y=predB,\n                         fill=\"Model = AirDaily + GageDaily + GageDiff\"),\n             shape=21,size=3,color=\"black\") +\n  annotate(geom=\"text\",x=as.Date(\"2015-08-05\",format=\"%Y-%m-%d\"),y=-Inf,\n           label=MSE_lbls[[\"B\"]],hjust=0,vjust=-0.5,size=11/.pt) +\n  annotate(geom=\"text\",x=as.Date(\"2015-05-30\",format=\"%Y-%m-%d\"),y=Inf,\n           label=\"b)\",vjust=1.5,size=11/.pt) +\n  scale_y_continuous(name=\"Temperature (^o^C)\",\n                     limits=c(0,30),breaks=scales::breaks_width(10)) +\n  scale_x_date(name=\"Date\") +\n  scale_fill_manual(name=\"legend\",\n                    values=c(\"In Situ\"=\"gray70\",\n                             \"Model = AirDaily + GageDaily + GageDiff\"=\"indianred3\")) +\n  scale_color_manual(name=\"legend\",\n                     values=c(\"In Situ\"=\"black\",\n                              \"Model = AirDaily + GageDaily + GageDiff\"=\"red4\")) +\n  theme_murphy() +\n  theme(axis.title=element_blank())\npB\n\n\n\n\n\n\n\n\nSimilar changes were made to produce objects for Panels C-E.\n\n\nCode\npC &lt;- ggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,\n            mapping=aes(x=DateTime,y=mnC,\n                        color=\"Model = AirDaily + GageDaily + GageDiff + Landsat\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                    y=In.Situ,yend=predC),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,\n             mapping=aes(x=DateTime+ddg,y=predC,\n                         fill=\"Model = AirDaily + GageDaily + GageDiff + Landsat\"),\n             shape=21,size=3,color=\"black\") +\n  annotate(geom=\"text\",x=as.Date(\"2015-08-05\",format=\"%Y-%m-%d\"),y=-Inf,\n           label=MSE_lbls[[\"C\"]],hjust=0,vjust=-0.5,size=11/.pt) +\n  annotate(geom=\"text\",x=as.Date(\"2015-05-30\",format=\"%Y-%m-%d\"),y=Inf,\n           label=\"c)\",vjust=1.5,size=11/.pt) +\n  scale_y_continuous(name=\"Temperature (^o^C)\",\n                     limits=c(0,30),breaks=scales::breaks_width(10)) +\n  scale_x_date(name=\"Date\") +\n  scale_fill_manual(name=\"legend\",\n                    values=c(\"In Situ\"=\"gray70\",\n                             \"Model = AirDaily + GageDaily + GageDiff + Landsat\"=\"indianred3\")) +\n  scale_color_manual(name=\"legend\",\n                     values=c(\"In Situ\"=\"black\",\n                              \"Model = AirDaily + GageDaily + GageDiff + Landsat\"=\"red4\")) +\n  theme_murphy() +\n  theme(axis.title.y=ggtext::element_markdown(),\n        axis.title.x=element_blank())\n\npD &lt;- ggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,\n            mapping=aes(x=DateTime,y=mnD,\n                        color=\"Model = AirDaily\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                    y=In.Situ,yend=predD),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,\n             mapping=aes(x=DateTime+ddg,y=predD,\n                         fill=\"Model = AirDaily\"),\n             shape=21,size=3,color=\"black\") +\n  annotate(geom=\"text\",x=as.Date(\"2015-08-05\",format=\"%Y-%m-%d\"),y=-Inf,\n           label=MSE_lbls[[\"D\"]],hjust=0,vjust=-0.5,size=11/.pt) +\n  annotate(geom=\"text\",x=as.Date(\"2015-05-30\",format=\"%Y-%m-%d\"),y=Inf,\n           label=\"d)\",vjust=1.5,size=11/.pt) +\n  scale_y_continuous(name=\"Temperature (^o^C)\",\n                     limits=c(0,30),breaks=scales::breaks_width(10)) +\n  scale_x_date(name=\"Date\") +\n  scale_fill_manual(name=\"legend\",\n                    values=c(\"In Situ\"=\"gray70\",\n                             \"Model = AirDaily\"=\"indianred3\")) +\n  scale_color_manual(name=\"legend\",\n                     values=c(\"In Situ\"=\"black\",\n                              \"Model = AirDaily\"=\"red4\")) +\n  theme_murphy() +\n  theme(axis.title.y=element_blank())\n\npE &lt;- ggplot() +\n  geom_line(data=sum2,mapping=aes(x=DateTime,y=mnIS,color=\"In Situ\"),\n            linewidth=2) +\n  geom_line(data=sum2,\n            mapping=aes(x=DateTime,y=mnE,\n                        color=\"Model = Landsat\"),\n            linewidth=2) +\n  geom_segment(data=dat2,mapping=aes(x=DateTime-ddg,xend=DateTime+ddg,\n                                    y=In.Situ,yend=predE),\n               color=\"gray80\",linewidth=1) +\n  geom_point(data=dat2,mapping=aes(x=DateTime-ddg,y=In.Situ,fill=\"In Situ\"),\n             shape=21,size=3,color=\"black\") +\n  geom_point(data=dat2,\n             mapping=aes(x=DateTime+ddg,y=predE,\n                         fill=\"Model = Landsat\"),\n             shape=21,size=3,color=\"black\") +\n  annotate(geom=\"text\",x=as.Date(\"2015-08-05\",format=\"%Y-%m-%d\"),y=-Inf,\n           label=MSE_lbls[[\"E\"]],hjust=0,vjust=-0.5,size=11/.pt) +\n  annotate(geom=\"text\",x=as.Date(\"2015-05-30\",format=\"%Y-%m-%d\"),y=Inf,\n           label=\"e)\",vjust=1.5,size=11/.pt) +\n  scale_y_continuous(name=\"Temperature (^o^C)\",\n                     limits=c(0,30),breaks=scales::breaks_width(10)) +\n  scale_x_date(name=\"Date\") +\n  scale_fill_manual(name=\"legend\",\n                    values=c(\"In Situ\"=\"gray70\",\n                             \"Model = Landsat\"=\"indianred3\")) +\n  scale_color_manual(name=\"legend\",\n                     values=c(\"In Situ\"=\"black\",\n                              \"Model = Landsat\"=\"red4\")) +\n  theme_murphy()"
  },
  {
    "objectID": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#putting-panels-together",
    "href": "blog/posts/2023-4-11_Murphyetal2021_Fig3/index.html#putting-panels-together",
    "title": "Murphy et al. (2021) Model Comparison Plots",
    "section": "Putting Panels Together",
    "text": "Putting Panels Together\nAll five panels were then placed together using patchwork() to produce the final Figure 3.\n\npA + pB + pC + pD + pE +\n  plot_layout(ncol=2)"
  },
  {
    "objectID": "blog/posts/2023-4-23_Multiple_ALKs/index.html",
    "href": "blog/posts/2023-4-23_Multiple_ALKs/index.html",
    "title": "Working with Multiple Age-Length Keys",
    "section": "",
    "text": "Introduction\nAn age-length key (ALK) describes the relationship between length (category) and age of fish. An estimated age may be assigned to unaged fish in a sample based on the ALK derived from aged fish from the same (or very closely related) sample as described by Isermann and Knight (2005) and implemented in FSA (Ogle 2016). It is critical to this process that the ALK is representative of the fish to which ages will be assigned. Thus, for example, it is not recommended to use an ALK developed from fish collected in one year to assign age to fish collected in another year.1\n1 The same argument can be made for fish from different areas.This recommendation can lead to cumbersome data wrangling during analysis. For example, suppose that samples of fish were collected from two different areas over a five year period. In this case separate age-length keys would be required to be developed and applied for the ten combinations of locations and years. For each location-year the specific data would need to be isolated and for that data the ALK developed and applied, which has several steps as was shown in Ogle (2016) and will be outlined below. The final data produced for each location-year would then need to be combined back together to make an overall data set. In other words, using the Isermann and Knight (2005) method to assign estimated ages to unaged fish for multiple groups of fish is quite a bit of work. The goal of this post is to provide a more efficient method to accomplish this task.\nThe following packages are loaded for use below.\n\nlibrary(tidyverse)  # for dplyr, tidyr, purr packages\nlibrary(FSA)        # for ALK functionality\n\nThe random number seed was set to ensure repeatability for the random components of alkIndivAge() below.\n\nset.seed(14354454)\n\n \n\n\nAge-Length Keys in FSA\n\nOriginal Example from Ogle (2016)\nOgle (2016) demonstrated the Isermann and Knight (2005) method for assigning ages to unaged Creek Chubs (Semotilus atromaculatus). The portion of the script used there to produce a final data frame with ages for all sampled fish is shown below.2\n2 This code was extracted from the script provided here. It is slightly modified here to maintain the original unaltered data frame in cc.\n## Load data\ncc &lt;- read.csv(\"https://derekogle.com/IFAR/scripts/CreekChub.csv\")\n## Add length category variable\ncc1 &lt;- cc |&gt; mutate(lcat10=lencat(len,w=10))\n## Separate aged and unaged fish\ncc1.unaged &lt;- filter(cc1,is.na(age))\ncc1.aged &lt;- filter(cc1,!is.na(age))\n## Develop ALK from aged fish\nalk.freq &lt;- xtabs(~lcat10+age,data=cc1.aged)\nalk &lt;- prop.table(alk.freq,margin=1)\n## Use I-K method to assign ages to unaged fish\ncc1.unaged.mod &lt;- alkIndivAge(alk,age~len,data=cc1.unaged)\n## Create overall data frame with ages for all fish\ncc1.fnl &lt;- rbind(cc1.aged,cc1.unaged.mod)\n\nThe lengths-at-age in this final data frame may be summarized as follows.\n\ncc1.fnl |&gt;\n  group_by(age) |&gt;\n  summarize(n=n(),\n            mn=mean(len,na.rm=TRUE),\n            sd=sd(len,na.rm=TRUE)) |&gt;\n  as.data.frame()\n\n#R|    age   n        mn       sd\n#R|  1   0  20  48.65000  5.62209\n#R|  2   1 142  74.64789 16.82163\n#R|  3   2  43 113.41860 15.77405\n#R|  4   3   8 151.87500 11.17954\n#R|  5   4   5 183.20000 17.25399\n\n\n\n\nSimplifying Function\nI have resisted writing a function that would combine all the steps above, as I did not want to create a “black-box” function for this analysis that could be implemented without much thought. However, such a function would be useful for efficiently applying ALKs to multiple groups of fish. Thus, I create such a function and demonstrate how it can be used to produce the same results as those shown for Creek Chub in Ogle (2016).3 In the next section, I demonstrate how this new function can then be used to efficiently apply ALKs for multiple groups of fish.\n3 he same at least within rounding because of the inherent randomization in the Isermann and Knight (2005) method.4 This allows the variable names to be supplied by user in the function call, rather than hard-coded in the function.The applyALK() function created below performs the code shown above to create a “final” data frame that has ages assigned to the unaged fish based on the ALK. The function largely repeats the code above but uses some “advanced” code (e.g., deparse(substitute()) and {{}}) to handle the use of unquoted variables names.4\n\n## Computes and applies an ALK\n##   data: The data frame with, at least, the age & length variables\n##   avar: The name (without quotes) of the age variable in data\n##   lvar: The name (without quotes) of the length variable in data\n##   w: The width of length categories/bins for use in the ALK\n## Returns the data data frame with ages in avar assigned from the ALK for\n##   unaged fish and a new length category (lcat) variable derived from w\n\napplyALK &lt;- function(data,avar,lvar,w) {\n  ## Get avar variable name as character for non-tidyverse functions below\n  avarn &lt;- deparse(substitute(avar))\n  ## Add length category variable\n  data &lt;- data |&gt; dplyr::mutate(lcat=FSA::lencat({{lvar}},w=w))\n  ## Separate into aged and unaged dataframes\n  aged &lt;- data |&gt; dplyr::filter(!is.na({{avar}}))\n  unaged &lt;- data |&gt; dplyr::filter(is.na({{avar}}))\n  ## Make ALK (find frequencies, convert to row proportions)\n  ALK &lt;- prop.table(xtabs(as.formula(paste0(\"~lcat+\",avarn)),data=aged),margin=1)\n  ## Apply ALK according to Isermann-Knight method\n  tmp &lt;- FSA::alkIndivAge(ALK,as.formula(paste0(avarn,\"~lcat\")),data=unaged)\n  ## Put aged and newly assigned age data frames together to return\n  dplyr::bind_rows(aged,tmp)\n}\n\nWith this new function the final data frame can be created by supplying the original data frame (with aged and unaged fish) as the first argument, the names of the age and length variables in avar= and lvar= respectively, and the width for the length categories/bins in w=. For example, the final data frame for the Creek Chub case study can be created as follows.5\n5 Note the use the original cc data frame without the length categorization variable.\ncc.fnl &lt;- applyALK(cc,avar=age,lvar=len,w=10)\n\nThe lengths-at-age summary for this final data frame is similar to that from above.6\n6 Again, not exact because of the inherent randomization in the Isermann and Knight (2005) method.\ncc.fnl |&gt;\n  group_by(age) |&gt;\n  summarize(n=n(),\n            mn=mean(len,na.rm=TRUE),\n            sd=sd(len,na.rm=TRUE)) |&gt;\n  as.data.frame()\n\n#R|    age   n        mn        sd\n#R|  1   0  20  48.95000  6.012925\n#R|  2   1 142  74.66197 16.955046\n#R|  3   2  43 113.23256 15.929663\n#R|  4   3   8 151.87500 11.179541\n#R|  5   4   5 183.20000 17.253985\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOne still needs to carefully consider the application of ALKs for each of the groups. For example, if the minimum length of unaged fish is less than the minimum length of aged fish (i.e., smaller than that which the ALK is based on) for any one group then this process will fail for ALL groups. In other words a complete fail will occur if there is a fail for any one group in the analysis.\n\n\n \n\n\n\nEfficiently Applying ALKs to Multiple Groups\n\nExample Data\nSchall et al. (2020) examined the effect of season (Spring and Fall) on the vital statistics of Channel Catfish (Ictalurus punctatus) and Walleye (Sander vitreus) in a large Nebraska reservoir. One part of their analysis required computing mortality rates from catch curves for each species and season combination. Prior to constructing the catch curve they used the Isermann and Knight (2005) method to assign estimated ages to unaged fish. Schall et al. (2020) provided the raw data as a CSV file in their Supplement Material Data S1. Note that I removed some variables here for simplicity of presentation in this post, and that their Month completely defined Season.\n\ndat &lt;- read.csv(\"JFWM-20-027.S1.csv\") |&gt;\n  select(-Year,-Weight,-Sex,-BCAge,-BCLength) |&gt;\n  mutate(Season=case_when(\n    Month==\"May\" ~ \"Spring\",\n    Month==\"September\" ~ \"Fall\"\n  ))\nheadtail(dat)\n\n#R|       Spp Length     Month Age Season\n#R|  1    CCF    279       May   3 Spring\n#R|  2    CCF    334       May   4 Spring\n#R|  3    CCF    351       May   4 Spring\n#R|  2377 WAE     NA September  NA   Fall\n#R|  2378 WAE     NA September  NA   Fall\n#R|  2379 WAE     NA September  NA   Fall\n\n\nA common “issue” with using the ALK to assign ages to unaged fish is that the lengths of some unaged fish are not represented within the lengths of aged fish used to derive the ALK. In other words, the ALK does not contain information for fish of those lengths. The summary below is used to find the sample size and valid sample size (i.e., after excluding fish with no length measurement) and minimum and maximum length for each combination of species, season, and whether an age was assigned or not.\n\ndat |&gt;\n  mutate(Aged=!is.na(Age)) |&gt;\n  group_by(Spp,Season,Aged) |&gt;\n  summarize(n=n(),\n            validn=FSA::validn(Length),\n            minL=min(Length,na.rm=TRUE),\n            maxL=max(Length,na.rm=TRUE))\n\n#R|  # A tibble: 8 × 7\n#R|  # Groups:   Spp, Season [4]\n#R|    Spp   Season Aged      n validn  minL  maxL\n#R|    &lt;chr&gt; &lt;chr&gt;  &lt;lgl&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#R|  1 CCF   Fall   FALSE   105    104   197   674\n#R|  2 CCF   Fall   TRUE     97     97   200   724\n#R|  3 CCF   Spring FALSE   394    393   260   733\n#R|  4 CCF   Spring TRUE    104    104   279   769\n#R|  5 WAE   Fall   FALSE   883    870   181   634\n#R|  6 WAE   Fall   TRUE    466    466   157   676\n#R|  7 WAE   Spring FALSE   117    116   332   724\n#R|  8 WAE   Spring TRUE    213    213   191   753\n\n\nIt is seen from this that there are some missing length measurements (n does not equal validn in all cases) and that the minimum length of unaged fish is less than the minimum length of aged fish for Channel Catfish in both the Spring and Fall. Thus, to appropriately use the Isermann and Knight (2005) method as implemented in alkIndivAge() of FSA, those records with missing lengths must be removed, as well as those records for Channel Catfish that are less than the minimum length for aged Channel Catfish in their respective season.7\n7 It is not clear that Schall et al. (2020) did this, but it is required when using alkIndivAge().\ndat &lt;- dat |&gt;\n  filter(!is.na(Length)) |&gt;\n  filter(!(Spp==\"CCF\" & Season==\"Spring\" & Length&lt;279)) |&gt;\n  filter(!(Spp==\"CCF\" & Season==\"Fall\" & Length&lt;200))\n\n\n\nMethod-Specific Data Wrangling\nThe first step in efficiently applying the ALK to all groups is to “split” the original data frame based on the “groups” with split(). Below dat is split by the combination of Spp (species) and Season.\n\ndat2 &lt;- split(dat,~Spp+Season)\n\nThe result, in dat2, is a list with four items. Each item in the list is a data frame with the same structure as the original dat but reduced to a specific group defined by Spp and Season. Below names() is used to show that the names of the four items in dat2 are combinations of the “levels” in Spp and Season.\n\nnames(dat2)\n\n#R|  [1] \"CCF.Fall\"   \"WAE.Fall\"   \"CCF.Spring\" \"WAE.Spring\"\n\n\nThe specifics of one of these items is examined by appending the item name to dat2, separated by a $. Below, as an example, are a few rows from the beginning and end of the data frame in CCF.Spring (Channel Catfish in Spring).8\n8 Note here that it appears that all Spp values are “CCF”, all Season values are “Spring”, and that some fish have ages (the top three) and some do not (the bottom three).\nheadtail(dat2$CCF.Spring)\n\n#R|      Spp Length Month Age Season\n#R|  1   CCF    279   May   3 Spring\n#R|  2   CCF    334   May   4 Spring\n#R|  3   CCF    351   May   4 Spring\n#R|  491 CCF    701   May  NA Spring\n#R|  492 CCF    730   May  NA Spring\n#R|  493 CCF    733   May  NA Spring\n\n\n\n\nApplying ALK\nThe idea now is to “apply” applyALK() to each data frame in each item of the list in dat2. This can be done with lapply() where the list of data frames is the first argument, the function to apply is the second argument, and the remaining arguments are further arguments to the function being applied. The result is a list with data frames in the items as before, but now with ages for all fish.\n\ndat3 &lt;- lapply(dat2,applyALK,avar=Age,lvar=Length,w=10)\nnames(dat3)\n\n#R|  [1] \"CCF.Fall\"   \"WAE.Fall\"   \"CCF.Spring\" \"WAE.Spring\"\n\nheadtail(dat3$CCF.Spring)\n\n#R|      Spp Length Month Age Season lcat\n#R|  1   CCF    279   May   3 Spring  270\n#R|  2   CCF    334   May   4 Spring  330\n#R|  3   CCF    351   May   4 Spring  350\n#R|  491 CCF    701   May  17 Spring  700\n#R|  492 CCF    730   May  17 Spring  730\n#R|  493 CCF    733   May  17 Spring  730\n\n\nThe exact same result9 is also obtained with map() from the purr package, which was loaded with library(tidyverse). The arguments to map() and the resulting list are the same as those for lapply().\n9 Disregarding the randomization inherent in alkIndivAge().\ndat3 &lt;- map(dat2,applyALK,avar=Age,lvar=Length,w=10)\nnames(dat3)\n\n#R|  [1] \"CCF.Fall\"   \"WAE.Fall\"   \"CCF.Spring\" \"WAE.Spring\"\n\nheadtail(dat3$CCF.Spring)\n\n#R|      Spp Length Month Age Season lcat\n#R|  1   CCF    279   May   3 Spring  270\n#R|  2   CCF    334   May   4 Spring  330\n#R|  3   CCF    351   May   4 Spring  350\n#R|  491 CCF    701   May  17 Spring  700\n#R|  492 CCF    730   May  17 Spring  730\n#R|  493 CCF    733   May  17 Spring  730\n\n\nThe reason for introducing map() from purr() is that ultimately the four data frames in dat3 need to be “row-bound” together to form a single data frame. There are multiple ways to do this, but the simplest is to use map_df() from purr. map_df() has the same arguments as map() and lapply() but it returns a single combined data frame, rather than a list with multiple data frames.\n\ndat3 &lt;- map_df(dat2,applyALK,avar=Age,lvar=Length,w=10)\nnames(dat3)  # now column names of the single data frame\n\n#R|  [1] \"Spp\"    \"Length\" \"Month\"  \"Age\"    \"Season\" \"lcat\"\n\nheadtail(dat3)\n\n#R|       Spp Length     Month Age Season lcat\n#R|  1    CCF    232 September   1   Fall  230\n#R|  2    CCF    216 September   1   Fall  210\n#R|  3    CCF    238 September   1   Fall  230\n#R|  2355 WAE    632       May  10 Spring  630\n#R|  2356 WAE    653       May  10 Spring  650\n#R|  2357 WAE    720       May  13 Spring  720\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe process, as shown above, will not handle situations where the length category bin width required differs among the groups.\n\n\n \n\n\n\nPreview - Summaries\nWhile there are still some issues to deal with before the mortality rate can be estimated via catch curve with these data, the summary table and graphic below provide an indication of how the final result from above can be visualized. The issue of estimating mortality rate from these data will be taken up in the next post.\n\nsumdat1 &lt;- dat3 |&gt;\n  group_by(Spp,Season,Age) |&gt;\n  summarize(Catch=n(),\n            meanL=mean(Length),\n            sdL=sd(Length))\nsumdat1\n\n#R|  # A tibble: 53 × 6\n#R|  # Groups:   Spp, Season [4]\n#R|     Spp   Season   Age Catch meanL   sdL\n#R|     &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#R|   1 CCF   Fall       1    15  227.  14.7\n#R|   2 CCF   Fall       2    22  262.  37.7\n#R|   3 CCF   Fall       3     3  309   27.7\n#R|   4 CCF   Fall       4    16  359.  15.4\n#R|   5 CCF   Fall       5    30  384.  37.5\n#R|   6 CCF   Fall       6    21  405.  23.4\n#R|   7 CCF   Fall       7    22  430.  25.1\n#R|   8 CCF   Fall       8    29  449.  40.6\n#R|   9 CCF   Fall       9    22  488.  40.3\n#R|  10 CCF   Fall      10    11  495.  29.5\n#R|  # … with 43 more rows\n\n\n\nggplot(dat=sumdat1,mapping=aes(x=Age,y=Catch,color=Season)) +\n  geom_point(size=2) +\n  geom_line(alpha=0.1,linewidth=1.5) +\n  scale_x_continuous(name=\"Age (yrs)\",expand=expansion(mult=0.02)) +\n  scale_y_continuous(name=\"Total Catch\",expand=expansion(mult=0.02),\n                     trans=\"log\",breaks=c(1,2,5,10,20,50,100,200,500)) +\n  facet_wrap(vars(Spp)) +\n  theme_bw() +\n  theme(panel.grid.major=element_blank(),\n        legend.position=c(1,1),\n        legend.justification=c(1.1,1.1),\n        legend.title=element_blank(),\n        legend.background=element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nIsermann, D., and C. Knight. 2005. A computer program for age–length keys incorporating age assignment to individual fish. North American Journal of Fisheries Management 25:1153–1160.\n\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\n\nSchall, B. J., C. W. Schoenebeck, and K. D. Koupal. 2020. Seasonal sampling influence on population dynamics and yield of Channel Catfish and Walleye in a large Great Plains reservoir. Journal of Fish and Wildlife Management 12(1):223–233.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Working with {Multiple} {Age-Length} {Keys}},\n  date = {2023-04-23},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023-4-23_Multiple_ALKs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, April 23. Working with Multiple Age-Length Keys. https://fishr-core-team.github.io/fishR//blog/posts/2023-4-23_Multiple_ALKs."
  },
  {
    "objectID": "blog/posts/2023-4-4_Size/index.html",
    "href": "blog/posts/2023-4-4_Size/index.html",
    "title": "How Does Size Work in ggplot2",
    "section": "",
    "text": "I often struggle with “size” of ggplot objects. It has seemed that size= behaved differently in certain situations and was unrelated to linewidth=. I often just fiddle with the values until I get something that looks like I want. Dissatisfied with this approach, I explored the idea of “size” in ggplot2 further.\nThe rest of this post is split into two main sections. In the first, I describe a confusion that I had from reading the ggplot2 documentation with respect to the size of text, points, and lines. That section finishes with a summary of my findings with respect to this confusion, which is largely a summary of one answer I received on StackOverflow (as of 1-Apr-2023), and a possible solution which seems to address my confusion. In the second section, I discuss how to set the “size” for text, lines, and points, and what these sizes mean.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "blog/posts/2023-4-4_Size/index.html#text",
    "href": "blog/posts/2023-4-4_Size/index.html#text",
    "title": "How Does Size Work in ggplot2",
    "section": "Text",
    "text": "Text\n\nDefault Sizes\nUnless modified by the user, the default “base” font size in most ggplot2 themes is 11 pt, as shown below for theme_grey() (the default ggplot2 theme). This can be modified with base_size= to the theme function.\n\ntheme_grey()$text\n\n#R|  List of 11\n#R|   $ family       : chr \"\"\n#R|   $ face         : chr \"plain\"\n#R|   $ colour       : chr \"black\"\n#R|   $ size         : num 11\n#R|   $ hjust        : num 0.5\n#R|   $ vjust        : num 0.5\n#R|   $ angle        : num 0\n#R|   $ lineheight   : num 0.9\n#R|   $ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n#R|    ..- attr(*, \"unit\")= int 8\n#R|   $ debug        : logi FALSE\n#R|   $ inherit.blank: logi TRUE\n#R|   - attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n\n\nSeveral of the default font sizes for other items are based off of this base size. For example, for most themes the axis titles are the base size, the plot title is 1.2× larger than the base size, and the axis tick labels and facet strip labels are 0.8× the base size.3\n3 I chose not to show the code by default for some of these plots because the point demonstrated by the plot is the point, not the code. However, click “code” if you would like to see the code.\n\nCode\npd &lt;- ggplot() +\n  geom_point() +\n  scale_x_continuous(name=\"Base Font Size (11)\",\n                     limits=c(0,1),breaks=c(0,1),expand=expansion(add=0.5),\n                     labels=rep(\"0.8X Base Font Size\",2)) +\n  scale_y_continuous(name=\"Base Font Size (11)\",\n                     limits=c(0,1),breaks=c(0,1),expand=expansion(add=0.25),\n                     labels=rep(\"0.8X Base Font Size\",2)) +\n  ggtitle(\"1.2X Base Font Size\")\npd\n\n\n\n\n\n\n\n\n\nText added to the plot with geom_text() is shown at the base font size by default.\n\n\nCode\npd +\n  geom_text(data=data.frame(x=0.5,y=0.5,label=\"Text at Default Base Font Size\"),\n            mapping=aes(x=x,y=y,label=label))\n\n\n\n\n\n\n\n\n\n\n\nText Label Font Sizes\ngeom_text() has a size= argument for changing the size of the text. As such, you might change it to 13.2 assuming that its size would then match the size of plot title (i.e., 1.2×11).\n\n\nCode\npd +\n  geom_text(data=data.frame(x=0.5,y=0.5,label=\"Text at 'size=13.2'\"),\n            mapping=aes(x=x,y=y,label=label),size=13.2)\n\n\n\n\n\n\n\n\n\nThis clearly did not meet expectations. Why? Because the units for size= is not pts, rather it is mm. In geom_text() those mm units get converted to pts for displaying the text. There are a little over 72 dots (or “pts”) per inch, which corresponds to approximately 2.835 dots per mm. Thus, size=13.2 sets the text size at approximately 13.2×2.835=37.4 pts. Thus, the very large text above.\nThis phenomenon is also evident in the default “size” used in geom_text() shown below. Note that 3.88×2.835=11.0 or, approximately, the 11 pt base font size.\n\nGeomText$default_aes\n\n#R|  Aesthetic mapping: \n#R|  * `colour`     -&gt; \"black\"\n#R|  * `size`       -&gt; 3.88\n#R|  * `angle`      -&gt; 0\n#R|  * `hjust`      -&gt; 0.5\n#R|  * `vjust`      -&gt; 0.5\n#R|  * `alpha`      -&gt; NA\n#R|  * `family`     -&gt; \"\"\n#R|  * `fontface`   -&gt; 1\n#R|  * `lineheight` -&gt; 1.2\n\n\nThe exact conversion factor from mm to pts is stored in the ggplot2 constant .pt.\n\n.pt\n\n#R|  [1] 2.845276\n\n\nThus, to use size= in geom_text() to display text at a specific pt size, then divide the desired pt size by .pt. For example, the plot below uses size=11*1.2/.pt for the top text and size=16/.pt for the bottom text.\n\n\nCode\npd +\n  geom_text(data=data.frame(x=c(0.5,0.5),y=c(0.75,0.25),\n                            label=c(\"Text at 1.2X Base Font Size\",\n                                    \"Text at 16 pt Font Size\")),\n            mapping=aes(x=x,y=y,label=label),size=c(11*1.2,16)/.pt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSet the font size in geom_text() or geom_label() to a desired point size by setting size= to the desired point size divided by the ggplot2 constant in .pt. For example, use size=16/.pt to use a 16 pt font size.\n\n\nThe following “chart” gives an idea of how a particular size= will be converted to a pt size (on the left) and how a particular pt size corresponds to a size= value (on the right).\n\n\nCode\ndat2 &lt;- data.frame(sz1=3:10,\n                   pts2=c(6,8,10,11,12,14,18,24)) |&gt;\n  dplyr::mutate(sz2=pts2/.pt,\n         pts1=sz1*.pt,\n         x1=rep(0,length(sz1)),\n         y=seq_along(sz1),\n         lbl1=glue::glue(\"size={sz1}; {round(pts1,1)} pt\"),\n         x2=rep(1,length(sz2)),\n         lbl2=glue::glue(\"{pts2} pts; size={round(sz2,1)}\"))\n\nggplot(data=dat2) +\n  geom_text(mapping=aes(x=x1,y=y,size=sz1,label=lbl1)) +\n  geom_text(mapping=aes(x=x2,y=y,size=sz2,label=lbl2)) +\n  geom_vline(xintercept=0.5,color=\"gray70\") +\n  scale_size_identity() +\n  scale_x_continuous(expand=expansion(add=0.5)) +\n  theme_classic() +\n  theme(axis.title=element_blank(),axis.text=element_blank(),\n        axis.ticks=element_blank(),axis.line=element_blank(),\n        legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\n\nTheme-Related Font Sizes\nText related to plot titles,4 axis titles, axis tick labels, and facet strip labels are all modified using element_text() with specific arguments in theme(). element_text() has size= for altering the size of text. However, the units of size= in this element function is pts and is not mm. The text sizes for the plot title, axis title, and axis tick marks for a ggplot2 object called pd2 are modified as shown below.\n4 And subtitles, captions, and tags.\n\nCode\npd2 &lt;- ggplot() +\n  geom_point() +\n  scale_x_continuous(name=\"16 pt Font\",\n                     limits=c(0,1),breaks=c(0,1),expand=expansion(add=0.5),\n                     labels=rep(\"12 pt Font\",2)) +\n  scale_y_continuous(name=\"18 pt Font\",\n                     limits=c(0,1),breaks=c(0,1),expand=expansion(add=0.25),\n                     labels=rep(\"11 pt Font\",2)) +\n  ggtitle(\"24 pt Font\")\n\n\n\npd2 +\n  theme(plot.title=element_text(size=24),\n        axis.title.y=element_text(size=18),\n        axis.title.x=element_text(size=16),\n        axis.text.y=element_text(size=11),\n        axis.text.x=element_text(size=12))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSet the font size for ggplot2 plot labels, axis titles, axis tick labels, and facet strip labels using size= in element_text() set equal to specific arguments related to these items in theme(). The values used in size= here is pts and, thus, do not need to be converted to pts."
  },
  {
    "objectID": "blog/posts/2023-4-4_Size/index.html#lines",
    "href": "blog/posts/2023-4-4_Size/index.html#lines",
    "title": "How Does Size Work in ggplot2",
    "section": "Lines",
    "text": "Lines\nObjects that are “lines” of some sort (see table below)5 all control their size or thickness with linewidth=. As noted in the first section, linewidth= is measured in mm but due to an historical error it actually scales to “roughly 0.75 mm.”6 The default linewidth for most geoms is 0.5, though some differ as shown below.7\n5 There are still other geoms that use linewidth=.6 The actual mm depends on the resolution of the display device.7 These defaults come from, for example, GeomLine$default_aes.\n\nCode\ntmp &lt;- data.frame(`geom_`=c(\"abline()\",\"area()\",\"bar()\",\"boxplot()\",\n                            \"col()\",\"contour()\",\"curve()\",\"errorbar()\"),\n                  default=c(GeomAbline$default_aes$linewidth,\n                            GeomArea$default_aes$linewidth,\n                            GeomBar$default_aes$linewidth,\n                            GeomBoxplot$default_aes$linewidth,\n                            GeomCol$default_aes$linewidth,\n                            GeomContour$default_aes$linewidth,\n                            GeomCurve$default_aes$linewidth,\n                            GeomErrorbar$default_aes$linewidth),\n                  `     `=rep(\"     \",8),\n                  `geom_`=c(\"hline()\",\"line()\",\"path()\",\"pointrange()\",\n                            \"polygon()\",\"segment()\",\"smooth()\",\"tile()\"),\n                  default=c(GeomHline$default_aes$linewidth,\n                            GeomLine$default_aes$linewidth,\n                            GeomPath$default_aes$linewidth,\n                            GeomPointrange$default_aes$linewidth,\n                            GeomPolygon$default_aes$linewidth,\n                            GeomSegment$default_aes$linewidth,\n                            GeomSmooth$default_aes$linewidth,\n                            GeomTile$default_aes$linewidth),\n                  check.names=FALSE)\ntmp |&gt; knitr::kable() |&gt;\n  kableExtra::kable_styling(full_width=FALSE,\n                            bootstrap_options=c(\"striped\",\"hover\")) |&gt;\n  kableExtra::column_spec(3,width=\"4em\")\n\n\n\n\n\n\ngeom_\ndefault\n\ngeom_\ndefault\n\n\n\n\nabline()\n0.5\n\nhline()\n0.5\n\n\narea()\n0.5\n\nline()\n0.5\n\n\nbar()\n0.5\n\npath()\n0.5\n\n\nboxplot()\n0.5\n\npointrange()\n0.5\n\n\ncol()\n0.5\n\npolygon()\n0.5\n\n\ncontour()\n0.5\n\nsegment()\n0.5\n\n\ncurve()\n0.5\n\nsmooth()\n1.0\n\n\nerrorbar()\n0.5\n\ntile()\n0.1\n\n\n\n\n\n\n\n\nThe display below gives an idea of the relative sizes of the different linewidth= values.8\n8 The most common default linewidth is shown in red.\n\nCode\ndat1 &lt;- data.frame(lw=c(0.1,0.25,0.5,0.75,1,1.5,2,3,4)) |&gt;\n  dplyr::mutate(x=rep(0,length(lw)),\n         xend=rep(1,length(lw)),\n         y=seq_along(lw),\n         yend=y)\n\nggplot(data=dat1) +\n  geom_segment(mapping=aes(x=x,xend=xend,y=y,yend=yend,linewidth=lw)) +\n  scale_y_continuous(name=\"linewidth=\",breaks=dat1$y,labels=dat1$lw) +\n  geom_segment(x=0,y=3,xend=1,yend=3,color=\"red\") +\n  scale_x_continuous(expand=expansion(mult=0)) +\n  scale_linewidth_identity() +\n  theme_classic() +\n  theme(axis.title.x=element_blank(),axis.text.x=element_blank(),\n        axis.ticks=element_blank(),axis.line=element_blank(),\n        axis.title.y=element_text(size=14),axis.text.y=element_text(size=12),\n        legend.position=\"none\")"
  },
  {
    "objectID": "blog/posts/2023-4-4_Size/index.html#points",
    "href": "blog/posts/2023-4-4_Size/index.html#points",
    "title": "How Does Size Work in ggplot2",
    "section": "Points",
    "text": "Points\nAs alluded to in the first section, points consist of a size= “fill” portion and a stroke= “color” outline portion. As shown above, the total “size” of the point is the sum of the size= and stroke= value, though these values seem to be scaled differently. The size= portion of the point is scaled to the same as size= for text and linewidth= for lines. However, stroke= is scaled larger and, thus, you cannot “trade” values from scale= to stroke= to get the same size of point.\nThe default values for points are size=1.5 and stroke=0.5.9\n9 Because of the small stroke= value, the issue discussed above is likely not going to be noticeable.\nGeomPoint$default_aes\n\n#R|  Aesthetic mapping: \n#R|  * `shape`  -&gt; 19\n#R|  * `colour` -&gt; \"black\"\n#R|  * `size`   -&gt; 1.5\n#R|  * `fill`   -&gt; NA\n#R|  * `alpha`  -&gt; NA\n#R|  * `stroke` -&gt; 0.5\n\n\nThe following figure gives an idea of what different sizes look like for different shapes of “points.”10\n10 The default stroke= of 0.5 was used.\n\nCode\nszs &lt;- c(1,1.5,2,2.5,3,4,5)\ndat4 &lt;- tidyr::expand_grid(Size=szs,Shape=c(19,21,1,17,24,2,15,22,0)) |&gt;\n  mutate(fShape=factor(Shape,levels=c(19,21,1,17,24,2,15,22,0)),\n         fSize=factor(Size))\n\nstroke &lt;- 0.5\n\nggplot(data=dat4,mapping=aes(x=fShape,y=fSize,size=Size,shape=Shape)) + \n  geom_hline(yintercept=seq_along(szs),linewidth=szs+stroke,color=\"gray90\") +\n  geom_hline(yintercept=seq_along(szs),linewidth=szs,color=\"gray95\") +\n  geom_point(fill=\"red\",stroke=stroke) +\n  scale_size_identity() +\n  scale_shape_identity() +\n  scale_y_discrete(name=\"Size\",expand=expansion(mult=0.1),breaks=szs) +\n  scale_x_discrete(name=\"Shape\",expand=expansion(mult=0.1)) +\n  theme_bw() +\n  theme(axis.line=element_blank(),\n        axis.title=element_text(size=14),axis.text=element_text(size=12),\n        panel.grid.minor=element_blank(),\n        panel.border=element_blank())"
  },
  {
    "objectID": "blog/posts/2023-4-4_Size/index.html#examples",
    "href": "blog/posts/2023-4-4_Size/index.html#examples",
    "title": "How Does Size Work in ggplot2",
    "section": "Examples",
    "text": "Examples\n\nScatterplot with Best-Fit Line\nAs an example, suppose that you have log-transformed catch-at-age data that looks like this.11\n11 This is typical, though idealistic, catch curve data.\n\nCode\nset.seed(173)\ndat &lt;- data.frame(age=1:8) |&gt;\n  mutate(catch=500*(1-0.4)^age+rnorm(8,sd=10),\n         catch=round(catch,0),\n         logcatch=log(catch))\ndat\n\n\n#R|    age catch logcatch\n#R|  1   1   311 5.739793\n#R|  2   2   176 5.170484\n#R|  3   3   125 4.828314\n#R|  4   4    60 4.094345\n#R|  5   5    42 3.737670\n#R|  6   6    18 2.890372\n#R|  7   7    12 2.484907\n#R|  8   8    26 3.258097\n\n\nAnd a simple “catch curve” plot is constructed that looks like this.\n\nggplot(data=dat,mapping=aes(x=age,y=logcatch)) +\n  geom_smooth(method=lm,color=\"black\",fill=\"gray80\") +\n  geom_point(shape=21,fill=\"gray60\") +\n  scale_y_continuous(name=\"log(Catch)\") +\n  scale_x_continuous(name=\"Age (years)\",breaks=1:8) +\n  theme_bw() +\n  theme(panel.grid.minor=element_blank())\n\n\n\n\n\n\n\n\nHowever, suppose that you made the following changes to meet some (perhaps yours) requirements.\n\nPoints that are twice as big (i.e., size=3 rather than default size=1.5).\nA “best-fit” line that is 50% thicker (i.e., linewidth=1.5 rather than default linewidth=1).\nAxis title labels that are 14 pt.\nAxis tick mark labels that are 12 pt.\nA species annotation label (in the upper-right corner) that uses 14 pt font.\n\n\nggplot(data=dat,mapping=aes(x=age,y=logcatch)) +\n  geom_smooth(method=lm,color=\"black\",fill=\"gray80\",linewidth=1.5) +\n  geom_point(shape=21,fill=\"gray60\",size=3) +\n  scale_y_continuous(name=\"log(Catch)\") +\n  scale_x_continuous(name=\"Age (years)\",breaks=1:8) +\n  annotate(geom=\"text\",label=\"Freshwater Drum\",x=Inf,y=Inf,vjust=1.5,hjust=1.1,\n           size=14/.pt) +\n  theme_bw() +\n  theme(panel.grid.minor=element_blank(),\n        axis.title=element_text(size=14),\n        axis.text=element_text(size=12))\n\n\n\n\n\n\n\n\n\n\nBar Chart with Labels\nSuppose these same data are presented as a bar chart without the fitted line.\n\nggplot(data=dat,mapping=aes(x=age,y=catch)) +\n  geom_col(fill=\"gray60\",color=\"black\") +\n  scale_y_continuous(name=\"Catch\",expand=expansion(mult=c(0,0.1))) +\n  scale_x_continuous(name=\"Age (years)\",breaks=1:8) +\n  theme_bw() +\n  theme(panel.grid.minor=element_blank())\n\n\n\n\n\n\n\n\nAnd further suppose that you want to make the same textual changes as above and …\n\nBorders of the bars two times heavier (i.e., linewidth=1 rather than the default linewidth=0.5).\nHorizontal gridlines four times heavier (i.e., linewidth=2 rather than the default linewidth=0.5).\nNumeric labels of the catch amount above each bar in 12 pt font.\n\n\nggplot(data=dat,mapping=aes(x=age,y=catch)) +\n  geom_col(fill=\"gray60\",color=\"black\",linewidth=1) +\n  geom_text(mapping=aes(label=catch),vjust=-0.5,size=12/.pt) +\n  scale_y_continuous(name=\"Catch\",expand=expansion(mult=c(0,0.1))) +\n  scale_x_continuous(name=\"Age (years)\",breaks=1:8) +\n  annotate(geom=\"text\",label=\"Freshwater Drum\",x=Inf,y=Inf,vjust=1.5,hjust=1.1,\n           size=14/.pt) +\n  theme_bw() +\n  theme(panel.grid.minor=element_blank(),\n        axis.title=element_text(size=14),\n        axis.text=element_text(size=12),\n        panel.grid.major.y=element_line(linewidth=2))"
  },
  {
    "objectID": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html",
    "href": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html",
    "title": "O’Malley et al. (2021) Regression Plots",
    "section": "",
    "text": "O’Malley et al. (2021) compared morphometric measurements made directly on Cisco (Coregonus artedi) to those made from digitized images. Their Figure 2 displayed the relationship between the two measurements for 12 morphometric measures. They tested whether the slope of the relationship between two measurements was equal to 1 for all 12 measures, but only showed the regression result on the plot if the slope was significantly different from 1. I use ggplot2 here to recreate their figure.\nThe following packages are loaded for use below. A few functions from each of FSA, tibble, scales, ggtext, and smatr are used with :: such that the entire packages are not attached here.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages"
  },
  {
    "objectID": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#individual-measures",
    "href": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#individual-measures",
    "title": "O’Malley et al. (2021) Regression Plots",
    "section": "Individual Measures",
    "text": "Individual Measures\nO’Malley et al. (2021) provided the raw data for producing Figure 1 in their Archived Material A1, which I loaded below. I retained only the variables needed for this post and renamed them to be shorter.\n\ndat &lt;- read.csv(\"CiscoMethodMSDataset_20200929.csv\") |&gt;\n  select(id=SpecimenID,measure=MorphometricAbbreviation,method=Method,\n         measurer=MeasurerInitials,value=Value)\nFSA::headtail(dat)\n\n#R|                   id measure        method measurer     value\n#R|  1    2018-50-886-20     BDD specimenBased      BPO  66.88000\n#R|  2    2018-50-886-20     CPL specimenBased      BPO  26.59000\n#R|  3    2018-50-886-20     DOH specimenBased      BPO  42.61000\n#R|  3562  2018-38-208-6     PVL    imageBased      JDS  40.41272\n#R|  3563  2018-38-208-6     STL    imageBased      JDS 275.50429\n#R|  3564  2018-38-208-6     TTL    imageBased      JDS 328.48593\n\n\nThe image-based measurements were made by two measurers. Figure 2 uses only those measurements made by the same measurer as for the specimen-based measurements (i.e., “BPO”).\n\ndat2 &lt;- dat |&gt;\n  filter(measurer==\"BPO\")\nFSA::headtail(dat2)\n\n#R|                   id measure        method measurer     value\n#R|  1    2018-50-886-20     BDD specimenBased      BPO  66.88000\n#R|  2    2018-50-886-20     CPL specimenBased      BPO  26.59000\n#R|  3    2018-50-886-20     DOH specimenBased      BPO  42.61000\n#R|  2374  2018-38-208-6     PVL    imageBased      BPO  38.73081\n#R|  2375  2018-38-208-6     STL    imageBased      BPO 276.25851\n#R|  2376  2018-38-208-6     TTL    imageBased      BPO 329.04718\n\n\nThese data need to be made “wider” by placing the two methods of measurement into their own columns, rather than in different rows.\n\ndat2 &lt;- dat2 |&gt;\n  pivot_wider(names_from=method,values_from=value)\nFSA::headtail(dat2)\n\n#R|                   id measure measurer specimenBased imageBased\n#R|  1    2018-50-886-20     BDD      BPO         66.88   69.05096\n#R|  2    2018-50-886-20     CPL      BPO         26.59   28.61097\n#R|  3    2018-50-886-20     DOH      BPO         42.61   44.97137\n#R|  1186  2018-38-208-6     PVL      BPO         38.64   38.73081\n#R|  1187  2018-38-208-6     STL      BPO        267.00  276.25851\n#R|  1188  2018-38-208-6     TTL      BPO        325.00  329.04718\n\n\nThese data are now ready for plotting imageBased versus specimenBased for each measure."
  },
  {
    "objectID": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#regression-summaries",
    "href": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#regression-summaries",
    "title": "O’Malley et al. (2021) Regression Plots",
    "section": "Regression Summaries",
    "text": "Regression Summaries\nFigure 2, however, also shows summary results from the linear regression of imageBased on specimenBased for each measure for which the slope was significantly different from 1. Thus, these regressions must be conducted for each measure and the results stored in a data frame for use when plotting. There are a variety of ways to do this, but sapply() is used below.1\n1 lmList() from nlme is another option, though manipulation of results afterwards is still required to produce the needed data frame.The original data frame is split on measure to form a list with a data frame for each measure. The result is too big to show here.\n\ndat2.split &lt;- split(dat2,dat2$measure)\n\nA function is then defined that will take a data frame as its sole argument, fit the regression of imageBased on specimenBased, extract certain results about the regression and combine them into a vector, and then return that vector.2 This function is created below and called regfun().\n2 To see how this works, run the tmp line below but with data=dat2 instead and then use str(tmp) to see what is stored from this result and then extracted in the following lines.\nregfun &lt;- function(d) {\n  tmp &lt;- summary(lm(imageBased~specimenBased,data=d))\n  c(slope=tmp$coefficients[\"specimenBased\",\"Estimate\"],\n    slopeSE=tmp$coefficients[\"specimenBased\",\"Std. Error\"],\n    intercept=tmp$coefficients[\"(Intercept)\",\"Estimate\"],\n    rsq=tmp$r.squared,\n    df=tmp$df[2])\n}\n\nAs an example, regfun is called below with only the data for the “BDD” morphometric measurement in dat2.split. Here you can see that regfun() returns the estimated intercept and slope, the standard error for the slope, the r-squared value, and the residual degrees-of-freedom for the regression.\n\nregfun(dat2.split$BDD)\n\n#R|         slope      slopeSE    intercept          rsq           df \n#R|   1.031474479  0.008749461  0.010705827  0.993068986 97.000000000\n\n\nregfun() can be applied to each item in dat2.split with sapply().\n\nsapply(dat2.split,FUN=regfun)\n\n#R|                     BDD        CPL         DOH         HLL         MXL\n#R|  slope      1.031474479  1.0757713  1.02856998  1.01445030  0.96781162\n#R|  slopeSE    0.008749461  0.0352823  0.02442481  0.02186351  0.04933639\n#R|  intercept  0.010705827  0.1554068 -0.44887045  0.44980926  1.16453067\n#R|  rsq        0.993068986  0.9055193  0.94813927  0.95688676  0.79867605\n#R|  df        97.000000000 97.0000000 97.00000000 97.00000000 97.00000000\n#R|                    OOL         PAD         PCL         POL         PVL\n#R|  slope      0.81350663  0.98313163  1.11895119  0.86621728  0.90627210\n#R|  slopeSE    0.05721728  0.02070146  0.02995096  0.05628229  0.04636973\n#R|  intercept  1.82831841  0.18526295 -5.15899945  1.78522043  2.71996110\n#R|  rsq        0.67574491  0.95876533  0.93501829  0.70946778  0.79748909\n#R|  df        97.00000000 97.00000000 97.00000000 97.00000000 97.00000000\n#R|                     STL         TTL\n#R|  slope      1.033775553  1.02287457\n#R|  slopeSE    0.008315711  0.01384976\n#R|  intercept  0.017022154 -7.17195762\n#R|  rsq        0.993762642  0.98252745\n#R|  df        97.000000000 97.00000000\n\n\nHowever, the result needs to be transposed with t() so that the summary statistics form the columns and the measures form the rows. The transposed matrix is converted to a data frame for further manipulation.\n\nrsum &lt;- sapply(dat2.split,FUN=regfun) |&gt;\n  t() |&gt;\n  as.data.frame()\nrsum\n\n#R|          slope     slopeSE   intercept       rsq df\n#R|  BDD 1.0314745 0.008749461  0.01070583 0.9930690 97\n#R|  CPL 1.0757713 0.035282303  0.15540677 0.9055193 97\n#R|  DOH 1.0285700 0.024424812 -0.44887045 0.9481393 97\n#R|  HLL 1.0144503 0.021863514  0.44980926 0.9568868 97\n#R|  MXL 0.9678116 0.049336389  1.16453067 0.7986760 97\n#R|  OOL 0.8135066 0.057217285  1.82831841 0.6757449 97\n#R|  PAD 0.9831316 0.020701457  0.18526295 0.9587653 97\n#R|  PCL 1.1189512 0.029950963 -5.15899945 0.9350183 97\n#R|  POL 0.8662173 0.056282293  1.78522043 0.7094678 97\n#R|  PVL 0.9062721 0.046369732  2.71996110 0.7974891 97\n#R|  STL 1.0337756 0.008315711  0.01702215 0.9937626 97\n#R|  TTL 1.0228746 0.013849764 -7.17195762 0.9825274 97\n\n\nThis data frame is modified by converting the rownames to a variable name (i.e., measure) with rownames_to_column() from tibble. In addition, a t test statistic for the comparison of the estimated slope to 1 is computed and a two-tailed p-value is computed from that test statistic using pt().\n\nrsum &lt;- rsum |&gt;\n  tibble::rownames_to_column(var=\"measure\") |&gt;\n  mutate(t=(slope-1)/slopeSE,\n         p=2*pt(abs(t),df=df,lower.tail=FALSE))\nrsum\n\n#R|     measure     slope     slopeSE   intercept       rsq df          t\n#R|  1      BDD 1.0314745 0.008749461  0.01070583 0.9930690 97  3.5973049\n#R|  2      CPL 1.0757713 0.035282303  0.15540677 0.9055193 97  2.1475736\n#R|  3      DOH 1.0285700 0.024424812 -0.44887045 0.9481393 97  1.1697113\n#R|  4      HLL 1.0144503 0.021863514  0.44980926 0.9568868 97  0.6609323\n#R|  5      MXL 0.9678116 0.049336389  1.16453067 0.7986760 97 -0.6524268\n#R|  6      OOL 0.8135066 0.057217285  1.82831841 0.6757449 97 -3.2593886\n#R|  7      PAD 0.9831316 0.020701457  0.18526295 0.9587653 97 -0.8148399\n#R|  8      PCL 1.1189512 0.029950963 -5.15899945 0.9350183 97  3.9715313\n#R|  9      POL 0.8662173 0.056282293  1.78522043 0.7094678 97 -2.3769948\n#R|  10     PVL 0.9062721 0.046369732  2.71996110 0.7974891 97 -2.0213164\n#R|  11     STL 1.0337756 0.008315711  0.01702215 0.9937626 97  4.0616555\n#R|  12     TTL 1.0228746 0.013849764 -7.17195762 0.9825274 97  1.6516215\n#R|                p\n#R|  1  5.081712e-04\n#R|  2  3.423944e-02\n#R|  3  2.449833e-01\n#R|  4  5.102225e-01\n#R|  5  5.156691e-01\n#R|  6  1.540325e-03\n#R|  7  4.171601e-01\n#R|  8  1.371816e-04\n#R|  9  1.941692e-02\n#R|  10 4.600229e-02\n#R|  11 9.887411e-05\n#R|  12 1.018456e-01\n\n\nThe authors determined the significance of the regression with a Bonferroni correction, which they implemented by comparing the calculated p-value to 0.05/12, where 0.05 is the overall rejection criterion value and 12 is the number of p-values calculated. An alternative to this is to multiply the calculated p-values by 12 and then compare those adjusted p-values to 0.05 to determine significance. Either method produces the same result, but the second method, when implemented with p.adjust() provides flexibility to try methods other than Bonferroni (see Further Comments below).3\n3 Bonferroni-corrected p-values greater than 1 are displayed as 1.\nrsum &lt;- rsum |&gt;\n  mutate(p.bonf=p.adjust(p,method=\"bonferroni\"))\nrsum\n\n#R|     measure     slope     slopeSE   intercept       rsq df          t\n#R|  1      BDD 1.0314745 0.008749461  0.01070583 0.9930690 97  3.5973049\n#R|  2      CPL 1.0757713 0.035282303  0.15540677 0.9055193 97  2.1475736\n#R|  3      DOH 1.0285700 0.024424812 -0.44887045 0.9481393 97  1.1697113\n#R|  4      HLL 1.0144503 0.021863514  0.44980926 0.9568868 97  0.6609323\n#R|  5      MXL 0.9678116 0.049336389  1.16453067 0.7986760 97 -0.6524268\n#R|  6      OOL 0.8135066 0.057217285  1.82831841 0.6757449 97 -3.2593886\n#R|  7      PAD 0.9831316 0.020701457  0.18526295 0.9587653 97 -0.8148399\n#R|  8      PCL 1.1189512 0.029950963 -5.15899945 0.9350183 97  3.9715313\n#R|  9      POL 0.8662173 0.056282293  1.78522043 0.7094678 97 -2.3769948\n#R|  10     PVL 0.9062721 0.046369732  2.71996110 0.7974891 97 -2.0213164\n#R|  11     STL 1.0337756 0.008315711  0.01702215 0.9937626 97  4.0616555\n#R|  12     TTL 1.0228746 0.013849764 -7.17195762 0.9825274 97  1.6516215\n#R|                p      p.bonf\n#R|  1  5.081712e-04 0.006098055\n#R|  2  3.423944e-02 0.410873238\n#R|  3  2.449833e-01 1.000000000\n#R|  4  5.102225e-01 1.000000000\n#R|  5  5.156691e-01 1.000000000\n#R|  6  1.540325e-03 0.018483900\n#R|  7  4.171601e-01 1.000000000\n#R|  8  1.371816e-04 0.001646179\n#R|  9  1.941692e-02 0.233002989\n#R|  10 4.600229e-02 0.552027438\n#R|  11 9.887411e-05 0.001186489\n#R|  12 1.018456e-01 1.000000000\n\n\nThe authors used slope.test() from smatr to compute the p-value for the comparison of the estimated slope to 1. The code below uses sapply() to compute these p-values for each group. A comparison of the p column below to the one above shows that the results above are the same as those from smatr.\n\nsth &lt;- function(d) with(d,smatr::slope.test(imageBased,specimenBased,\n                                            test.value=1,method=\"OLS\"))$p\nsapply(dat2.split,FUN=sth) |&gt; \n  as.data.frame() |&gt;\n  rename(p=1)\n\n#R|                 p\n#R|  BDD 5.081712e-04\n#R|  CPL 3.423944e-02\n#R|  DOH 2.449833e-01\n#R|  HLL 5.102225e-01\n#R|  MXL 5.156691e-01\n#R|  OOL 1.540325e-03\n#R|  PAD 4.171601e-01\n#R|  PCL 1.371816e-04\n#R|  POL 1.941692e-02\n#R|  PVL 4.600229e-02\n#R|  STL 9.887411e-05\n#R|  TTL 1.018456e-01\n\n\nFigure 2 in O’Malley et al. (2021) showed the regression line, its slope, and the corresponding r2 value only for those measures for which the slope was significantly different from 1. To facilitate this, three new variables are created that contain the slope, the intercept, and the regression label only for those measures with slopes that differed significantly from 1.4 Use of HTML and markdown code in the label was described in this post.\n4 I also remove three variables that are not needed further.\nrsum &lt;- rsum |&gt;\n  mutate(b=ifelse(p.bonf&lt;0.05,intercept,NA),\n         m=ifelse(p.bonf&lt;0.05,slope,NA),\n         lbl=ifelse(p.bonf&lt;0.05,paste0(\"slope = \",round(slope,3),\n                                       \"&lt;br&gt;R^2^ = \",round(rsq,2)),\n                    NA)) |&gt;\n  select(-slopeSE,-df,-t)\nrsum\n\n#R|     measure     slope   intercept       rsq            p      p.bonf           b\n#R|  1      BDD 1.0314745  0.01070583 0.9930690 5.081712e-04 0.006098055  0.01070583\n#R|  2      CPL 1.0757713  0.15540677 0.9055193 3.423944e-02 0.410873238          NA\n#R|  3      DOH 1.0285700 -0.44887045 0.9481393 2.449833e-01 1.000000000          NA\n#R|  4      HLL 1.0144503  0.44980926 0.9568868 5.102225e-01 1.000000000          NA\n#R|  5      MXL 0.9678116  1.16453067 0.7986760 5.156691e-01 1.000000000          NA\n#R|  6      OOL 0.8135066  1.82831841 0.6757449 1.540325e-03 0.018483900  1.82831841\n#R|  7      PAD 0.9831316  0.18526295 0.9587653 4.171601e-01 1.000000000          NA\n#R|  8      PCL 1.1189512 -5.15899945 0.9350183 1.371816e-04 0.001646179 -5.15899945\n#R|  9      POL 0.8662173  1.78522043 0.7094678 1.941692e-02 0.233002989          NA\n#R|  10     PVL 0.9062721  2.71996110 0.7974891 4.600229e-02 0.552027438          NA\n#R|  11     STL 1.0337756  0.01702215 0.9937626 9.887411e-05 0.001186489  0.01702215\n#R|  12     TTL 1.0228746 -7.17195762 0.9825274 1.018456e-01 1.000000000          NA\n#R|             m                          lbl\n#R|  1  1.0314745 slope = 1.031&lt;br&gt;R^2^ = 0.99\n#R|  2         NA                         &lt;NA&gt;\n#R|  3         NA                         &lt;NA&gt;\n#R|  4         NA                         &lt;NA&gt;\n#R|  5         NA                         &lt;NA&gt;\n#R|  6  0.8135066 slope = 0.814&lt;br&gt;R^2^ = 0.68\n#R|  7         NA                         &lt;NA&gt;\n#R|  8  1.1189512 slope = 1.119&lt;br&gt;R^2^ = 0.94\n#R|  9         NA                         &lt;NA&gt;\n#R|  10        NA                         &lt;NA&gt;\n#R|  11 1.0337756 slope = 1.034&lt;br&gt;R^2^ = 0.99\n#R|  12        NA                         &lt;NA&gt;\n\n\nBelow, b and m will be used to show regressions lines on each facet, but for measures where these are NA no line will be shown. This same idea will be true for adding the results labels; i.e., measures where lbl is NA will not show a label. Thus, the regression line and results will only be shown in facets for measures with slopes significantly different from 1."
  },
  {
    "objectID": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#same-axes-in-each-facet",
    "href": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#same-axes-in-each-facet",
    "title": "O’Malley et al. (2021) Regression Plots",
    "section": "Same Axes in Each Facet",
    "text": "Same Axes in Each Facet\nThe last part of Figure 2 in O’Malley et al. (2021) that I could not recreate simply was to ensure that the x- and y-axes were the same (same limits and breaks) in each facet. An especially egregious example of my failure here is for the “OOL” facet.\nThis StackOverflow answer provided a method for making the x- and y-axes the same within each facet, but also the same across the facets. A simple modification of that answer results in the x- and y-axes being the same within each facet, but different across facets.\nThere are two “tricks” in this process. The first is to create a data fame that finds the minimum and maximum value of imageBased and specimenBased combined for each measure, then place those values in a single variable that is matched against the measure name, and then repeat that column in a second variable.\n\nfctlims &lt;- dat2 |&gt;\n  group_by(measure) |&gt;\n  summarize(min=min(imageBased,specimenBased),\n            max=max(imageBased,specimenBased)) |&gt;\n  pivot_longer(cols=min:max,values_to=\"x\") |&gt;\n  mutate(y=x) |&gt;\n  select(-name)\nFSA::headtail(fctlims)\n\n#R|     measure        x        y\n#R|  1      BDD  33.1800  33.1800\n#R|  2      BDD 104.7049 104.7049\n#R|  3      CPL  15.6600  15.6600\n#R|  22     STL 395.8014 395.8014\n#R|  23     TTL 173.8007 173.8007\n#R|  24     TTL 463.2827 463.2827\n\n\nThese “data” are then added to the plot from above using geom_blank() where x= and y= are mapped to the two columns of “min” and “max” values. This then sets the range of data to be presented on the x- and y-axes, and because the two columns are the same then the axes will appear the same. geom_blank() does not plot any actual data so this effectively just sets the x- and y-axis limits and breaks.\n\nggplot() +\n  geom_blank(data=fctlims,mapping=aes(x=x,y=y)) +\n  geom_abline(slope=1,intercept=0,linetype=\"dashed\",linewidth=0.75,color=\"gray50\") +\n  geom_point(data=dat2,mapping=aes(x=specimenBased,y=imageBased),\n             shape=1,color=\"deepskyblue4\") +\n  geom_abline(data=rsum,mapping=aes(intercept=b,slope=m),\n              color=\"black\",linewidth=1) +\n  ggtext::geom_richtext(data=rsum,mapping=aes(label=lbl),\n                        x=-Inf,y=Inf,vjust=1.1,hjust=-0.1,\n                        size=9/.pt,label.color=NA) +\n  scale_x_continuous(name=\"Specimen-based (mm)\",expand=expansion(mult=0.1)) +\n  scale_y_continuous(name=\"Image-based (mm)\",expand=expansion(mult=0.1)) +\n  facet_wrap(vars(measure),nrow=3,scales=\"free\") +\n  theme_bw() +\n  theme(panel.grid=element_blank())\n\n\n\n\n\n\n\n\nThis is pretty nice, but I still could not get the exact breaks in O’Malley et al. (2021). I started to try facetted_pos_scales() from ggh4x but lost my patience with it."
  },
  {
    "objectID": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#alternative-multiple-comparison-procedures",
    "href": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#alternative-multiple-comparison-procedures",
    "title": "O’Malley et al. (2021) Regression Plots",
    "section": "Alternative Multiple Comparison Procedures",
    "text": "Alternative Multiple Comparison Procedures\nThe Bonferroni-correction for multiple comparisons is a “brute-force” method that becomes “too conservative” (i.e., overall error rate less than 0.05), especially as the number of comparisons increases. This, ultimately results in a loss of statistical power (i.e., potentially detecting fewer truly significant results). The Bonferroni method is often employed because it is simple to implement as shown above – you either divide or multiply by the number of tests. However, p.adjust() provides the ability to use a wider variety of other methods that are less conservative than the Bonferroni.5 Below, the Holm method is demonstrated.\n5 See ?p.adjust for more explanation of these methods.\np.adjust.methods\n\n#R|  [1] \"holm\"       \"hochberg\"   \"hommel\"     \"bonferroni\" \"BH\"        \n#R|  [6] \"BY\"         \"fdr\"        \"none\"\n\nrsum |&gt;\n  mutate(p.holm=p.adjust(p,method=\"holm\")) |&gt;\n  select(measure,p,p.bonf,p.holm)\n\n#R|     measure            p      p.bonf      p.holm\n#R|  1      BDD 5.081712e-04 0.006098055 0.005081712\n#R|  2      CPL 3.423944e-02 0.410873238 0.239676055\n#R|  3      DOH 2.449833e-01 1.000000000 0.979933278\n#R|  4      HLL 5.102225e-01 1.000000000 1.000000000\n#R|  5      MXL 5.156691e-01 1.000000000 1.000000000\n#R|  6      OOL 1.540325e-03 0.018483900 0.013862925\n#R|  7      PAD 4.171601e-01 1.000000000 1.000000000\n#R|  8      PCL 1.371816e-04 0.001646179 0.001508997\n#R|  9      POL 1.941692e-02 0.233002989 0.155335326\n#R|  10     PVL 4.600229e-02 0.552027438 0.276013719\n#R|  11     STL 9.887411e-05 0.001186489 0.001186489\n#R|  12     TTL 1.018456e-01 1.000000000 0.509227920\n\n\nIn this situation the Bonferroni and Holm methods did not provide different conclusions, so I did not recreate the b, m, and lbl variables. However, alternative correction methods may provide different conclusions in other situations."
  },
  {
    "objectID": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#descriptive-facet-labels",
    "href": "blog/posts/2023-4-6_OMalleyetal2021_Fig2/index.html#descriptive-facet-labels",
    "title": "O’Malley et al. (2021) Regression Plots",
    "section": "Descriptive Facet Labels",
    "text": "Descriptive Facet Labels\nI believe that O’Malley et al. (2021) used the measure abbreviations in their Figure 2 to tie the results to their Figure 1, which showed the measures on a schematic fish. However, there may be situations where more descriptive facet labels may be useful. For example, below a named vector is created where the name (before the =) is the measure abbreviation used in measure and the “value” (after the =) is a more descriptive name for the measure.\n\nlongnames &lt;- c(\"BDD\"=\"Body Depth\",\n               \"CPL\"=\"Caudal Peduncle Length\",\n               \"DOH\"=\"Dorsal Fin Height\",\n               \"HLL\"=\"Head Length\",\n               \"MXL\"=\"Maxillary Length\",\n               \"OOL\"=\"Orbital Length\",\n               \"PAD\"=\"Pelvic-Anal Fin Length\",\n               \"PCL\"=\"Pectoral Fin Length\",\n               \"POL\"=\"Preorbital Length\",\n               \"PVL\"=\"Pelvic Fin Length\",\n               \"STL\"=\"Standard Length\",\n               \"TTL\"=\"Total Length\")\n\nThe abbreviations in the facet strip can be replaced with the longer names by setting labeller= in facet_wrap() to labeller() which has the variable name that creates the facets (i.e., measure here) set equal to the named vector just created.\n\nggplot() +\n  geom_abline(slope=1,intercept=0,linetype=\"dashed\",linewidth=0.75,color=\"gray50\") +\n  geom_point(data=dat2,mapping=aes(x=specimenBased,y=imageBased),\n             shape=1,color=\"deepskyblue4\") +\n  geom_abline(data=rsum,mapping=aes(intercept=b,slope=m),\n              color=\"black\",linewidth=1) +\n  ggtext::geom_richtext(data=rsum,mapping=aes(label=lbl),\n                        x=-Inf,y=Inf,vjust=1.1,hjust=-0.1,\n                        size=9/.pt,label.color=NA) +\n  scale_x_continuous(name=\"Specimen-based (mm)\",expand=expansion(mult=0.1)) +\n  scale_y_continuous(name=\"Image-based (mm)\",expand=expansion(mult=0.1)) +\n  facet_wrap(vars(measure),nrow=3,scales=\"free\",\n             labeller=labeller(measure=longnames)) +\n  theme_bw() +\n  theme(panel.grid=element_blank())"
  },
  {
    "objectID": "blog/posts/2023_2_14_Quistetal2022_BCData/index.html",
    "href": "blog/posts/2023_2_14_Quistetal2022_BCData/index.html",
    "title": "Quist et al. (2022) Back-Calculation Figure",
    "section": "",
    "text": "Introduction\nQuist et al. (2022) examined three structures (scales, sectioned otoliths, and whole otoliths) to estimate age of Yellowstone Cutthroat Trout (Oncorhynchus clarkii bouvieri). In a previous post I largely recreated their Figures 1 and 2 related to age precision and bias between readers and between structures. In this post I attempt to recreate their Figure 3 which examined back-calculated lengths-at-age between structures (and observed lengths-at-age).\n \n\n\nGetting Setup\nThe following packages are loaded for use below.\n\nlibrary(tidyverse)  # for dplyr, ggplot2 packages\n\nThe following ggplot2 theme was used below.1\n1 See this post for more information on creating and using ggplot2 themes.\ntheme_q &lt;- function(base_size=14) {\n  theme_bw(base_size=base_size) +\n    theme(\n      # margin for the plot\n      plot.margin=unit(c(0.5,0.5,0.5,0.5),\"cm\"),\n      # set axis label (i.e., title) colors and margins\n      axis.title.y=element_text(colour=\"black\",margin=margin(t=0,r=10,b=0,l=0)),\n      axis.title.x=element_text(colour=\"black\",margin=margin(t=10,r=0,b=0,l=0)),\n      # set tick label color, margin, and position and orientation\n      axis.text.y=element_text(colour=\"black\",margin=margin(t=0,r=5,b=0,l=0),\n                               vjust=0.5,hjust=1),\n      axis.text.x=element_text(colour=\"black\",margin=margin(t=5,r=0,b=0,l=0),\n                               vjust=0,hjust=0.5,),\n      # set size of the tick marks for y- and x-axis\n      axis.ticks=element_line(linewidth=0.5),\n      # adjust length of the tick marks\n      axis.ticks.length=unit(0.2,\"cm\"),\n      # set the axis size,color,and end shape\n      axis.line=element_line(colour=\"black\",linewidth=0.5,lineend=\"square\"),\n      # adjust size of text for legend\n      legend.text=element_text(size=12),\n      # remove grid\n      panel.grid=element_blank()\n    )\n}\n\n \n\n\nData\nUnfortunately data for constructing Figure 3 was not provided along with the published paper.2 Thus, I simulated similar data using my FSAsim package. Given that FSAsim is a work-in-progress I don’t show the code for simulating the data. However, the data can be downloaded as a CSV file from here.\n2 The supplement with the published paper only included the age estimates data.\n\n\nThe resultant df data frame has the following four variables:\n\nstrux: Identifies the structure used to assign age (sectioned otoliths or scales), or whether the data are observed lengths and ages.\nid: Unique fish identifier (not used in this analysis).\nage: Age-at-capture for observed data or an age back-calculated to for otoliths and scales.\nlen: Length-at-capture for observed data or a back-calculated length for otoliths and scales.\n\n\nFSA::peek(df,n=10)\n\n#R|         strux  id age len\n#R|  1   otoliths   1   1 178\n#R|  80  otoliths  25   4 326\n#R|  159 otoliths  53   3 464\n#R|  239 otoliths  73   1 202\n#R|  319 otoliths  95   5 371\n#R|  398   scales  19   1 262\n#R|  478   scales  46   1 254\n#R|  558   scales  77   2 283\n#R|  637 observed  20   6 444\n#R|  717 observed 100   3 353\n\n\n \n\n\nRecreating Figure 3\nFigure 3 in Quist et al. (2022) is a boxplot. geom_boxplot() in ggplot2 requires that the x-axis variable be “discrete” (or categorical). Thus, a new variable, fage, is created that is a factored (i.e., categorical) version of age. Additionally, the order of strux is alphabetical by default, which is not the order plotted by Quist et al. (2022). Thus, strux is modified below to set the order of the levels.\n\ndf &lt;- df |&gt;\n  mutate(fage=factor(age),\n         strux=factor(strux,levels=c(\"scales\",\"otoliths\",\"observed\")))\n\nThe default use of geom_boxplot() gets us close to Figure 3 in Quist et al. (2022).\n\nggplot(data=df,mapping=aes(x=fage,y=len,fill=strux)) +\n  geom_boxplot() +\n  scale_x_discrete(name=\"Age (yrs)\") +\n  scale_y_continuous(name=\"Length (mm)\",\n                     limits=c(0,600),breaks=seq(0,600,100),\n                     expand=expansion(mult=0)) +\n  theme_q()\n\n\n\n\n\n\n\n\nHowever, we need to adjust the colors used, remove the legend, narrow the boxes, put “caps” on the ends of the whiskers, and change the outliers to open circles.\nThe colors to be used are defined in the named vector clrs below. This will be given to scale_fill_manual() below.\n\nclrs &lt;- c(\"scales\"=\"white\",\"otoliths\"=\"gray80\",\"observed\"=\"gray40\")\n\nThe width of the boxes is set in boxwid and the width of the “caps” on the whisker is set at 80% of this box width. The distance between adjacent boxes at the same age is controlled with position= with a function called position_dodge(). It was not clear to me what to set these values to but I settled on the following after a little back-and-forth tinkering.3\n3 The dodge width should be larger than the box width so that the boxes don’t touch.\nboxwid &lt;- 0.4                       # box width\ncapwid &lt;- 0.8*boxwid                # whisker cap width\nboxsep &lt;- position_dodge(width=0.5) # dodge amount between boxes\n\nThe use of stat_boxplot() below is a trick to put an “error bar” behind the boxplot, which gives the impression that the whiskers are “capped” (i.e., the error bar caps are in the same place that the whisker caps would be). The shape4, boundary color, and fill for the outlier marks are defined with outlier.shape=, outlier.color=, and outlier.fill=, respectively. Finally, show.legend=FALSE in geom_boxplot() is used to remove the legend.\n4 “21” means a circle that has both a boundary and a fill color.\nggplot(data=df,mapping=aes(x=fage,y=len,fill=strux)) +\n  stat_boxplot(geom=\"errorbar\",width=capwid,position=boxsep) +\n  geom_boxplot(width=boxwid,position=boxsep,\n               outlier.shape=21,outlier.color=\"black\",outlier.fill=\"white\",\n               show.legend=FALSE) +\n  scale_fill_manual(values=clrs) +\n  scale_x_discrete(name=\"Age (yrs)\") +\n  scale_y_continuous(name=\"Length (mm)\",\n                     limits=c(0,600),breaks=seq(0,600,100),\n                     expand=expansion(mult=0)) +\n  theme_q()\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nReferences\n\nQuist, M. C., D. K. McCarrick, and L. M. Harris. 2022. Comparison of structures used to estimate age and growth of Yellowstone Cutthroat Trout. Journal of Fish and Wildlife Management 13(2):544–551.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h. ogle2023,\n  author = {H. Ogle, Derek},\n  title = {Quist Et Al. (2022) {Back-Calculation} {Figure}},\n  date = {2023-02-14},\n  url = {https://fishr-core-team.github.io/fishR//blog/posts/2023_2_14_Quistetal2022_BCData},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2023, February 14. Quist et al. (2022) Back-Calculation\nFigure. https://fishr-core-team.github.io/fishR//blog/posts/2023_2_14_Quistetal2022_BCData."
  },
  {
    "objectID": "blog/posts/2024-2-6_LVB_Stan/index.html",
    "href": "blog/posts/2024-2-6_LVB_Stan/index.html",
    "title": "Bayesian LVB II - rstan",
    "section": "",
    "text": "Introduction\nThe use of Bayesian inference in fisheries biology has been increasing. For good reason, as there are many benefits to taking a Bayesian approach. I won’t go into those reasons here but you can read about them in Dorazio (2016) and Doll and Jacquemin (2018). This post assumes you have already decided to use Bayesian methods and will present how to estimate parameters of the von Bertalanffy growth model. Previous posts describe frequentist methods here and here\nThis is the second post where I describe how to fit a model using Bayesian inference. The first post used Stan with the brms package. This post will use rstan and I will write the full Stan model code.\nBoth methods will fit the typical three parameter von Bertalanffy growth model\n\\[\nTL_i=L_\\infty * (1-e^{(-\\kappa * (t_i-t_0))} )\n\\]\nwhere \\(TL_i\\) is total length of individual i, \\(L_\\infty\\) is the average maximum length obtained, \\(\\kappa\\) is the Brody growth coefficient, \\(t_i\\) is the age of individual i, and \\(t_0\\) is the theoretical age at zero length. To finish the model, an error term is added:\n\\[\nTL_i=L_\\infty * (1-e^{(-\\kappa * (t_i-t_0))} + \\epsilon_i)\n\\] \\[\n\\epsilon_i \\sim normal(0,\\sigma)\n\\] where \\(\\epsilon_i\\) is a random error term for individual i with a mean of 0 and standard deviation \\(\\sigma\\).\n\n\nPrior probabilities\nAt the heart of Bayesian analysis is the prior probability distribution.This post will use non-informative prior probability distributions. When and how to use informative priors when fitting a von Bertalanffy growth model will be discussed in a future post. However, you can read about it in Doll and Jacquemin (2018). The prior probability distributions used in this post are:\n\n\n\nParameter\nPrior Probability Distribution\n\n\n\n\n\\(L_\\infty\\)\nnormal(0,1000)\n\n\n\\(\\kappa\\)\nnormal(0,10)\n\n\n\\(t_0\\)\nnormal(0,10)\n\n\n\\(\\sigma\\)\nstudent-t(3,0,30)\n\n\n\nStan parameterizes the normal distribution with the mean and standard deviation and the student-t distribution with the degrees of freedom, mean, and standard deviation\n\n\nPreliminaries\nFirst step is to load the necessary packages\n\nlibrary(FSA)\nlibrary(FSAdata)   # for data\nlibrary(dplyr)     # for filter(), select()\nlibrary(tidyr)     # for transposing data wide to long\nlibrary(ggplot2)   # for potting\nlibrary(rstan)     # for fitting Stan models\nlibrary(tidybayes) # for plotting posterior results\nlibrary(bayesplot) # for plotting posterior predictive checks\n\n\n\nData\nThe WalleyeErie2 data available in the FSAdata package was used in previous posts demonstrating von Bertalanffy growth models and will once again be used here. These data are Lake Erie Walleye (Sander vitreus) captured during October-November, 2003-2014. As before, the primary interest here is in the tl (total length in mm) and age variables. The data will also be filtered to focus only on female Walleye from location 1 captured in 2014.\n\ndata(WalleyeErie2,package=\"FSAdata\")\nwf14T &lt;- WalleyeErie2 %&gt;%\n  filter(year==2014,sex==\"female\",loc==1) %&gt;%\n  select(-year,-sex,-setID,-loc,-grid)\nheadtail(wf14T)\n\n#R|       tl    w      mat age\n#R|  1   445  737 immature   2\n#R|  2   528 1571   mature   4\n#R|  3   380  506 immature   1\n#R|  323 488 1089 immature   2\n#R|  324 521 1408   mature   3\n#R|  325 565 1745   mature   3\n\n\nNext step is to create a list to hold the data used in the model. One important note, the items in this list must match the data used in the model code. For example, we will use tl for total length in the data list and tl must be specified in the Stan data block exactly as tl.\n\ndataList=list(\n  tl=wf14T$tl,\n  age=wf14T$age ,\n  N=length((wf14T$age))\n)\n\n\n\nInitial values\nAn optional step is to specify initial values for the parameters. It is always good practice to specify initial values, particularly with non-linear and other complex models. I will fit the model using multiple chains and it is advisable to use different starting values for each chain. To accomplish this, we will specify a function and use a random number generator for each parameter Adjust the range for the uniform distribution to cover a large range of values that make sense for your data. You can use other distributions as long as they match the declared range in the model code. In this example, I am using the random uniform function because \\(L_\\infty\\) and \\(\\kappa\\) are restricted to be positive in the model. Therefore, the starting value must be positive.\n\ninitsLst &lt;- function() list(\n  Linf=runif(1, 200, 800),\n  K=runif(1, 0.05, 3.00),\n  t0 =rnorm(1, 0, 0.5)\n)\n\n\n\nStan\nNow we can introduce the Stan model code. Stan models are structured with “blocks”. Generally, the minimum number of blocks used is three; data, parameters, and model blocks. In our example, we will use a fourth block, the generated quantities block. All of the blocks must be in a single .stan file. The blocks are presented separately here for demonstration. Other blocks that can be used in Stan are; functions, transformed data, and transformed parameters. See the Stan manual for a description of each.\nThe data block declares all of the data needed for the model and the data object must match how they are entered in the data list above. There are several different types of data that can be declared. For this example, I use int to declare N, which is the number of observations, as an integer. You’ll also notice that I include a lower bounds for this variable. This isn’t necessary but good practice if your variable has a floor or ceiling. tl is declared as a real data type. The real data type is typically used for single numbers or a vector of numbers. This example declares the real data type as a vector of length N. age is another vector of length N but it is a vector of integers, int.\n\ndata {\n  int&lt;lower=0&gt; N ;  // number of observations\n  real tl[N] ;      // Total lengths of individual fish\n  int age[N] ;      // Age of individual fish\n}\n\nThe second block in our model is a parameters block. This block will declare all the estimated parameters in the model. Our model has four parameters. sigma_y is the standard deviation, \\(\\sigma\\), of the individual observations; t0 is the \\(t_0\\) LVB parameter; Linf is the \\(L_\\infty\\) parameter; and k is the \\(\\kappa\\) parameter. Each is declared as a single real value. All of the parameters are constrained in at least one direction. sigma_y is constrained to be positive using the &lt;lower=0&gt; argument because standard deviations must be positive. t0 is being constrained to be between -5 and 5 using the &lt;lower=-5&gt; and &lt;upper=5&gt; arguments. This is declared based on prior knowledge to prevent the sampler from searching outside these bounds. The remaining two, Linf and k, are only constrained to be positive using &lt;lower=0&gt;.\nSetting bounds can help the sampler find the posterior distribution, particularly with complex models. So it is recommended to always set bounds that “make sense”. Use your knowledge of the system and the model to declare these bounds. This is separate from declare your prior probability distribution but setting bounds will influence your prior probability distributions. The priors are set in the model block.\n\nparameters {\n  real&lt;lower=0&gt; sigma_y;              //LVB standard deviation\n  real&lt;lower=-5, upper=5&gt; t0;         //LVB t0 parameter\n  real&lt;lower=0&gt; Linf;                 //LVB Linf parameter\n  real&lt;lower=0&gt; k;                    //LVB k parameter\n}\n\nThe third block declares the model. The model block starts off with declaring any local variables that might be needed. Note that any variables declared in the model block can’t be referenced in other blocks. In this case, I need a vector ypred of length N to hold the mean total length which is a function of the von Bertlanffy growth model. This is followed by our declaration of prior probability distributions. Stan will automatically assign uniform prior probabilities if a prior is not specified here. I don’t recommend letting Stan declare uniform priors by default. Always be explicit in the prior probability distribution in your model and assign prior probability distributions that are suitable for the parameter and defensible against a skeptical audience. A future post will dive deeper into setting and selecting informative vs non-informative prior probability distribution.\nThe final part of the model block is the likelihood. This will include a for loop that will iterate through each of the observed values and include the model. Following this is the normal() distribution which specifies the distribution of our observations and is parameterized with the mean y_mean and standard deviation sigma_y. This last line can be indexed over individuals and included in the for loop. Or vectorized outside the loop, as it is written here.\n\nmodel {\n  vector[N] y_mean; //variable used in the model block only\n\n  //The next three lines of code specify reference prior probability distributions.\n  sigma_y ~ student_t(3,0,40);  \n  Linf ~ normal(0, 1000);\n  k ~ normal(0, 100);\n  t0 ~ normal(0, 10); \n  \n  // calculate likelihood of data\n  for(i in 1:N){\n    y_mean[i]= Linf * (1-exp(-(k * (age[i]-t0) )) );\n  }\n  tl~normal(y_mean, sigma_y);\n}\n\nThe last block needed is a generated quantities block. This block is used to calculate derived variables, posterior predictions, and to generate model predictions. I will use it here to generate a vector named y_rep of length N that will hold individual predicted observations. y_rep is the posterior predicted values of total length that will be used to plot the observed and model predicted total length. The posterior predicted distribution is generated using the normal_rng function which generates a random value from the normal distribution based on a mean (the model) and standard deviation. The posterior predicted distribution generated in in a for loop to iterate through each observed individual.\n\ngenerated quantities{\n  //the next four lines of code generate predicted values to use for inspecting model fit\n  vector[N] y_rep;\n  for(i in 1:N){\n    y_rep[i] = normal_rng(Linf * (1-exp(-(k * (age[i]-t0) )) ) ,sigma_y);\n  }\n}\n\nBefore sending the model to rstan, the full model code needs to be saved into a file with the .stan extension. For example “Walleye_lvb.stan”. The .stan file also has to be saved in the working directory.\nNow it is time to bundle all the pieces together.\n\n#Compile and fit model using Stan \nLVBfit &lt;- stan(file='Walleye_lvb.stan', # The file containing the Stan model\n                data=dataList,          # a list containing the data\n                init=initsLst,          # a list containing initial values \n                chains=3,               # number of chains, typically 3 to 4\n                cores=3,                # number of cores for multi-core processing;\n                                        #  Typically set to match number of chains\n                iter=3000 ,             # number of iterations\n                warmup=1000 ,           # number of warm up steps to discard\n                control=list(adapt_delta=0.80,  # Adjustments to the algorithm\n                             max_treedepth=15)) # to improve convergence.\n\n\n\nAssess convergence\nLet’s inspect convergence and how well the model fits the data. The first thing to do is to examine the trace plots and histograms of the important model parameters.\n\nplot(LVBfit, plotfun=\"trace\", pars=c(\"sigma_y\",\"Linf\",\"k\",\"t0\"), inc_warmup=FALSE)\n\n\n\n\n\n\n\n\n\nplot(LVBfit, plotfun=\"hist\", pars=c(\"sigma_y\",\"Linf\",\"k\",\"t0\"), inc_warmup=FALSE)\n\n\n\n\n\n\n\n\nThe chains appear to have mixed well for all parameters and reached a stationary posterior. This is seen by a unimodal distribution in the histograms and caterpillar plots for each parameter appear “on top” of each other. We can move on to assessing how well the model fits the data using a posterior predictive check.\n\ny_rep &lt;- as.matrix(LVBfit, pars=\"y_rep\") #Extract the posterior predicted observations\nppc_dens_overlay(wf14T$tl, y_rep[1:50, ])  #Density plot of y and y_rep\n\n\n\n\n\n\n\n\nThe y_rep curves do a good job at covering our observations and we can conclude the model does a sufficient job at representing the data. Now we can move on to look at individual parameters.\n\n\nPosterior summary\nBecause we included a generated quantities block and are calculating the posterior predicted distribution, our output if very long. The summary(LVBfit) command would return a vary long summary of all the parameters and individual predicted total lengths. Instead, I use the print() function here and only specify the parameters of interest.\n\nprint(LVBfit, pars=c(\"sigma_y\",\"Linf\",\"k\",\"t0\"), probs=c(0.025,0.5,0.975),digits_summary=6)\n\n#R|  Inference for Stan model: anon_model.\n#R|  3 chains, each with iter=3000; warmup=1000; thin=1; \n#R|  post-warmup draws per chain=2000, total post-warmup draws=6000.\n#R|  \n#R|                mean  se_mean        sd       2.5%        50%     97.5% n_eff\n#R|  sigma_y  18.374523 0.014576  0.697872  17.056527  18.351687  19.78513  2292\n#R|  Linf    648.420363 0.255814 10.053525 629.794538 648.129750 669.59872  1544\n#R|  k         0.361929 0.000548  0.021013   0.321538   0.361925   0.40304  1471\n#R|  t0       -1.286594 0.002216  0.087222  -1.466487  -1.282706  -1.12793  1550\n#R|              Rhat\n#R|  sigma_y 1.002078\n#R|  Linf    1.001539\n#R|  k       1.001829\n#R|  t0      1.001628\n#R|  \n#R|  Samples were drawn using NUTS(diag_e) at Tue May  7 08:07:37 2024.\n#R|  For each parameter, n_eff is a crude measure of effective sample size,\n#R|  and Rhat is the potential scale reduction factor on split chains (at \n#R|  convergence, Rhat=1).\n\n\nThe summary table provides the point estimates, 95% credible intervals, Rhat values, and n_eff. The Rhat values are another check to assess model convergence. It is generally accepted that you want Rhat values less than 1.10. The n_eff is also used to assess convergence. n_eff refers to “Effective Sample Size”. Because of the nature of MCMC methods, each successive sample from the posterior will typically be autocorrelated within a chain. Autocorrelation within the chains can increase uncertainty in the estimates. One way to assessing how much autocorrelation is present and big of an effect it might be, is with the “Effective Sample Size”. ESS represents the number of independent draws from the posterior. The ESS will be lower than the actual number of draws and you are looking for a high ESS. It has been recommended that an ESS of 1,000 for each parameter is sufficient Bürkner (2017).\n\n\nPosterior plotting\nThe final part is to create a figure with observed and predicted values. This code block is a little different than what was used with brms. Instead of add_predicted_draw(), I have to combine the predicted draws manually then convert them from wide to long.\n\ncbind(tl=wf14T$tl, age=wf14T$age,data.frame(t(y_rep))) %&gt;%   # Combine observed data with predictions\n        pivot_longer(col=-c(tl,age),values_to=\".prediction\") %&gt;% \n  ggplot(aes(x=age, y=tl)) +  \n  stat_lineribbon(aes(y=.prediction), .width=c(.95, .80, .50),  # regression line and CI\n                  alpha=0.5, colour=\"black\") +\n  geom_point(data=wf14T, colour=\"darkblue\", size=3) +   # raw data\n  scale_fill_brewer(palette=\"Greys\") +\n  ylab(\"Total length (mm)\\n\") + \n  xlab(\"\\nAge (years)\") +\n  theme_bw() +\n  theme(legend.title=element_blank(),\n        legend.position=c(0.15, 0.85))\n\n\n\n\n\n\n\n\nThe figure above shows the observed data in blue circles, the prediction line as a solid black line, and the posterior prediction intervals (0.50, 0.80, and 0.95) in different shades of gray.\n\n\n\n\n\nReferences\n\nBürkner, P.-C. 2017. Brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software 80:1–28.\n\n\nDoll, J. C., and S. J. Jacquemin. 2018. Introduction to Bayesian modeling and inference for fisheries scientists. Fisheries 43(3):152–161.\n\n\nDorazio, R. M. 2016. Bayesian data analysis in population ecology: Motivations, methods, and benefits. Population Ecology 58:31–44.\n\nReuseCC BY 4.0CitationBibTeX citation:@online{doll2024,\n  author = {Doll, Jason},\n  title = {Bayesian {LVB} {II} - Rstan},\n  date = {2024-02-06},\n  url = {https://fishr-core-team.github.io/fishR/blog/posts/2024-2-6_LVB_Stan/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDoll, J. 2024, February 6. Bayesian LVB II - rstan. https://fishr-core-team.github.io/fishR/blog/posts/2024-2-6_LVB_Stan/."
  },
  {
    "objectID": "pages/books.html",
    "href": "pages/books.html",
    "title": "Fisheries-Related R Books",
    "section": "",
    "text": "You may submit additions or corrections by using the links at the bottom of the sidebar menu to the right. Thank you for your contribution to this resource.\n\n\n\n \n\n\n\n\nAge and Growth of Fish: Principles and Techniques\n\n\n\n\n\n\n\n\nEditors: Michael Quist and Daniel Isermann\nYear: 2017\nBook Webpage\nR Resources\nReviews: Here and here.\n\n\n\n\n \n\n\n\n\n\nAnalysis and Interpretation of Freshwater Fisheries Data I\n\n\n\n\n\n\n\n\nEditors: Christopher Guy and Michael Brown\nYear: 2007\nBook Webpage\nR Resources\nReviews: Here (2nd page), here (paywall), and here (paywall)\n\n\n\n\n \n\n\n\n\n\nIntroductory Fisheries Analyses with R\n\n\n\n\n\n\n\n\nAuthor: Derek H. Ogle\nYear: 2015\nBook Webpage\nR Resources: within book link and related package\nReviews: Here (paywall)\n\n\n\n\n \n\n\n\n\n\nUsing R for Modelling and Quantitative Methods in Fisheries\n\n\n\n\n\n\n\n\nAuthor: Malcolm Haddon\nYear: 2020\nBook Webpage\nR Resources: within book link and related package"
  },
  {
    "objectID": "pages/data_Comp_byPackage.html",
    "href": "pages/data_Comp_byPackage.html",
    "title": "Fisheries-related datasets in CRAN packages",
    "section": "",
    "text": "Datasets from packages not controlled by the fishR Core Team (i.e., FSA and FSAdata) are entered manually, so please submit a GitHub Issue (see link at bottom of right sidebar menu) with any corrections or additions.\n\n\n\n \n\nalr3\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nlakemary\nAge and length of Bluegill from Lake Mary, MN\n\n\nwalleye\nAge and length of Walleye from Butternut Lake, WI in 3 different management periods\n\n\nwblake\nAge and length of Smallmouth Bass from West Bearsking Lake, MN (exclude age-9 and older)\n\n\nwblake2\nAge and length of Smallmouth Bass from West Bearsking Lake, MN (all fish)\n\n\n\n \n\n\nAquaticLifeHistory\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\ngrowth_data\nLength-at-age data for common blacktip sharks (Carcharhinus limbatus) from Indonesia\n\n\nmaturity_data\nLength-at-maturity and age-at-maturity data for female silky sharks (Carcharhinus falciformis) from Papua New Guinea\n\n\n\n \n\n\nCatDyn\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\ngayhake\nIndustrial and Artisanal Catch and Effort Data from the Chilean Hake Fishery\n\n\ngayhakelm\nLength and Month data of Chilean hake from Artisanal Fishery\n\n\ngayhakelw\nLength and Weight data of Chilean hake from Artisanal Fishery\n\n\nlolgahi\nIndustrial Trawling Data from the Squid Fishery of the Falkland Islands\n\n\ntwelver\nExhaustive recorded operational activity of the Anguilla japonica sport fishery in Taiwan during the 1983-1984 season\n\n\n\n \n\n\nCOUNT\n\n\n\nDataset\nDescription\n\n\n\n\nfishing\nSite abundance and characteristics from commercial fishing\n\n\n\n \n\n\nfishkirkko2015\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nfishkirkkojarvi2015\nWeight-length data for six freshwater species from Lake Kirkkojarvi, Finland\n\n\n\n \n\n\nfishmethods\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nGerking\nMarked and released sunfish in an Indiana lake for 14 days by Gerking (1953)\n\n\nHightower\nThe complete capture histories of striped bass for Lake Gaston, North Carolina\n\n\nJensen\nThe age data are from reconstructed catches of lake whitefish reported by Jensen (1996) in Table 1 and were expanded to individual observations from the age frequency table\n\n\nKimura\nLength and age data for male and female Pacific Hake\n\n\nP.donacina\nGrowth increment data derived from a tagging experiment on Paphis donacina\n\n\nShepherd\nSeasonal length frequency data of Raja clavata\n\n\nalkdata\nAge-length key (summarized form) for Gulf of Hauraki snapper shown in Table 8.3 of Quinn and Deriso (1999)\n\n\nbonito\nGrowth increment data derived from tagging experiments on Pacific bonito (Sarda chiliensis)\n\n\ncatch\nNumber of cod captured in 10 standardized bottom trawl hauls from Massachusetts, 1985\n\n\ncodcluslen\nLengths of Atlantic cod caught during Massachusetts Division of Marine Fisheries bottom trawl survey, spring 1985\n\n\ncodstrcluslen\nLengths of Atlantic cod caught during Massachusetts Division of Marine Fisheries stratified random bottom trawl survey, spring 1985\n\n\ncounts\nRun size data of alewife (Alosa pseudoharengus) in Herring River, MA from 1980-2010\n\n\ncowcod\nCatch data (metric tons) for cowcod Sebastes levis 1900 to 2008\n\n\ndarter\nSequence of catch data for the faintail darter from removal experiments by Mahon\n\n\ngoosefish\nThe mean lengths by year and number of observations for length&gt;=smallest length at first capture (Lc) for northern goosefish used in Gedamke and Hoenig (2006)\n\n\nhaddock\nAge, weight at spawning, partial recruitment, and fraction mature data for haddock (Melanogrammus aeglefinus) used by Gabriel et al. (1989) to calculate spawning stock biomass-per-recruit\n\n\nkappenman\nPacific cod catch per effort from Table 1 in Kappenman (1999)\n\n\nlingcod\nCatch data (metric tons) for lingcod 1889 to 2001\n\n\nmaki\nFrom Table 1 of Maki et al. (2001) for 3 years combined\n\n\nmenhaden\nAge, fecundity-at-age, partial recruitment, fraction mature, and nautral mortality data for menhaden to calculate eggs-per-recruit\n\n\nnshrimp\nRecruit and postrecruit survey indices and catch data for Gulf of Maine northern shrimp (Pandulus borealis), 1985-2007\n\n\npinfish\nLength, age and sex data for pinfish (Lagodon rhomboides) from Tampa Bay, Florida\n\n\nrig\nTagging growth increment data for New Zealand rig (Mustelus lenticulatus), after removal of outliers\n\n\nrockbass\nAge data from a sample of rock bass trap-netted from Cayuga Lake, New York\n\n\nsblen\nTotal length (inches) of striped bass collected by Massachusetts volunteer anglers in 2014\n\n\nsbotos\nAges of striped bass interpreted from the same otolith sections by two age readers\n\n\nsole\nFlathead sole CPUEs for a side-by-side trawl calibration study of National Marine Fisheries Service (NMFS) and Alaska Department of Fish and Game (ADFG) vessels\n\n\nstriper\nEstimates of recruits and female spawning stock biomass for striped bass from the Atlantic State Marine Fisheries 2016 stock assessment\n\n\ntrout\nRelease lengths, recapture lengths and times-at-large for trout in the Kenai River\n\n\nwolffish\nSpring untransformed mean catch per tow for wolffish (Anarhichas lupus)\n\n\nyellowtail\nFall average catch per tow for southern New England yellowtail flounder\n\n\n\n \n\n\nfishMod\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nTigerFlathead\nData set arising from a south-east Australia fish survey, see Bax and Williams (1999)\n\n\n\n \n\n\nFSA\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nBluegillJL\nCapture histories (2 samples) of Bluegill from Jewett Lake, MI.\n\n\nBrookTroutTH\nCatch-at-age for Tobin Harbor, Isle Royale Brook Trout.\n\n\nChinookArg\nLengths and weights for Chinook Salmon from three locations in Argentina.\n\n\nCodNorwegian\nStock and recruitment data for Norwegian cod, 1937-1960.\n\n\nCutthroatAL\nCapture histories (9 samples) of Cutthroat Trout from Auke Lake.\n\n\nEcoli\nPopulation growth of Escherichia coli.\n\n\nMirex\nMirex concentration, weight, capture year, and species of Lake Ontario salmon.\n\n\nPSDlit\nGabelhouse five-cell length categories for various species.\n\n\nPikeNY\nSummarized multiple mark-recapture data for all Northern Pike from Buckhorn Marsh, NY.\n\n\nPikeNYPartial1\nCapture histories (4 samples), in capture history format, of a subset of Northern Pike from Buckhorn Marsh, NY.\n\n\nSMBassLS\nCatch-effort data for Little Silver Lake (Ont) Smallmouth Bass.\n\n\nSMBassWB\nGrowth increment data for West Bearskin Lake, MN, Smallmouth Bass.\n\n\nSpotVA1\nAge and length of spot.\n\n\nWR79\nAges and lengths for a hypothetical sample from Westerheim and Ricker (1979).\n\n\nWSlit\nAll known standard weight equations.\n\n\nWhitefishLC\nAssigned ages from two readers on three structures for Lake Whitefish from Lake Champlain.\n\n\n\n \n\n\nFSAdata\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nAHerringChile\nAges and lengths of Araucanian Herring from Chilean waters.\n\n\nAfricanRivers\nCharacteristics of a sample of West African rivers.\n\n\nAlewifeLH\nAges of Lake Huron Alewife assigned from otoliths and scales.\n\n\nAnchovetaChile\nAges and lengths of Anchoveta from Chilean waters.\n\n\nBGHRfish\nFish information from samples collected from Big Hill Reservoir, KS, 2014.\n\n\nBGHRsample\nInformation for each electrofishing sample from Big Hill Reservoir, KS, 2014.\n\n\nBSkateGB\nStock and recruitment data for Barndoor Skate from Georges Bank, 1966-2007.\n\n\nBassFL\nCatch-at-age for Suwanee and Largemouth Bass.\n\n\nBlackDrum2001\nBiological data for Black Drum from Virginia waters of the Atlantic Ocean, 2001.\n\n\nBloaterLH\nStock and recruitment data for Lake Huron Bloaters, 1981-1996.\n\n\nBlueCatfish\nAges and lengths of Blue Catfish.\n\n\nBlueCrab\nCatch and effort data for male Blue Crabs.\n\n\nBluefishAge\nAges of Bluefish assigned from otoliths by two readers.\n\n\nBluegillIL\nLength-at-marking and recapture and time-at-large of Bluegill.\n\n\nBluegillLM\nLengths and weights for Bluegill from Lake Mary, MN.\n\n\nBluntnoseIL1\nSubampled lengths of Bluntnose Minnows from Inch Lake, WI.\n\n\nBonito\nAges and lengths of Australian Bonito.\n\n\nBrookTroutNC\nStock and recruitment data for Brook Trout from Ball Creek, NC, 1991-2004.\n\n\nBrookTroutNEWP\nCatches in removal events for Brook Trout in the Nashwaak Experimental Watersheds Project.\n\n\nBrookTroutNEWP1\nCatches in removal events for Brook Trout in the Nashwaak Experimental Watersheds Project.\n\n\nBrookTroutOnt\nSummarized single mark-recapture data for Brook Trout across many years.\n\n\nBrownTroutVC1\nSingle census mark-recapture data with lengths for Brown Trout from Valley Creek, MN.\n\n\nBullTroutRML1\nLengths and weights for Bull Trout from two Rocky Mountain lakes and two eras.\n\n\nBullTroutRML2\nAges and lengths of Bull Trout from two Rocky Mountain lakes at two times.\n\n\nBullTroutTC\nCatch-at-age for Bull Trout in Trestle Creek, ID.\n\n\nCCatfishNB\nCatch-at-age of Channel Catfish from two sections of the Platte River, NB.\n\n\nCabezon\nAges, lengths, and maturity for female Cabezon from Oregon.\n\n\nCasselman1990\nInstantaneous growth rates for two calcified ageing structures.\n\n\nChinookKR\nStock and recruitment data for Klamath River Chinook Salmon, 1979-2000.\n\n\nCiscoTL\nLengths, weights, and sex of Cisco from Trout Lake, WI.\n\n\nCrappieARMS\nStock and recruitment data for Crappies from four reservoirs in Arkansas and Mississippi, USA.\n\n\nCreekChub\nAges (subsample) and lengths (all fish) for Creek Chub.\n\n\nCreelMN\nResults of a large number of creel surveys in Minnestoa lakes.\n\n\nCroaker1\nAges of Atlantic Croaker assigned from otoliths by two readers.\n\n\nCroaker2\nAges, lengths, and sexes of Atlantic Croaker by sex.\n\n\nCutthroatALf\nCapture histories (9 samples) of Cutthroat Trout from Auke Lake.\n\n\nDarterMahon\nCatch and effort data for Fantail Darter.\n\n\nDarterOnt\nAges and lengths of Channel Darters from two locations.\n\n\nDeckeretal1999\nCatches in removal events of Cutthroat Trout and Coho Salmon in Little Stawamus Creek (British Columbia, Canada) in 1997.\n\n\nEuroPerchTJ\nAges, lengths, and sexes of European Perch.\n\n\nFHCatfish\nCatch-at-age of Flathead Catfish from three southeastern rivers.\n\n\nFHCatfishATL\nCatch-at-age of Flathead Catfish from three Atlantic rivers.\n\n\nFSAdata\nData to support the FSA package.\n\n\nFWDrumLE1\nAges and lengths of Lake Erie Freshwater Drum.\n\n\nFWDrumLE2\nAges (subsample) and lengths (all fish) for Freshwater Drum from Lake Erie.\n\n\nGhats\nSpecies accumulation data for fish of the Western Ghats of India.\n\n\nGreensCreekMine\nCatches in removal events of Coho Salmon and Dolly Varden Char at various locations near the Greens Creek (AK) Mine site.\n\n\nHake\nStock and recruitment data for Hake, 1982-1996.\n\n\nHalibutPAC\nStock and recruitment data for Pacific Halibut, 1929-1991.\n\n\nHerman\nLengths for Walleye, Yellow Perch, Black Crappie, and Black Bullheads from Lake Herman, SD.\n\n\nHerringBWE\nStock and recruitment data for Blackwater Estuary Herring, 1962-1997.\n\n\nHerringISS\nStock and recruitment data for Icelandic summer spawning Herring, 1946-1996.\n\n\nHumpbackWFCR\nCapture histories (2 sample) of Humpback Whitefish.\n\n\nInchLake1\nLengths for all fish captured in Inch Lake, WI, in two years\n\n\nInchLake2\nLengths and weights for fish captured in Inch Lake\n\n\nJobfishSIO\nCatch and effort data for South Indian Ocean Jobfish.\n\n\nJonesStockwell\nCatches in removal events of Brown and Rainbow Trout at various locations.\n\n\nJonubi1\nAges and lengths of male Jonubi.\n\n\nJonubi2\nAges (subsample) and lengths (all fish) of Jonubi.\n\n\nKingCrabAK\nStock and recruitment data for Red King Crab in Alaska, 1960-2004.\n\n\nLJCisco\nAges and lengths of Longjaw Cisco from two locations in Lake Michigan.\n\n\nLMBassBL\nLengths for Largemouth Bass from Boomer Lake, OK.\n\n\nLMBassLCB\nLengths for Largemouth Bass from Lake Carl Blackwell, OK.\n\n\nLakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nLakeTroutEggs\nLength and egg deposition of Lake Superior Lake Trout.\n\n\nLakeTroutGIS\nStock and recruitment data for Lake Trout from Gull Island Shoal, Lake Superior, 1964-1991.\n\n\nLakeTroutMI\nStock and recruitment data for Lake Trout in Lake Superior, 1971-1991.\n\n\nLizardfish\nStock and recruitment data for Greater Lizardfish, 1955-1964.\n\n\nLobsterHI\nCatch and effort data for Hawaiian Islands Slipper Lobster.\n\n\nLobsterPEI\nCatch and effort data for Prince Edward Island Lobster.\n\n\nMenhaden1\nCatch-at-age for Gulf Menhaden, 1964-2004.\n\n\nMorwong1\nAges of Morwong assigned from otoliths by Reader A at two times.\n\n\nMorwong2\nAges of Morwong assigned from otoliths by Reader B at two times.\n\n\nMorwong3\nAges of Morwong assigned from otoliths by two readers.\n\n\nMorwong4\nAges and lengths of Morwong.\n\n\nMorwong4a\nAges (subsample) and lengths (all fish) for Morwong from Morwong4.\n\n\nMosquitofish\nAges and lengths of Eastern Mosquitofish from ten locations from southern France to southern Spain.\n\n\nMulletBS\nAges of Red Mullet assigned from whole and broken-burnt otoliths.\n\n\nMuskieSLR\nAges of Muskellunge assigned from scales and cleithra.\n\n\nMuskieWI06MR\nSummarized mark-recapture data for Muskellunge from many Wisconsin Lakes, 2006.\n\n\nPSalmonAK\nStock and recruitment data for Alaskan Pink Salmon, 1960-1990.\n\n\nPallid\nLengths and weights for Pallid Sturgeon from four locations in the Missouri River.\n\n\nPathfinder\nCatch and effort for three Snapper species in a depletion experiment.\n\n\nPikeHL\nCapture histories (2 samples) of Northern Pike from Harding Lake.\n\n\nPikeIL\nCatch and effort data for Northern Pike from Island Lake, NB.\n\n\nPikeNYPartial2\nCapture histories (4 samples) of a subset of Northern Pike from Buckthorn Marsh.\n\n\nPikeWindermere\nStock and recruitment data for Northern Pike from Lake Windermere, 1944-1981.\n\n\nPygmyWFBC\nBiological data for Pygmy Whitefish from Dina Lake #1 (British Columbia), 2000 and 2001.\n\n\nRBSmeltErie\nRecruitment time-series for Rainbow Smelt in Lake Erie, 1977-1996.\n\n\nRBSmeltLM\nLengths for Rainbow Smelt from Lake Michigan, 1977.\n\n\nRBTroutKenai\nLength-at-marking and recapture and time-at-large of Rainbow Trout.\n\n\nRBTroutUNSP\nCapture histories (2 sample) of Rainbow Trout.\n\n\nRWhitefishAI\nAges and lengths of Round Whitefish.\n\n\nRWhitefishIR\nAges and lengths of Round Whitefish.\n\n\nRedDrum\nAges and lengths for Red Drum from the Atlantic Coast.\n\n\nRiffleshell\nSummarized multiple mark-recapture data for Tan Riffleshell.\n\n\nRockBassCL\nCatch-at-age of Cayuga Lake Rock Bass.\n\n\nRockBassLO1\nAges and lengths of Lake Ontario Rock Bass.\n\n\nRockBassLO2\nAges (subsample) and lengths (all fish) for Rock Bass from Lake Ontario.\n\n\nRuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nRuffeTL89\nLengths of Ruffe captured from the St. Louis River in July, 1989.\n\n\nSLampreyGL\nStock and recruitment data for Sea Lamprey in the Great Lakes, 1997-2007.\n\n\nSalmonADP\nCatches in removal events of salmon parr.\n\n\nSalmonidsMCCA\nCatches in removal events of Cutthroate Trout and Steelhead of various sizes in two reaches of McGarvey Creek (CA).\n\n\nSardineChile\nAges and lengths of two year-classes of Sardine from Chilean waters.\n\n\nSardineLK\nAges and lengths of larval Lake Tanganyika Sardine.\n\n\nSardinesPacific\nStock and recruitment data for Pacific Sardines, 1935-1990.\n\n\nSculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nShadCR\nAges of American Shad assigned from scales by three readers at two times.\n\n\nShrimpGuam\nCatch and effort data for Deepwater Caridean Shrimp.\n\n\nSimonsonLyons\nCatches in removal events of trout at various locations.\n\n\nSiscowetMI2004\nAges (subsample) and lengths (all fish) for male and female Siscowet Lake Trout captured at four locations in Michigan waters of Lake Superior.\n\n\nSnapper\nLengths for Snapper from Australia.\n\n\nSnapperHG1\nAge (subsample) and length (all fish) of Snapper from two survey locations.\n\n\nSnapperHG2\nAges (subsample) and lengths (all fish) for Snapper.\n\n\nSockeyeKL\nStock and recruitment data for Sockeye Salmon from Karluk Lake, AK, 1921-1948.\n\n\nSockeyeSR\nStock and recruitment data for Skeena River Sockeye Salmon, 1940-1967.\n\n\nSpotVA2\nAges (subsample) and lengths (all fish) for Spot.\n\n\nSpottedSucker1\nAges and lengths of Spotted Sucker.\n\n\nStripedBass1\nAges of Striped Bass assigned from scales and otoliths.\n\n\nStripedBass2\nAges and lengths of Atlantic Ocean Striped Bass.\n\n\nStripedBass3\nAges (subsample) and lengths (all fish) for Striped Bass.\n\n\nStripedBass4\nAges of Striped Bass assigned from scales by two readers.\n\n\nStripedBass5\nAges of Striped Bass assigned from otoliths by two readers.\n\n\nStripedBass6\nAges of Striped Bass assigned from scales and otoliths.\n\n\nSturgeonBL\nSummarized multiple mark-recapture data for Lake Sturgeon.\n\n\nSturgeonGB\nCapture years and ages for Lake Sturgeon from Goulais Bay, Lake Superior, Ont.\n\n\nSunfishIN\nSummarized multiple mark-recapture data for Redear Sunfish.\n\n\nSunfishLP\nCatch-at-age for Bluegill and Redear Sunfish in Florida.\n\n\nTPrawnsEG\nStock and recruitment data for Exmouth Gulf Tiger Prawn, 1970-83.\n\n\nTroutADP\nCatches in removal events of trout.\n\n\nTroutBR\nAges and lengths of migratory Brown and Rainbow Trout.\n\n\nTroutperchLM1\nAges, lengths, and sexes of Troutperch.\n\n\nTroutperchLM2\nLengths for Troutperch from Lake Michigan, 1977.\n\n\nTroutperchLM3\nSubsampled lengths of Troutperch from Lake Michigan, 1977.\n\n\nVendaceLP\nStock and recruitment data for Vendace from Lake Puulavesi, 1982-1996.\n\n\nVendaceLP2\nStock and recruitment data for Vendace from Lake Pyhajarvi.\n\n\nWShrimpGA\nStock and recruitment data for White Shrimp off the coast of Georgia (USA), 1979-2000.\n\n\nWalleyeConsumption\nConsumption of prey by Walleye.\n\n\nWalleyeEL\nStock and recruitment data for Walleye from Escanaba Lake, WI, 1958-1992.\n\n\nWalleyeErie\nRecruitment time-series for Walleye in Lake Erie, 1959-1972.\n\n\nWalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nWalleyeKS\nCatch-at-age for Walleye from eight Kansas reservoirs.\n\n\nWalleyeML\nBack-calculated lengths-at-age for Walleye from Lake Mille Lacs, 2000-2011.\n\n\nWalleyeMN06a\nCatch-at-age for Walleye.\n\n\nWalleyeMN06b\nSummarized multiple mark-recapture data for Walleye from four lakes in Northern Minnesota.\n\n\nWalleyePL\nSummarized multiple mark-recapture data for YOY walleye.\n\n\nWalleyePS\nAges of Walleye assigned from otoliths, scales, and spines.\n\n\nWalleyeRL\nGrowth increment data for Red Lakes Walleye.\n\n\nWalleyeWad\nCatches-at-age for male and female Walleye from Lake Winnebago, WI, 2010.\n\n\nWalleyeWyrlng\nAnnual catches of yearling Walleye in bottom trawls from Lake Winnebago, WI, 1986-2010.\n\n\nWhiteGrunt1\nCatch-at-age for White Grunt.\n\n\nWhiteGrunt2\nAges, lengths, and sexes of White Grunt.\n\n\nWhitefishGSL\nCatch-at-age of Great Slave Lake Whitefish (commercial) by area.\n\n\nWhitefishLS\nLandings and value of Lake Superior Lake Whitefish.\n\n\nWhitefishMB\nAges of Lake Whitefish from four lakes assigned from scales and fin-rays.\n\n\nWhitefishTB\nStock and recruitment data for Lake Whitefish in Thunder Bay, Lake Superior, 1975-1988.\n\n\nYERockfish\nAges, lengths, and maturity for Yelloweye Rockfish.\n\n\nYPerchCB1\nCatch-at-age for Yellow Perch from Chequamegon Bay, Lake Superior.\n\n\nYPerchCB2\nStock and recruitment data for Yellow Perch in Chequamegon Bay, 1975-1986.\n\n\nYPerchGB\nRecruitment time-series for Yellow Perch in Green Bay, 1978-1992.\n\n\nYPerchGL\nLengths and weights of Yellow Perch from Grafton Lake (ME) by year.\n\n\nYPerchRL\nRecruitment time-series for Yellow Perch in Red Lakes, MN, 1942-1960.\n\n\nYPerchSB\nStock and recruitment data for Yellow Perch from South Bay, Lake Huron, 1950-1983.\n\n\nYPerchSB1\nLengths for Yellow Perch from two locations in Saginaw Bay, Lake Michigan.\n\n\nYPerchTL\nLengths and weights for Yellow Perch from Trout Lake, WI.\n\n\nYTFlounder\nAges of Yellowtail Flounder assigned from scales and otoliths.\n\n\n\n \n\n\nMQMF\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nLatA\nSimulated age data for redfish from eastern Australia in the 1990s\n\n\nabdat\nCatch data for the Tasmanian abalone fishery\n\n\nblackisland\nLength from tagging data for blacklip abalone at the Black Island site, Tasmania\n\n\ndataspm\nCatch data for Pink Ling\n\n\nminnow\nWeekly length measurements of a minnow for use with seasonal growth curves\n\n\nnpf\nCatch data (year, biomass, effort) for several species in the Australian Northern Prawn Fishery\n\n\npttuna\nYellowfin tuna fishery data (catch effort by year) from Pella-Tomlinson 1969\n\n\nschaef\nYellowfin tuna fishery data (catch effort by year) from Schaefer 1957\n\n\ntasab\nMaturity and length data for Blacklip Abalone from Tasmania\n\n\ntigers\nPrawn recruitment data from Penn and Caputi (1986)\n\n\n\n \n\n\nStat2Data\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nFishEggs\nPercent dry mass of eggs, age, and month for Lake Ontario Lake Trout\n\n\n\n \n\n\nTropFishR\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nalba\nLength-frequency data of the clam Abra alba as presented by Brey et al. (1988)\n\n\nbream\nData of a covered codend experimental catch of the species Threadfin bream (Nemipterus japonicus) in South China Sea\n\n\nemperor\nInformation about sky emperor (Lethrinus mahsena) and its fisheries of offshore Mauritius banks (Nazareth banks). It can be used for production models\n\n\ngillnet\nData of an experiment with several gillnets with different mesh sizes\n\n\ngoatfish\nData of Yellowstriped goatfish (Upeneus vittatus) from Manila Bay, Philippines. Can be used for the estimation of the instantaneous mortality rate (Z)\n\n\nhaddock\nData of a covered codend experimental catch of the haddock (Melanogrammus aeglefinus)\n\n\nhake\nLength-frequency data and biological characteristics about hake (Merluccius merluccius) and its fisheries off Senegal\n\n\ntrammelnet\nData of an experiment with several trammel nets with different mesh sizes\n\n\ntrawl_fishery_java\nTimes series of catch and effort data from the trawl fishery off the North coast of Java\n\n\nwhiting\nDataset of North Sea whiting Merlangius merlangus caught during the period 1974-1980"
  },
  {
    "objectID": "pages/data_fishR_alpha.html",
    "href": "pages/data_fishR_alpha.html",
    "title": "Datasets from FSA and FSAdata",
    "section": "",
    "text": "Dataset\nDescription\n\n\n\n\nAHerringChile\nAges and lengths of Araucanian Herring from Chilean waters.\n\n\nAfricanRivers\nCharacteristics of a sample of West African rivers.\n\n\nAlewifeLH\nAges of Lake Huron Alewife assigned from otoliths and scales.\n\n\nAnchovetaChile\nAges and lengths of Anchoveta from Chilean waters.\n\n\nBGHRfish\nFish information from samples collected from Big Hill Reservoir, KS, 2014.\n\n\nBGHRsample\nInformation for each electrofishing sample from Big Hill Reservoir, KS, 2014.\n\n\nBSkateGB\nStock and recruitment data for Barndoor Skate from Georges Bank, 1966-2007.\n\n\nBassFL\nCatch-at-age for Suwanee and Largemouth Bass.\n\n\nBlackDrum2001\nBiological data for Black Drum from Virginia waters of the Atlantic Ocean, 2001.\n\n\nBloaterLH\nStock and recruitment data for Lake Huron Bloaters, 1981-1996.\n\n\nBlueCatfish\nAges and lengths of Blue Catfish.\n\n\nBlueCrab\nCatch and effort data for male Blue Crabs.\n\n\nBluefishAge\nAges of Bluefish assigned from otoliths by two readers.\n\n\nBluegillIL\nLength-at-marking and recapture and time-at-large of Bluegill.\n\n\nBluegillJL\nCapture histories (2 samples) of Bluegill from Jewett Lake, MI.\n\n\nBluegillLM\nLengths and weights for Bluegill from Lake Mary, MN.\n\n\nBluntnoseIL1\nSubampled lengths of Bluntnose Minnows from Inch Lake, WI.\n\n\nBonito\nAges and lengths of Australian Bonito.\n\n\nBrookTroutNC\nStock and recruitment data for Brook Trout from Ball Creek, NC, 1991-2004.\n\n\nBrookTroutNEWP\nCatches in removal events for Brook Trout in the Nashwaak Experimental Watersheds Project.\n\n\nBrookTroutNEWP1\nCatches in removal events for Brook Trout in the Nashwaak Experimental Watersheds Project.\n\n\nBrookTroutOnt\nSummarized single mark-recapture data for Brook Trout across many years.\n\n\nBrookTroutTH\nCatch-at-age for Tobin Harbor, Isle Royale Brook Trout.\n\n\nBrownTroutVC1\nSingle census mark-recapture data with lengths for Brown Trout from Valley Creek, MN.\n\n\nBullTroutRML1\nLengths and weights for Bull Trout from two Rocky Mountain lakes and two eras.\n\n\nBullTroutRML2\nAges and lengths of Bull Trout from two Rocky Mountain lakes at two times.\n\n\nBullTroutTC\nCatch-at-age for Bull Trout in Trestle Creek, ID.\n\n\nCCatfishNB\nCatch-at-age of Channel Catfish from two sections of the Platte River, NB.\n\n\nCabezon\nAges, lengths, and maturity for female Cabezon from Oregon.\n\n\nCasselman1990\nInstantaneous growth rates for two calcified ageing structures.\n\n\nChinookArg\nLengths and weights for Chinook Salmon from three locations in Argentina.\n\n\nChinookKR\nStock and recruitment data for Klamath River Chinook Salmon, 1979-2000.\n\n\nCiscoTL\nLengths, weights, and sex of Cisco from Trout Lake, WI.\n\n\nCodNorwegian\nStock and recruitment data for Norwegian cod, 1937-1960.\n\n\nCrappieARMS\nStock and recruitment data for Crappies from four reservoirs in Arkansas and Mississippi, USA.\n\n\nCreekChub\nAges (subsample) and lengths (all fish) for Creek Chub.\n\n\nCreelMN\nResults of a large number of creel surveys in Minnestoa lakes.\n\n\nCroaker1\nAges of Atlantic Croaker assigned from otoliths by two readers.\n\n\nCroaker2\nAges, lengths, and sexes of Atlantic Croaker by sex.\n\n\nCutthroatAL\nCapture histories (9 samples) of Cutthroat Trout from Auke Lake.\n\n\nCutthroatALf\nCapture histories (9 samples) of Cutthroat Trout from Auke Lake.\n\n\nDarterMahon\nCatch and effort data for Fantail Darter.\n\n\nDarterOnt\nAges and lengths of Channel Darters from two locations.\n\n\nDeckeretal1999\nCatches in removal events of Cutthroat Trout and Coho Salmon in Little Stawamus Creek (British Columbia, Canada) in 1997.\n\n\nEcoli\nPopulation growth of Escherichia coli.\n\n\nEuroPerchTJ\nAges, lengths, and sexes of European Perch.\n\n\nFHCatfish\nCatch-at-age of Flathead Catfish from three southeastern rivers.\n\n\nFHCatfishATL\nCatch-at-age of Flathead Catfish from three Atlantic rivers.\n\n\nFSAdata\nData to support the FSA package.\n\n\nFWDrumLE1\nAges and lengths of Lake Erie Freshwater Drum.\n\n\nFWDrumLE2\nAges (subsample) and lengths (all fish) for Freshwater Drum from Lake Erie.\n\n\nGhats\nSpecies accumulation data for fish of the Western Ghats of India.\n\n\nGreensCreekMine\nCatches in removal events of Coho Salmon and Dolly Varden Char at various locations near the Greens Creek (AK) Mine site.\n\n\nHake\nStock and recruitment data for Hake, 1982-1996.\n\n\nHalibutPAC\nStock and recruitment data for Pacific Halibut, 1929-1991.\n\n\nHerman\nLengths for Walleye, Yellow Perch, Black Crappie, and Black Bullheads from Lake Herman, SD.\n\n\nHerringBWE\nStock and recruitment data for Blackwater Estuary Herring, 1962-1997.\n\n\nHerringISS\nStock and recruitment data for Icelandic summer spawning Herring, 1946-1996.\n\n\nHumpbackWFCR\nCapture histories (2 sample) of Humpback Whitefish.\n\n\nInchLake1\nLengths for all fish captured in Inch Lake, WI, in two years\n\n\nInchLake2\nLengths and weights for fish captured in Inch Lake\n\n\nJobfishSIO\nCatch and effort data for South Indian Ocean Jobfish.\n\n\nJonesStockwell\nCatches in removal events of Brown and Rainbow Trout at various locations.\n\n\nJonubi1\nAges and lengths of male Jonubi.\n\n\nJonubi2\nAges (subsample) and lengths (all fish) of Jonubi.\n\n\nKingCrabAK\nStock and recruitment data for Red King Crab in Alaska, 1960-2004.\n\n\nLJCisco\nAges and lengths of Longjaw Cisco from two locations in Lake Michigan.\n\n\nLMBassBL\nLengths for Largemouth Bass from Boomer Lake, OK.\n\n\nLMBassLCB\nLengths for Largemouth Bass from Lake Carl Blackwell, OK.\n\n\nLakeTroutALTER\nBiological data for Lake Trout from the Arctic LTER (AK).\n\n\nLakeTroutEggs\nLength and egg deposition of Lake Superior Lake Trout.\n\n\nLakeTroutGIS\nStock and recruitment data for Lake Trout from Gull Island Shoal, Lake Superior, 1964-1991.\n\n\nLakeTroutMI\nStock and recruitment data for Lake Trout in Lake Superior, 1971-1991.\n\n\nLizardfish\nStock and recruitment data for Greater Lizardfish, 1955-1964.\n\n\nLobsterHI\nCatch and effort data for Hawaiian Islands Slipper Lobster.\n\n\nLobsterPEI\nCatch and effort data for Prince Edward Island Lobster.\n\n\nMenhaden1\nCatch-at-age for Gulf Menhaden, 1964-2004.\n\n\nMirex\nMirex concentration, weight, capture year, and species of Lake Ontario salmon.\n\n\nMorwong1\nAges of Morwong assigned from otoliths by Reader A at two times.\n\n\nMorwong2\nAges of Morwong assigned from otoliths by Reader B at two times.\n\n\nMorwong3\nAges of Morwong assigned from otoliths by two readers.\n\n\nMorwong4\nAges and lengths of Morwong.\n\n\nMorwong4a\nAges (subsample) and lengths (all fish) for Morwong from Morwong4.\n\n\nMosquitofish\nAges and lengths of Eastern Mosquitofish from ten locations from southern France to southern Spain.\n\n\nMulletBS\nAges of Red Mullet assigned from whole and broken-burnt otoliths.\n\n\nMuskieSLR\nAges of Muskellunge assigned from scales and cleithra.\n\n\nMuskieWI06MR\nSummarized mark-recapture data for Muskellunge from many Wisconsin Lakes, 2006.\n\n\nPSDlit\nGabelhouse five-cell length categories for various species.\n\n\nPSalmonAK\nStock and recruitment data for Alaskan Pink Salmon, 1960-1990.\n\n\nPallid\nLengths and weights for Pallid Sturgeon from four locations in the Missouri River.\n\n\nPathfinder\nCatch and effort for three Snapper species in a depletion experiment.\n\n\nPikeHL\nCapture histories (2 samples) of Northern Pike from Harding Lake.\n\n\nPikeIL\nCatch and effort data for Northern Pike from Island Lake, NB.\n\n\nPikeNY\nSummarized multiple mark-recapture data for all Northern Pike from Buckhorn Marsh, NY.\n\n\nPikeNYPartial1\nCapture histories (4 samples), in capture history format, of a subset of Northern Pike from Buckhorn Marsh, NY.\n\n\nPikeNYPartial2\nCapture histories (4 samples) of a subset of Northern Pike from Buckthorn Marsh.\n\n\nPikeWindermere\nStock and recruitment data for Northern Pike from Lake Windermere, 1944-1981.\n\n\nPygmyWFBC\nBiological data for Pygmy Whitefish from Dina Lake #1 (British Columbia), 2000 and 2001.\n\n\nRBSmeltErie\nRecruitment time-series for Rainbow Smelt in Lake Erie, 1977-1996.\n\n\nRBSmeltLM\nLengths for Rainbow Smelt from Lake Michigan, 1977.\n\n\nRBTroutKenai\nLength-at-marking and recapture and time-at-large of Rainbow Trout.\n\n\nRBTroutUNSP\nCapture histories (2 sample) of Rainbow Trout.\n\n\nRWhitefishAI\nAges and lengths of Round Whitefish.\n\n\nRWhitefishIR\nAges and lengths of Round Whitefish.\n\n\nRedDrum\nAges and lengths for Red Drum from the Atlantic Coast.\n\n\nRiffleshell\nSummarized multiple mark-recapture data for Tan Riffleshell.\n\n\nRockBassCL\nCatch-at-age of Cayuga Lake Rock Bass.\n\n\nRockBassLO1\nAges and lengths of Lake Ontario Rock Bass.\n\n\nRockBassLO2\nAges (subsample) and lengths (all fish) for Rock Bass from Lake Ontario.\n\n\nRuffeSLRH92\nBiological data for Ruffe captured from the St. Louis River in 1992.\n\n\nRuffeTL89\nLengths of Ruffe captured from the St. Louis River in July, 1989.\n\n\nSLampreyGL\nStock and recruitment data for Sea Lamprey in the Great Lakes, 1997-2007.\n\n\nSMBassLS\nCatch-effort data for Little Silver Lake (Ont) Smallmouth Bass.\n\n\nSMBassWB\nGrowth increment data for West Bearskin Lake, MN, Smallmouth Bass.\n\n\nSalmonADP\nCatches in removal events of salmon parr.\n\n\nSalmonidsMCCA\nCatches in removal events of Cutthroate Trout and Steelhead of various sizes in two reaches of McGarvey Creek (CA).\n\n\nSardineChile\nAges and lengths of two year-classes of Sardine from Chilean waters.\n\n\nSardineLK\nAges and lengths of larval Lake Tanganyika Sardine.\n\n\nSardinesPacific\nStock and recruitment data for Pacific Sardines, 1935-1990.\n\n\nSculpinALTER\nBiological data for Slimy Sculpin from the Arctic LTER (AK).\n\n\nShadCR\nAges of American Shad assigned from scales by three readers at two times.\n\n\nShrimpGuam\nCatch and effort data for Deepwater Caridean Shrimp.\n\n\nSimonsonLyons\nCatches in removal events of trout at various locations.\n\n\nSiscowetMI2004\nAges (subsample) and lengths (all fish) for male and female Siscowet Lake Trout captured at four locations in Michigan waters of Lake Superior.\n\n\nSnapper\nLengths for Snapper from Australia.\n\n\nSnapperHG1\nAge (subsample) and length (all fish) of Snapper from two survey locations.\n\n\nSnapperHG2\nAges (subsample) and lengths (all fish) for Snapper.\n\n\nSockeyeKL\nStock and recruitment data for Sockeye Salmon from Karluk Lake, AK, 1921-1948.\n\n\nSockeyeSR\nStock and recruitment data for Skeena River Sockeye Salmon, 1940-1967.\n\n\nSpotVA1\nAge and length of spot.\n\n\nSpotVA2\nAges (subsample) and lengths (all fish) for Spot.\n\n\nSpottedSucker1\nAges and lengths of Spotted Sucker.\n\n\nStripedBass1\nAges of Striped Bass assigned from scales and otoliths.\n\n\nStripedBass2\nAges and lengths of Atlantic Ocean Striped Bass.\n\n\nStripedBass3\nAges (subsample) and lengths (all fish) for Striped Bass.\n\n\nStripedBass4\nAges of Striped Bass assigned from scales by two readers.\n\n\nStripedBass5\nAges of Striped Bass assigned from otoliths by two readers.\n\n\nStripedBass6\nAges of Striped Bass assigned from scales and otoliths.\n\n\nSturgeonBL\nSummarized multiple mark-recapture data for Lake Sturgeon.\n\n\nSturgeonGB\nCapture years and ages for Lake Sturgeon from Goulais Bay, Lake Superior, Ont.\n\n\nSunfishIN\nSummarized multiple mark-recapture data for Redear Sunfish.\n\n\nSunfishLP\nCatch-at-age for Bluegill and Redear Sunfish in Florida.\n\n\nTPrawnsEG\nStock and recruitment data for Exmouth Gulf Tiger Prawn, 1970-83.\n\n\nTroutADP\nCatches in removal events of trout.\n\n\nTroutBR\nAges and lengths of migratory Brown and Rainbow Trout.\n\n\nTroutperchLM1\nAges, lengths, and sexes of Troutperch.\n\n\nTroutperchLM2\nLengths for Troutperch from Lake Michigan, 1977.\n\n\nTroutperchLM3\nSubsampled lengths of Troutperch from Lake Michigan, 1977.\n\n\nVendaceLP\nStock and recruitment data for Vendace from Lake Puulavesi, 1982-1996.\n\n\nVendaceLP2\nStock and recruitment data for Vendace from Lake Pyhajarvi.\n\n\nWR79\nAges and lengths for a hypothetical sample from Westerheim and Ricker (1979).\n\n\nWShrimpGA\nStock and recruitment data for White Shrimp off the coast of Georgia (USA), 1979-2000.\n\n\nWSlit\nAll known standard weight equations.\n\n\nWalleyeConsumption\nConsumption of prey by Walleye.\n\n\nWalleyeEL\nStock and recruitment data for Walleye from Escanaba Lake, WI, 1958-1992.\n\n\nWalleyeErie\nRecruitment time-series for Walleye in Lake Erie, 1959-1972.\n\n\nWalleyeErie2\nBiological data for Walleye from Lake Erie, 2003-2014.\n\n\nWalleyeKS\nCatch-at-age for Walleye from eight Kansas reservoirs.\n\n\nWalleyeML\nBack-calculated lengths-at-age for Walleye from Lake Mille Lacs, 2000-2011.\n\n\nWalleyeMN06a\nCatch-at-age for Walleye.\n\n\nWalleyeMN06b\nSummarized multiple mark-recapture data for Walleye from four lakes in Northern Minnesota.\n\n\nWalleyePL\nSummarized multiple mark-recapture data for YOY walleye.\n\n\nWalleyePS\nAges of Walleye assigned from otoliths, scales, and spines.\n\n\nWalleyeRL\nGrowth increment data for Red Lakes Walleye.\n\n\nWalleyeWad\nCatches-at-age for male and female Walleye from Lake Winnebago, WI, 2010.\n\n\nWalleyeWyrlng\nAnnual catches of yearling Walleye in bottom trawls from Lake Winnebago, WI, 1986-2010.\n\n\nWhiteGrunt1\nCatch-at-age for White Grunt.\n\n\nWhiteGrunt2\nAges, lengths, and sexes of White Grunt.\n\n\nWhitefishGSL\nCatch-at-age of Great Slave Lake Whitefish (commercial) by area.\n\n\nWhitefishLC\nAssigned ages from two readers on three structures for Lake Whitefish from Lake Champlain.\n\n\nWhitefishLS\nLandings and value of Lake Superior Lake Whitefish.\n\n\nWhitefishMB\nAges of Lake Whitefish from four lakes assigned from scales and fin-rays.\n\n\nWhitefishTB\nStock and recruitment data for Lake Whitefish in Thunder Bay, Lake Superior, 1975-1988.\n\n\nYERockfish\nAges, lengths, and maturity for Yelloweye Rockfish.\n\n\nYPerchCB1\nCatch-at-age for Yellow Perch from Chequamegon Bay, Lake Superior.\n\n\nYPerchCB2\nStock and recruitment data for Yellow Perch in Chequamegon Bay, 1975-1986.\n\n\nYPerchGB\nRecruitment time-series for Yellow Perch in Green Bay, 1978-1992.\n\n\nYPerchGL\nLengths and weights of Yellow Perch from Grafton Lake (ME) by year.\n\n\nYPerchRL\nRecruitment time-series for Yellow Perch in Red Lakes, MN, 1942-1960.\n\n\nYPerchSB\nStock and recruitment data for Yellow Perch from South Bay, Lake Huron, 1950-1983.\n\n\nYPerchSB1\nLengths for Yellow Perch from two locations in Saginaw Bay, Lake Michigan.\n\n\nYPerchTL\nLengths and weights for Yellow Perch from Trout Lake, WI.\n\n\nYTFlounder\nAges of Yellowtail Flounder assigned from scales and otoliths."
  },
  {
    "objectID": "pages/packages.html",
    "href": "pages/packages.html",
    "title": "Fisheries-Related R Packages",
    "section": "",
    "text": "You may submit additions or corrections to the list of packages by using the links at the bottom of the sidebar menu to the right. Before creating an issue or pull request …\n\nassure that your package/resource is not already in the list,\nyou include a short name, a link, and a short description for the package/resource,\n[if editing the page] your package/resource is placed alphabetically within the relevant topic section, and\nthe information you supply is complete and correct.\n\nThank you for your contribution to this resource."
  },
  {
    "objectID": "pages/packages.html#general-or-traditional",
    "href": "pages/packages.html#general-or-traditional",
    "title": "Fisheries-Related R Packages",
    "section": "General or Traditional",
    "text": "General or Traditional\n\nmarked\nmra\nmrds\nRcapture\nRMark\nunmarked"
  },
  {
    "objectID": "pages/packages.html#spatially-explicit",
    "href": "pages/packages.html#spatially-explicit",
    "title": "Fisheries-Related R Packages",
    "section": "Spatially Explicit",
    "text": "Spatially Explicit\n\nascr\nsecr\nsecrdesign\nsecrlinear"
  },
  {
    "objectID": "pages/packages.html#other",
    "href": "pages/packages.html#other",
    "title": "Fisheries-Related R Packages",
    "section": "Other",
    "text": "Other\n\ndga\nSCRbayes"
  },
  {
    "objectID": "teaching/posts/2016-6-12_Halflife_of_K/index.html",
    "href": "teaching/posts/2016-6-12_Halflife_of_K/index.html",
    "title": "Half-Life Property of K",
    "section": "",
    "text": "Note\n\n\n\nThe following packages are loaded for use below. One function from FSA is also used below, but the full package was not loaded here. In addition, the plot was created with ggplot2, though the code is not shown in this post.\n\n\n\nlibrary(dplyr)  # for mutate()\n\n \n\nIntroduction\nA colleague recently questioned whether \\(\\frac{log_{e}(2)}{K}\\), where \\(K\\) is the Brody growth coefficient in the typical parameterization of the von Bertalanffy growth function, represents the “time it takes for a fish to grow from any length to a length halfway between the initial length and the asymptotic mean length (\\(L_{\\infty}\\))”. This phenomenon is briefly illustrated below.\n \n\n\nOne Age\n\nCreate an R function for the typical von Bertalanffy growth function.\n\n\nvb &lt;- FSA::vbFuns()\n\n\nDeclare parameter values.\n\n\nLinf &lt;- 30\nK &lt;- 0.3\nt0 &lt;- -0.5\n\n\nPredict mean length at some initial age.\n\n\ninitA &lt;- 1\n( initL &lt;- vb(initA,Linf,K,t0) )\n\n#R|  [1] 10.87116\n\n\n\nPredict mean length at the initial age plus \\(\\frac{log_{e}(2)}{K}\\).\n\n\nnextA &lt;- initA+log(2)/K\n( nextL &lt;- vb(nextA,Linf,K,t0) )\n\n#R|  [1] 20.43558\n\n\n\nFind the length that is halfway between the initial length and \\(L_{\\infty}\\).1\n\n1 all.equal() is used to test equality with a tolerance for machine precision.\n( hwL &lt;- mean(c(initL,Linf)) )\n\n#R|  [1] 20.43558\n\nall.equal(nextL,hwL)\n\n#R|  [1] TRUE\n\n\nNote that these last two values are equal, which illustrates the statement above about the “half-life” meaning of \\(K\\).\n \nThis can be repeated for a different initial age.\n\ninitA &lt;- 7\n( initL &lt;- vb(initA,Linf,K,t0) )\n\n#R|  [1] 26.83802\n\nnextA &lt;- initA+log(2)/K\n( nextL &lt;- vb(nextA,Linf,K,t0) )\n\n#R|  [1] 28.41901\n\n( hwL &lt;- mean(c(initL,Linf)) )\n\n#R|  [1] 28.41901\n\nall.equal(nextL,hwL)\n\n#R|  [1] TRUE\n\n\n \nThe two examples above can be examined in Figure 1, where the horizontal green lines illustrate the increase from the initial ages (e.g., 1 and 7) by \\(\\frac{log_{e}(2)}{K}\\), the vertical red line is the change in length from the initial length to half of \\(L_{\\infty}\\), and the vertical orange line is the remaining change in length to \\(L_{\\infty}\\). The half-life property of \\(K\\) is thus illustrated by the equivalent lengths of the paired red and orange lines.\n\n\n\n\n\n\n\n\nFigure 1: Demonstration of the half-life property of the K parameter in the von Bertalanffy growth function. (see text for description).\n\n\n\n\n\n \n\n\nMultiple Ages\nThis process is repeated below for several initial age values. Note that the differences between the predicted mean length at the new age and the point halfway between the initial length and \\(L_{\\infty}\\) are equal (within machine precision) for each initial age. Again, illustrating the statement about \\(K\\).\n\ndata.frame(initA=1:20) |&gt;\n  mutate(initL=vb(initA,Linf,K,t0),\n         nextA=initA+log(2)/K,\n         nextL=vb(nextA,Linf,K,t0),\n         hwL=(initL+Linf)/2,\n         areEqual=all.equal(nextL,hwL))\n\n#R|     initA    initL     nextA    nextL      hwL areEqual\n#R|  1      1 10.87116  3.310491 20.43558 20.43558     TRUE\n#R|  2      2 15.82900  4.310491 22.91450 22.91450     TRUE\n#R|  3      3 19.50187  5.310491 24.75093 24.75093     TRUE\n#R|  4      4 22.22279  6.310491 26.11140 26.11140     TRUE\n#R|  5      5 24.23850  7.310491 27.11925 27.11925     TRUE\n#R|  6      6 25.73178  8.310491 27.86589 27.86589     TRUE\n#R|  7      7 26.83802  9.310491 28.41901 28.41901     TRUE\n#R|  8      8 27.65755 10.310491 28.82878 28.82878     TRUE\n#R|  9      9 28.26467 11.310491 29.13234 29.13234     TRUE\n#R|  10    10 28.71444 12.310491 29.35722 29.35722     TRUE\n#R|  11    11 29.04763 13.310491 29.52382 29.52382     TRUE\n#R|  12    12 29.29447 14.310491 29.64723 29.64723     TRUE\n#R|  13    13 29.47733 15.310491 29.73866 29.73866     TRUE\n#R|  14    14 29.61280 16.310491 29.80640 29.80640     TRUE\n#R|  15    15 29.71315 17.310491 29.85658 29.85658     TRUE\n#R|  16    16 29.78750 18.310491 29.89375 29.89375     TRUE\n#R|  17    17 29.84257 19.310491 29.92129 29.92129     TRUE\n#R|  18    18 29.88338 20.310491 29.94169 29.94169     TRUE\n#R|  19    19 29.91360 21.310491 29.95680 29.95680     TRUE\n#R|  20    20 29.93600 22.310491 29.96800 29.96800     TRUE\n\n\n \n\n\nDifferent Parameters\nThe code below illustrates the same phenomenon for a very different set of parameter values.\n\nLinf &lt;- 300\nK &lt;- 0.9\nt0 &lt;- 1\ndata.frame(initA=1:20) |&gt;\n  mutate(initL=vb(initA,Linf,K,t0),\n         nextA=initA+log(2)/K,\n         nextL=vb(nextA,Linf,K,t0),\n         hwL=(initL+Linf)/2,\n         areEqual=all.equal(nextL,hwL))\n\n#R|     initA    initL     nextA    nextL      hwL areEqual\n#R|  1      1   0.0000  1.770164 150.0000 150.0000     TRUE\n#R|  2      2 178.0291  2.770164 239.0146 239.0146     TRUE\n#R|  3      3 250.4103  3.770164 275.2052 275.2052     TRUE\n#R|  4      4 279.8383  4.770164 289.9192 289.9192     TRUE\n#R|  5      5 291.8029  5.770164 295.9014 295.9014     TRUE\n#R|  6      6 296.6673  6.770164 298.3337 298.3337     TRUE\n#R|  7      7 298.6450  7.770164 299.3225 299.3225     TRUE\n#R|  8      8 299.4491  8.770164 299.7246 299.7246     TRUE\n#R|  9      9 299.7760  9.770164 299.8880 299.8880     TRUE\n#R|  10    10 299.9089 10.770164 299.9545 299.9545     TRUE\n#R|  11    11 299.9630 11.770164 299.9815 299.9815     TRUE\n#R|  12    12 299.9849 12.770164 299.9925 299.9925     TRUE\n#R|  13    13 299.9939 13.770164 299.9969 299.9969     TRUE\n#R|  14    14 299.9975 14.770164 299.9988 299.9988     TRUE\n#R|  15    15 299.9990 15.770164 299.9995 299.9995     TRUE\n#R|  16    16 299.9996 16.770164 299.9998 299.9998     TRUE\n#R|  17    17 299.9998 17.770164 299.9999 299.9999     TRUE\n#R|  18    18 299.9999 18.770164 300.0000 300.0000     TRUE\n#R|  19    19 300.0000 19.770164 300.0000 300.0000     TRUE\n#R|  20    20 300.0000 20.770164 300.0000 300.0000     TRUE\n\n\n \n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_AgeComparisons_LCWhitefish/index.html",
    "href": "teaching/posts/2019-3-8_AgeComparisons_LCWhitefish/index.html",
    "title": "Age Comparisons",
    "section": "",
    "text": "Herbst and Marsden (2011) compared the precision, bias, and reader uncertainty of ages estimated from the scales, dorsal fin rays, and otoliths of 151 Lake Whitefish (Coregonus clupeaformis) captured from Lake Champlain in 2009. The initial age estimates from two readers and a consensus age between the two readers from each of the three structures (variables are the structure name with a “1” or “2” appended to denote the reader or “C” for the consensus age) are in WhitefishLC.1"
  },
  {
    "objectID": "teaching/posts/2019-3-8_AgeComparisons_LCWhitefish/index.html#version-a---otoliths",
    "href": "teaching/posts/2019-3-8_AgeComparisons_LCWhitefish/index.html#version-a---otoliths",
    "title": "Age Comparisons",
    "section": "Version A - Otoliths",
    "text": "Version A - Otoliths\n\nUse tabular, graphical, and statistical methods to describe any apparent bias in age estimates from otoliths between the two readers.\nDescribe precision in age estimates between the two readers for otoliths.\nDescribe any apparent bias between the consensus age estimates from otoliths and scales."
  },
  {
    "objectID": "teaching/posts/2019-3-8_AgeComparisons_LCWhitefish/index.html#version-b---scales",
    "href": "teaching/posts/2019-3-8_AgeComparisons_LCWhitefish/index.html#version-b---scales",
    "title": "Age Comparisons",
    "section": "Version B - Scales",
    "text": "Version B - Scales\n\nUse tabular, graphical, and statistical methods to describe any apparent bias in age estimates from scales between the two readers.\nDescribe precision in age estimates between the two readers for scales.\nDescribe any apparent bias between the consensus age estimates from scales and fin rays."
  },
  {
    "objectID": "teaching/posts/2019-3-8_AgeComparisons_LCWhitefish/index.html#version-c---fin-rays",
    "href": "teaching/posts/2019-3-8_AgeComparisons_LCWhitefish/index.html#version-c---fin-rays",
    "title": "Age Comparisons",
    "section": "Version C - Fin Rays",
    "text": "Version C - Fin Rays\n\nUse tabular, graphical, and statistical methods to describe any apparent bias in age estimates from fin rays between the two readers.\nDescribe precision in age estimates between the two readers for fin rays.\nDescribe any apparent bias between the consensus age estimates from fin rays and otoliths.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers."
  },
  {
    "objectID": "teaching/posts/2019-3-8_ALK_RockBassLO/index.html",
    "href": "teaching/posts/2019-3-8_ALK_RockBassLO/index.html",
    "title": "Age-Length Key",
    "section": "",
    "text": "Background\nWolfert (1980) measured the total length (TL) of 1288 Rock Bass (Ambloplites rupestris) from Eastern Lake Ontario in the late 1970s. In addition, scales were removed for age estimation from as many as 10 specimens from each 10 mm length interval. All data are recorded in RockBassLO2.1\n1 See “CSV file” link in “Source” section of linked page. Also note that the filename contains an “oh” not a “zero.” \n\n\nConstruct an ALK\n\nAdd a variable to the data frame that contains the 10 mm TL categories and then separate the observed data into age- and length-samples. How many fish are in each sample?\nConstruct a table of the number (not proportion) of fish in each age and 10 mm TL category in the age-sample. From these results, compute each of the following by hand (i.e., not using R, but you can use a calculator).\n\nHow many Rock Bass are in the 180 mm TL category?\nHow many Rock Bass are age 7?\nWhat proportion of Rock Bass in the 140 mm TL category are age 4?\nWhat proportion of Rock Bass in the 200 mm TL category are age 8?\n\nConstruct an observed age-length key from the table above (using R). From these results answer the following questions.\n\nWhat proportion of Rock Bass in the 210 mm TL category should be assigned age 5?\nHow many of thirty Rock Bass in the 180 mm TL category should be assigned age 5?\nConstruct a plot of the observed age-length key. Are there any potential anomalies in the plot that would suggest that a smoothed age-length key could be appropriate?\n\nConstruct a smoothed age-length key. From these results answer the following questions.\n\nWhat proportion of Rock Bass in the 210 mm length category should be assigned age 5?\nHow many of thirty Rock Bass in the 180 mm length category should be assigned age 5?\n\n\n \n\n\nApply an ALK I\nContinue with the age- and length-sample data frames and the observed age-length key from the previous section.\n\nUse the semi-random age assignment technique from Isermann and Knight (2005) and the observed age-length key to assign ages to the unaged fish in the length-sample. Combine the age-sample and the age-assigned length-sample into a single data frame to answer the following questions.\n\nHow many fish are estimated to be age 5?\nHow many fish are estimated to be age 11?\nPlot the age distribution for all fish.\nHow many fish are in the 150 mm TL interval?\nWhat is the mean TL of age-5 fish?\nPlot the length-at-age with the mean length-at-age superimposed for all fish.\n\nCompare your results from the previous question to someone else’s results (or repeat your code and answers to the previous question). Did you both get the exact same results? Why or why not? If not, how different were they?\n\n\n\n\n\n\n\nSave Your Script\n\n\n\nThe data frame with ages assigned to all fish will be used in this growth exercise.\n\n\n \n\n\nApply an ALK II\nContinue with the age- and length-sample data frames and the observed age-length key from the first section.\n\nUse the “classical” method to estimate the age distribution (with standard errors) for all sampled fish.\n\nHow many fish are estimated to be age 5?\nHow many fish are estimated to be age 11?\nPlot the age distribution for all fish.\n\nUse the “classical” method to estimate the mean length-at-age (with standard deviations) for all sampled fish.\n\nWhat is the mean TL of age-5 fish?\nPlot the mean length-at-age.\n\nCompare your results to someone else’s results (or repeat the steps above). Did you both get the exact same results? Why or why not? If not, how different were they?\nCompare your results using the “classical” method here to your results from using the Isermann and Knight (2005) method in the previous section.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nIsermann, D., and C. Knight. 2005. A computer program for age–length keys incorporating age assignment to individual fish. North American Journal of Fisheries Management 25:1153–1160.\n\n\nWolfert, D. R. 1980. Age and growth of Rock Bass in eastern Lake Ontario. New York Fish and Game Journal 27:88–90.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Condition_BGInch/index.html",
    "href": "teaching/posts/2019-3-8_Condition_BGInch/index.html",
    "title": "Condition",
    "section": "",
    "text": "Continuation\n\n\n\nThis exercise is a continuation of this data wrangling exercise and, thus, depends on data frames constructed there. Please load/run your script from that exercise to access the Bluegill only data frames.\n\n\n \n\nBasic Analysis I\nFurther prepare the Bluegill only data frame for computation of relative weights by adding a relative weight variable and reducing the data frame to only fish with lengths for which a standard weight should be computed.\n\nFor Bluegills captured only in 2007 …\n\nCompute the number of observations and mean, standard deviation, and standard error of relative weight for each Gabelhouse length category.\nDetermine if the mean relative weight differs among Gabelhouse length categories.\nConstruct a plot, with confidence intervals and appropriate significance letters, that depict your results.\n\n\n \n\n\nBasic Analysis II\n\nRepeat the previous question for Bluegills captured only in 2008.\nEmpirically compare your results between years. Provide at least three observations from your findings and at least two plausible explanations for what you observed.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Graphing_Inch/index.html",
    "href": "teaching/posts/2019-3-8_Graphing_Inch/index.html",
    "title": "Graphing",
    "section": "",
    "text": "Continuation\n\n\n\nThis exercise is a continuation of this data wrangling exercise and, thus, depends on data frames constructed there. Please load/run your script from that exercise to access the Bluegill, Largemouth Bass, and non-game species only data frames.\n\n\n \n\nDescribe the following from plots constructed with the Bluegill only data.frame.\n\nThe distribution of lengths.\nThe distribution of weights.\nThe distribution of lengths separately for each year (without creating separate data frames for each year).\nThe relationship between weight and length.\nThe relationship between the natural logs of weight and length.\nThe relationship between the natural logs of weight and length (without creating separate data frames for each year).\nThe difference in mean lengths between the two years (plot should include confidence intervals).\n\nRepeat the previous question but using Largemouth Bass.\nDescribe the species distribution of (only) non-game species.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Growth_EuroPerch/index.html",
    "href": "teaching/posts/2019-3-8_Growth_EuroPerch/index.html",
    "title": "Individual Growth",
    "section": "",
    "text": "Mooij et al. (1999) examined length-at-age data of European Perch (Perca fluviatilis) from Lake Tjeukemeer (The Netherlands) to identify possible sexual dimorphism in growth trajectories. Their data consisted of fork length (FL; cm), ages (yrs) from otoliths, and sex from 69 fish and are recorded in EuroPerchTJ.1\n1 See “CSV file” link in “Source” section of linked page. \n\nPlot FL versus age with different symbols for each sex.\n\nDo you foresee any model fitting problems with these data?\nDo you observe any possible differences in growth between the sexes?\n\nFit the typical VBGF with additive errors separately to both sexes.\n\nDescribe any problems that you encountered in the model fitting.\nCompute point and bootstrapped 95% confidence interval estimates for each parameter in the separate models.\nDo you see any issues with the confidence intervals? If so, describe.\n\nFit the typical VBGF with additive errors where all parameters differ by sex.\n\nDescribe any problems that you encountered.\nAssess the assumptions from this model fit.\nCompute point estimates for each parameter in this model.\nHow do the point estimates from this model compare to the point estimates from the separate models fit in #2 above?\n\nFind the most parsimonious model that is a subset of the model fit above.\n\nUse the extra sums-of-squares test.\nUsing the likelihood ratio test.\nUsing the \\(AICc\\) criterion.\nSummarize (in words) the results of the most parsimonious model identified with the extra sums-of-squares test.\n\nConstruct a summary graphic that shows the growth trajectories superimposed on the observed data for both sexes.2\n\n2 See this post. \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nMooij, W., J. van Rooij, and S. Wijnhoven. 1999. Analysis and comparison of fish growth from small samples of length-at-age data: Detection of sexual dimorphism in Eurasian Perch as an example. Transactions of the American Fisheries Society 128:483–490.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Growth_Sculpin/index.html",
    "href": "teaching/posts/2019-3-8_Growth_Sculpin/index.html",
    "title": "Individual Growth",
    "section": "",
    "text": "Background\nThe total length (mm) and otolith age of Slimy Sculpin (Cottus cognatus) captured in the Arctic Long-Term Ecological Research area were recorded in SculpinALTER.1\n1 See “CSV file” link in “Source” section of linked page. \n\n\nFit Traditional VBGF\n\nExamine the plot of TL versus age. Make observations regarding the “shape” of the data (do the results look linear or like a von Bertalanffy growth curve, is there an obvious asymptote, are young fish well represented, how variable are lengths within ages).\nFit the typical parameterization of the von Bertalanffy growth function (VBGF).\n\nHow realistic do the point estimates of \\(L_{\\infty}\\), \\(K\\), and \\(t_{0}\\) seem?\nWrite the typical VBGF with parameters replaced by their estimated values.\nCarefully interpret the meaning of each parameter.\nConstruct 95% bootstrapped confidence intervals for each parameter. Comment on the widths of these confidence intervals. What explains this?\nPredict the mean TL, with 95% confidence interval, for an age-3 fish. Comment on the width of this confidence interval. What explains this?\nPlot TL versus age and superimpose the best-fit VBGF.2 Comment on model fit.\nConstruct a residual plot. Comment on model fit.\nCompute the correlation between parameter values. Comment.\n\n\n2 This post may be useful. \n\n\nAlternative Parameterization\n\nFit the von Bertalanffy’s original parameterization.3\n\nInterpret the interval estimate for the \\(L_{0}\\) parameter.\nWrite the VBGF with parameters replaced by their estimated values.\nConstruct 95% bootstrapped confidence intervals for each parameter. Comment on the widths of these confidence intervals. What explains this?\nPredict the mean TL, with 95% confidence interval, for an age-d fish. Comment on the width of this confidence interval. What explains this?\nPlot TL versus age and superimpose the best-fit VBGF. Comment on model fit.\nCompute the correlation between parameter values. Comment\nHow does the estimate of \\(L_{\\infty}\\) and \\(K\\) from fitting this parameterization compare to that from the typical VBGF fit above. Explain your observation.\n\n\n3 See growthFunShow(\"vonBertalanffy\",param=\"original\",plot=TRUE)) and this. \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Mortality_LSSRLakeTrout/index.html",
    "href": "teaching/posts/2019-3-8_Mortality_LSSRLakeTrout/index.html",
    "title": "Mortality",
    "section": "",
    "text": "Curtis (1990) examined the population dynamics related to the recovery of an offshore Lake Trout population near Stannard Rock, Lake Superior. As part of this study, mortality rates were estimated from the relative abundance of Lake Trout longer than 43.2 cm. Relative abundance was recorded as the catch-per-unit-effort (CPE) of each age group in each year expressed as the number of fish caught per 50,000 m of 114.3 mm mesh gillnet. The results are shown in the table below. [Note: (1) the values in the table have been rounded to integers; (2) values recorded as “tr” in the original paper were recorded as “0.5” in this table; and (3) the years of capture are not contiguous (there is a break between 1959 and 1963 and again between 1969 and 1973).]\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\n      VI\n     VII\n    VIII\n      IX\n       X\n      XI\n     XII\n    XIII\n     XIV\n\n\n\n\n1959\n64\n219\n241\n121\n33\n9\n1\n0.5\n1\n\n\n….\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n1963\n129\n339\n331\n192\n70\n16\n0.5\n0.5\n0.5\n\n\n1964\n149\n524\n515\n201\n63\n18\n2\n0.5\n0.5\n\n\n1965\n75\n379\n501\n328\n133\n39\n11\n1\n0.5\n\n\n1966\n149\n488\n459\n172\n64\n22\n5\n0.5\n0.5\n\n\n1967\n63\n368\n287\n130\n55\n19\n6\n0.5\n0.5\n\n\n1968\n50\n215\n259\n141\n55\n18\n5\n1\n0.5\n\n\n1969\n45\n150\n153\n76\n23\n6\n0.5\n0.5\n0.5\n\n\n….\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n1973\n101\n759\n1268\n1116\n491\n141\n40\n4\n0.5\n\n\n1974\n151\n733\n1114\n1092\n571\n163\n50\n9\n5\n\n\n1975\n109\n901\n1517\n1606\n1076\n342\n117\n12\n7\n\n\n1976\n53\n604\n1204\n1560\n1146\n396\n156\n18\n10\n\n\n1977\n157\n867\n1343\n1410\n1031\n417\n192\n17\n7\n\n\n1978\n89\n735\n1307\n1623\n1150\n445\n198\n18\n14\n\n\n1979\n29\n299\n718\n1268\n1195\n585\n300\n36\n14\n\n\n\n \nUse these results to answer the following questions.\n\nWhat year-class of fish is represented by the 339 age-VII fish caught in 1963?\nShow the data frame of catches and corresponding ages for the following groups of fish.\n\nFor fish captured in 1963.\nFor fish of the 1963 year-class.\nThe earliest year-class that is fully represented for ages IX through XII.\nThe latest year-class that is fully represented for ages IX through XII.\n\nFor each data frame created above, identify whether the data represent a cross-sectional or longitudinal catch curve.\nFor the third data frame created above (earliest year class).\n\nEstimate, with 95% confidence interval, the instantaneous total mortality rate using the catch-curve regression method for ages IX through XII.\nEstimate, with 95% confidence interval, the annual total mortality rate using the catch-curve regression method for ages IX through XII. Carefully interpret this result.\n\nRepeat the previous question for the last data frame created above (latest year class).\nDetermine if the instantaneous mortality rate is significantly different between the earliest and latest year-classes that are fully represented for ages IX through XII.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\nReferences\n\nCurtis, G. L. 1990. Recovery of an offshore Lake Trout (Salvelinus Namaycush) population in eastern Lake Superior. Journal of Great Lakes Research 16(2):279–287.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_MR_UNSPRainbowTrout/index.html",
    "href": "teaching/posts/2019-3-8_MR_UNSPRainbowTrout/index.html",
    "title": "Abundance from Mark-Recapture Data",
    "section": "",
    "text": "Warren et al. (2004) examined the population of Rainbow Trout (Oncorhynchus mykiss) in the Upper Niagara Springs Pond in 2000. Fish were captured at two times by using an electrofishing unit attached to a driftboat. The capture history of all fish examined in the two samples that were 100 mm and longer is in RBTroutUNSP.1 Use these data to answer the following questions.\n1 See “CSV file” link in “Source” section of linked page.\nCreate a summary of the capture histories.\nFrom your capture history summary assign values to each of \\(M\\), \\(n\\), and \\(m\\).2\nConstruct an appropriate population estimate, with a 95% confidence interval, for Upper Niagara Springs Pond Rainbow Trout in 2000. Carefully interpret the results.\nWhich method did you use to construct the confidence interval? Explain why you chose that method.\n\n2 Symbols are as used in Ogle (2016).\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nOgle, D. H. 2016. Introductory Fisheries Analyses with R. CRC Press, Boca Raton, FL.\n\n\nWarren, C. D., K. A. Frank, and F. E. Partridge. 2004. Regional fisheries management investigations - Magic Valley region. Completion Report, Idaho Department of Fish and Game.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_MR_WIYOYWalleye/index.html",
    "href": "teaching/posts/2019-3-8_MR_WIYOYWalleye/index.html",
    "title": "Abundance from Mark-Recapture Data",
    "section": "",
    "text": "Mraz (1968) examined the population dynamics of young-of-the-year (YOY) walleye (Sander vitreus) in Pine Lake, an inland lake in Wisconsin. In fall 1962, YOY walleye were captured, marked, and returned to the lake on five sampling dates. On each date, the number of fish caught, the number of caught fish that were previously marked, and the number of marked fish returned to the lake were recorded. The results from these sampling efforts are shown in the table below.\n\n\n\n\n\n\nSample\nCaught\nRecaptured\nReturned\n\n\n\n\n1\n321\n--\n321\n\n\n2\n412\n45\n412\n\n\n3\n178\n55\n178\n\n\n4\n415\n93\n415\n\n\n5\n367\n113\n--\n\n\n\n\n\n\n\n\nUse these results to answer the following questions.\n\nEstimate the initial population size, with 95% confidence interval, of YOY Walleye in Pine Lake. Carefully interpret the result.\nComment on the validity of the assumptions for the model you used to estimate the size of the population.\n\n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\nReferences\n\nMraz, D. 1968. Recruitment, growth, exploitation and management of walleyes in a southeastern Wisconsin lake. Technical Bulletin, Wisconsin Department of Natural Resources, Madison, WI.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_SizeStrux_YPerchTL/index.html",
    "href": "teaching/posts/2019-3-8_SizeStrux_YPerchTL/index.html",
    "title": "Size Structure",
    "section": "",
    "text": "Continuation\n\n\n\nThis exercise is a continuation of this data wrangling exercise and, thus, depends on data frames constructed there. Please load/run your script from that exercise to access the data frame of Yellow Perch captured in beach seines and fyke nets in 2000.\n\n\n\nDescribe the distribution of lengths for Yellow Perch captured in fyke nets in 2000 using both a histogram and frequency table with 10 mm length categories.\nRepeat the previous question with Yellow Perch captured with beach seines.\nStatistically compare the length frequency distributions between Yellow Perch captured with a beach seine and in fyke nets in 2000.\nWrite a short paragraph describing what you learned about the size structure of Trout Lake Yellow Perch, with specific comments about the two gears examined.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_WLLMB_Inch/index.html",
    "href": "teaching/posts/2019-3-8_WLLMB_Inch/index.html",
    "title": "Weight-Length Relationship",
    "section": "",
    "text": "Continuation\n\n\n\nThis exercise is a continuation of this data wrangling exercise and, thus, depends on data frames constructed there. Please load/run your script from that exercise to access the Largemouth Bass only data frame.\n\n\n \n\nBasic Analysis\n\nConstruct graphs appropriate to answer the following questions.1\n\nDescribe the relationship between weight and length (in mm here and throughout).\nDescribe the relationship between log-transformed weight and length.\n\nFrom the plots above there is a clear minimum length for which the weights were precisely obtained. What is that length? [Reduce the data frame to fish greater than this minimum length for the questions below.]\nCompute the weight-length relationship with an appropriate linear regression.\n\nPlot the results (data and the fitted relationship) on both the transformed and raw scales.2 Comment on the fit.\nConstruct a residual plot.3 Comment.\nExpress your results as an equation on the transformed scale.\nExpress your results as an equation on the raw scale.\nCarefully interpret the meaning of the slope of the weight-length relationship.\nIs there statistical evidence for isometric or allometric growth?\n\n\n1 If you completed this graphing exercise then you created the necessary graphs there.2 This post may be useful.3 This post may be useful. \n\n\nExtension\n\nRecompute the weight-length relationship using the original length in inches. How do the slope and y-intercept from this model compare to the results from the previous question?\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2019-3-8_Wrangling_Inch/index.html",
    "href": "teaching/posts/2019-3-8_Wrangling_Inch/index.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Inch Lake is a 12.5 ha inland lake in northern Wisconsin that has been managed as catch-and-release for all species since 2006. Researchers at Northland College have monitored fish populations in Inch Lake since 2007. The total lengths (inches) and weights (g) for subsamples of several species of fish collected from Inch Lake in May of 2007 and 2008 are recorded in InchLake2.1 Use these data to answer the following questions.\n1 See “CSV file” link in “Source” section of linked page.\nCreate a new variable that contains lengths in millimeters.\nRemove the netID and fishID variables.\nCreate a new data frame of just Bluegill.\nCreate a new data frame of just Largemouth Bass.\nCreate a new data frame of non-game species (Bluntnose Minnow, Fathead Minnow, Iowa Darter, and Tadpole Madtom).\nFor the non-game species data frame …\n\nSort by species.\nSort by length within species within year.\n\nFor both the Bluegill and Largemouth Bass only data frames …\n\nChange the names of the weight variable to wt and the length in millimeters variable to tl (if you did not call it that above).\nCreate two new variables that are the common logarithms of the lengths (in mm) and weights.\nAdd appropriate five-cell Gabelhouse length categories.\n\n\n\n\n\n\n\n\nSave Your Script\n\n\n\nThe data frames created here will be used in this graphing, this weight-length relationhip, and this condition exercises.\n\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2022-12-19_AgeComp_WalleyePS/index.html",
    "href": "teaching/posts/2022-12-19_AgeComp_WalleyePS/index.html",
    "title": "Age Comparisons",
    "section": "",
    "text": "Kocovsky and Carline (2000) examined methods to estimate the age of the unexploited population of Walleye in Pymatuning Sanctuary (PA). Scales, dorsal spines, and otoliths were extracted from fish sampled in trap nets during the spawning run in March and April of 1997. Two readers independently examined each structure without knowledge of fish length or sex. Only consensus ages for each structure are recorded in WalleyePS.1\n1 See “CSV file” link in “Source” section of linked page.\nDescribe any apparent bias (or not) between age estimates from scales and otoliths from tabular, graphical, and statistical summaries.\nDoes the sex of the fish impact your descriptions of bias (or not) between scales and otoliths?\nRepeat the previous two questions for dorsal spines and otoliths.\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nKocovsky, P. M., and R. F. Carline. 2000. A comparison of methods for estimating ages of unexploited Walleyes. North American Journal of Fisheries Management 20:1044–1048.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2022-12-19_Using_fishR_Data/index.html",
    "href": "teaching/posts/2022-12-19_Using_fishR_Data/index.html",
    "title": "Using fishR Data with Students",
    "section": "",
    "text": "Note\n\n\n\nNo special packages are loaded for use in this post.\n\n\n \n\nIntroduction\nA large number and variety of data sets are provided in the FSA and FSAdata packages, which I collectively call here the “fishR data.” Lists of the data available in these packages are available alphabetically ordered or arranged by fisheries topic.1 Items on those lists are linked to a documentation file where the origin of the data, its variables, and other items are described. This is a rich source of open-source data that can be used for teaching purposes.\n1 A more comprehensive list of fisheries data in all CRAN packages is here.Data sets in FSA and FSAdata can be, as with data in all packages, accessed with data() by including the name of the data as the first argument and the package name in package=. For example, the WalleyeErie2 data from FSAdata are loaded below.\n\ndata(WalleyeErie2,package=\"FSAdata\")\nhead(WalleyeErie2)\n\n#R|      setID loc grid year  tl   w  sex    mat age\n#R|  1 2003001   1  940 2003 360 460 male mature   2\n#R|  2 2003001   1  940 2003 371 571 male mature   2\n#R|  3 2003001   1  940 2003 375 507 male mature   2\n#R|  4 2003001   1  940 2003 375 584 male mature   2\n#R|  5 2003001   1  940 2003 375 537 male mature   2\n#R|  6 2003001   1  940 2003 376 553 male mature   2\n\n\n \nWhile this method for accessing these data is efficient, I don’t like to use it with students because in the “real world” they will not be accessing data from an R package, rather they will be using their own data stored in some other format. With students just learning R I usually have them load CSV files that I either provide for them or they produce themselves.2 They then load the data with read.csv() from base R.3\n2 We also discuss the advantages of CSV files – lightweight, not proprietary, etc.3 For more advanced students I will use read_csv() from readr. \n\n\nUsing CSV Files\nTo aid use of the fishR data as CSV files we have provided links to the raw CSV files in the R documentation,4 in the on-line documentation, or in the PDF documentation on CRAN. In all instances you will see a highlighted “CSV file” link at the end of the description in the “source” section of documentation (Figure 1). Pressing this link will bring up the raw CSV file which can then be saved to your personal computer.5\n4 For example, (e.g., try ?FSAdata::WalleyeErie2.5 Alternatively, right-click on the CSV link to save the file.\n\n\n\n\n\n\n\nFigure 1: Partial view of the online documentation for WalleyeErie2 data set in FSAdata showing the “CSV file” link.\n\n\n\n\n\n \nStudents can link directly to URL for the CSV file but this requires a connection to the internet (each time the data is loaded) and does not help students learn how to organize data on their own computers.\n\nWalleyeErie2 &lt;- read.csv(\"https://raw.githubusercontent.com/fishR-Core-Team/FSAdata/main/data-raw/WalleyeErie2.csv\")\nhead(WalleyeErie2)\n\n#R|      setID loc grid year  tl   w  sex    mat age\n#R|  1 2003001   1  940 2003 360 460 male mature   2\n#R|  2 2003001   1  940 2003 371 571 male mature   2\n#R|  3 2003001   1  940 2003 375 507 male mature   2\n#R|  4 2003001   1  940 2003 375 584 male mature   2\n#R|  5 2003001   1  940 2003 375 537 male mature   2\n#R|  6 2003001   1  940 2003 376 553 male mature   2\n\n\nHowever, this last example also demonstrates how an instructor could link directly to the CSV file in the resources they provide the student.\n \n\n\nConclusion\nIn summary, we hope you will take advantage of the data resources provided in the FSA and FSAdata packages. However, we encourage you not to have students access the data through data() but instead to use the CSV files linked to in the documentation as described above.\n \n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2022-12-21_SizeStrux_BGLouise/index.html",
    "href": "teaching/posts/2022-12-21_SizeStrux_BGLouise/index.html",
    "title": "Size Structure of Bluegills Collected with Different Gears",
    "section": "",
    "text": "Original Source:\n\n\n\nThis case was modified from Case 16 by Dr. David Willis from Case Studies in Fisheries Conservation & Management: Applied Critical Thinking & Problem Solving. Much of the text is exactly as it appeared in the original chapter, though slightly rearranged. I also added the R analysis portion"
  },
  {
    "objectID": "teaching/posts/2022-12-21_SizeStrux_BGLouise/index.html#motivation",
    "href": "teaching/posts/2022-12-21_SizeStrux_BGLouise/index.html#motivation",
    "title": "Size Structure of Bluegills Collected with Different Gears",
    "section": "Motivation",
    "text": "Motivation\nStudents (and biologists for that matter!) have a tendency to accept sampling data at face value. If a gear type primarily collects small fish then they assume the population is dominated by small fish. If a gear type captures big fish and lots of them then they assume that the population is dominated by large fish. In reality, many biases are possible and are actually very common. To truly understand sampling data, biologists must first understand the biases associated with each gear, and only then will the true nature of the population’s structure (e.g., size or age structure) and dynamics (i.e., recruitment, growth, and mortality) be revealed.\nVarious sampling gears may be differentially effective for different species, and even differentially effective for different sizes of the same species. For example, Largemouth Bass are commonly sampled with electrofishing gear. The numbers and sizes of Largemouth Bass collected can vary widely across seasons. During the spring and fall, more and larger Largemouth Bass tend to be nearshore and vulnerable to the electrofishing gear, which is used in that shallow-water habitat. During midsummer, fewer Largemouth Bass would be sampled at the same locations because many of the larger bass will have moved offshore to deeper water as a result of the warm summer water temperatures.\nIn this case study, you will explore the differential size structure of Bluegills captured by two common sampling gears – electrofishing and trap nets (also known as modified fyke nets).\n\n\n\n\n\nStudents lifting a trap (modified fyke) net."
  },
  {
    "objectID": "teaching/posts/2022-12-21_SizeStrux_BGLouise/index.html#data",
    "href": "teaching/posts/2022-12-21_SizeStrux_BGLouise/index.html#data",
    "title": "Size Structure of Bluegills Collected with Different Gears",
    "section": "Data",
    "text": "Data\nLake Louise is a 45-ha impoundment located in Hand County, South Dakota. The maximum depth is 6.5 m with a mean depth of 2.7 m. Fish were sampled with trap nets that had 1.2- X 1.5-m frames, dual throats, and 19-mm bar mesh. Night electrofishing was undertaken with pulsed DC electricity at approximately 250 V and 8 A. Samples were collected in late May at a water temperature of 23oC.\nTotal length of each Bluegill was recorded for fish captured in both gears in LakeLouiseBG.csv. The variables in this data frame are defined as follows,\n\nlen: The total length (mm) of the sampled fish.\ngear: The gear used to capture the fish. Choices are A and B (described later)."
  },
  {
    "objectID": "teaching/posts/2022-12-23_Depletion_Ruffe/index.html",
    "href": "teaching/posts/2022-12-23_Depletion_Ruffe/index.html",
    "title": "Abundance from Depletion Data",
    "section": "",
    "text": "Czypinski and Ogle (2011) performed an experiment to determine if a population of invasive Ruffe (Gymnocephalus cernuus) could be significantly reduced by sequential removal of Ruffe with bottom trawls. The following catches of Ruffe were made at the Kakagon River location with the corresponding minutes of trawling effort.\nSample    1    2    3    4    5    6    7    8    9   10\n--------------------------------------------------------\nCatch   282  346   27    4    9   27    4    0    1    0\nEffort 19.0 30.0 31.0 20.0 29.5 42.5 34.5 36.0 27.0 25.0 \n \n\nEnter the catch and effort data into separate R vectors and create a vector that represents CPE in each sampling period. Carefully interpret the meaning of the first value in the CPE vector.\nWhat is the best estimate and 95% confidence interval for the initial population size?1\nWhat is the best estimate and 95% confidence interval for the catchability coefficient? Very carefully interpret what this estimate means relative to the capture of Ruffe at this site.\n\n1 Note that Czypinski and Ogle (2011) used the Ricker modified formula. \n\nSuppose that 5 minutes of trawling was considered to be one unit of effort. Convert the effort vector accordingly and calculate a new CPE vector. Carefully interpret the meaning of the first value in the CPE vector.\nWhat is the best estimate and 95% confidence interval for the initial population size? How does this estimate compare to the same estimate from using the original effort data? What does this mean?\nWhat is the best estimate and 95% confidence interval for the catchability coefficient? How does this compare to the same estimate from using the original effort data? What does this mean?\n\n \n\nWhat is the best estimate for the percentage of the Ruffe population that was removed in all trawling efforts at this site?\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nCzypinski, G. D., and D. H. Ogle. 2011. Evaluating the physical removal of Ruffe (Gymnocephalus cernuus) with bottom trawling. Journal of Freshwater Ecology 26:441–443.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/posts/2022-12-23_Removal_Crayfish/index.html",
    "href": "teaching/posts/2022-12-23_Removal_Crayfish/index.html",
    "title": "Abundance from Removal Data",
    "section": "",
    "text": "Chadwick et al. (2021) examined a novel method to determine the abundance of Signal Crayfish (Pacifastacus leniusculus) in an isolated portion of a stream. At one experimental location their novel method resulted in the removal of 1477, 133, and 46 crayfish in three successive samplings. Use these data to answer the following questions.\n\nWhat is the best estimate and 95% confidence interval for the population size?\nWhat is the best estimate and 95% confidence interval for the probability of capture for each sampling period.\nIs there evidence that the probability of capture differed after the first pass?\n\n \n\n\n\n\n\n\nSolution Code:\n\n\n\nAvailable upon request to students not in a class. Contact fishR maintainers.\n\n\n\n\n\n\n\n\n\nReferences\n\nChadwick, D. D. A., E. G. Pritchard, P. Bradley, C. D. Sayer, M. A. Chadwick, L. J. B. Eagle, and J. C. Axmacher. 2021. A novel “triple drawdown” method highlights deficiencies in invasive alien crayfish survey and control techniques. Journal of Applied Ecology 58:316–326.\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "teaching/resources/Crappiesum.html",
    "href": "teaching/resources/Crappiesum.html",
    "title": "Crappie Harvest Summaries",
    "section": "",
    "text": "Table 1: Frequency, percent, and reverse cumulative sum of percent of anglers by number of Crappie harvested.\n\n\n\n\n\n\n\nHarvested\nFrequency\n%\nReverse Cum %\n\n\n\n\n0\n813\n58.4%\n100.0%\n\n\n1\n213\n15.3%\n41.6%\n\n\n2\n58\n4.2%\n26.3%\n\n\n3\n88\n6.3%\n22.1%\n\n\n4\n58\n4.2%\n15.8%\n\n\n5\n33\n2.4%\n11.6%\n\n\n6\n22\n1.6%\n9.3%\n\n\n7\n26\n1.9%\n7.7%\n\n\n8\n10\n0.7%\n5.8%\n\n\n9\n8\n0.6%\n5.1%\n\n\n10\n10\n0.7%\n4.5%\n\n\n11\n33\n2.4%\n3.8%\n\n\n12\n3\n0.2%\n1.4%\n\n\n14\n11\n0.8%\n1.2%\n\n\n15\n6\n0.4%\n0.4%\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nFigure 1: Percentage (left) and reverse cumulative percentage (right) of anglers by number of Crappie harvested."
  },
  {
    "objectID": "teaching/resources/NOPsum.html",
    "href": "teaching/resources/NOPsum.html",
    "title": "Northern Pike Harvest Summaries",
    "section": "",
    "text": "Table 1: Frequency, percent, and reverse cumulative sum of percent of anglers by number of Northern Pike harvested.\n\n\n\n\n\n\n\nHarvested\nFrequency\n%\nReverse Cum %\n\n\n\n\n0\n1166\n66.2%\n100.0%\n\n\n1\n463\n26.3%\n33.8%\n\n\n2\n62\n3.5%\n7.5%\n\n\n3\n70\n4.0%\n4.0%\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nFigure 1: Percentage (left) and reverse cumulative percentage (right) of anglers by number of Northern Pike harvested."
  },
  {
    "objectID": "teaching/resources/WAEsum.html",
    "href": "teaching/resources/WAEsum.html",
    "title": "Walleye Harvest Summaries",
    "section": "",
    "text": "Table 1: Frequency, percent, and reverse cumulative sum of percent of anglers by number of Walleye harvested.\n\n\n\n\n\n\n\nHarvested\nFrequency\n%\nReverse Cum %\n\n\n\n\n0\n4848\n72.9%\n100.0%\n\n\n1\n1112\n16.7%\n27.1%\n\n\n2\n433\n6.5%\n10.4%\n\n\n3\n133\n2.0%\n3.9%\n\n\n4\n67\n1.0%\n1.9%\n\n\n5\n33\n0.5%\n0.9%\n\n\n6\n27\n0.4%\n0.4%\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nFigure 1: Percentage (left) and reverse cumulative percentage (right) of anglers by number of Walleye harvested."
  },
  {
    "objectID": "blog/posts/2025-1-5_ALKPlots_GGplot/index.html",
    "href": "blog/posts/2025-1-5_ALKPlots_GGplot/index.html",
    "title": "Age-Length Key Plots",
    "section": "",
    "text": "Introduction\nIt is often important to examine the structure of an age-length key to ascertain its utility. The plots created with alkPlot() from FSA (see here) provide a quick method to create useful plots for this purpose. However, those plots are generally not of “publication-quality” and because some characteristics of the plots are hard-coded in the function, they are difficult (if not impossible) to customize. Modifying alkPlot() to be more flexible looked to require more work than benefit to be gained. Thus, here, I will demonstrate how to make similar plots using ggplot2 that will allow the user great flexibility for customizing the plots for publication (or exploration).\nThe following packages are loaded for use below.\n\nlibrary(tidyverse)  # for dplyr, tibble, and ggplot2 packages\nlibrary(FSA)        # for lencat(), fact2num(), alkPlot() and WR79 data\n\nI made a simple ggplot2 theme for use below. This can be modified as described, for example, here to further alter the overall appearance of the plots.\n\ntheme_ALK &lt;- function(...) {\n  theme_bw(...) +\n    theme(panel.grid=element_blank(),\n          axis.title=element_text(size=14,face=\"bold\"),\n          axis.text=element_text(size=12))\n} \n\n \n\n\nConstruct Example Age-Length Key\nThis post uses the same WR79 from FSA (described here) to create the same age-length key used in the examples for alkPlot(). The age-length key from these data is created below as demonstrated in Introductory Fisheries Analsyses with R.\n\nWR.age &lt;- droplevels(subset(WR79, !is.na(age)))\nWR.age$LCat &lt;- lencat(WR.age$len,w=5)\nraw &lt;- xtabs(~LCat+age,data=WR.age)\nWR.key &lt;- prop.table(raw, margin=1)\n\n\nround(WR.key,3)\n\n#R|       age\n#R|  LCat      4     5     6     7     8     9    10    11\n#R|    35  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    40  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    45  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    50  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    55  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    60  0.600 0.400 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    65  0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    70  0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    75  0.000 0.889 0.111 0.000 0.000 0.000 0.000 0.000\n#R|    80  0.000 0.250 0.750 0.000 0.000 0.000 0.000 0.000\n#R|    85  0.000 0.000 0.909 0.091 0.000 0.000 0.000 0.000\n#R|    90  0.000 0.000 0.263 0.632 0.105 0.000 0.000 0.000\n#R|    95  0.000 0.000 0.059 0.706 0.176 0.000 0.059 0.000\n#R|    100 0.000 0.000 0.000 0.556 0.167 0.278 0.000 0.000\n#R|    105 0.000 0.000 0.000 0.286 0.429 0.143 0.143 0.000\n#R|    110 0.000 0.000 0.000 0.200 0.200 0.200 0.200 0.200\n#R|    115 0.000 0.000 0.000 0.000 0.000 0.000 1.000 0.000\n\n\nThis age-length key is converted to a data.frame for plotting with ggplot2.\n\nkeydf &lt;- as.data.frame(WR.key)\nstr(keydf)\n\n#R|  'data.frame':  136 obs. of  3 variables:\n#R|   $ LCat: Factor w/ 17 levels \"35\",\"40\",\"45\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#R|   $ age : Factor w/ 8 levels \"4\",\"5\",\"6\",\"7\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#R|   $ Freq: num  1 1 1 1 1 0.6 0 0 0 0 ...\n\nlevels(keydf$age)\n\n#R|  [1] \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\"\n\n\nBy default the LCat and age variables are factors in the new data.frame.1 Having age as a factor is useful, but the naturally increasing order of the levels (see above) causes the vertical ordering of ages in the plots below to be opposite of what is desired. Thus, the order of the age levels is reversed with fct_rev() from forcats and the name is changed to fage to make it clear that it is a factor. Some of the plots below require age as a numeric, rather than a factor, so the numeric ages are added to the data.frame in nage using fact2num() from FSA.2\n1 One could use stringsAsFactors=FALSE in as.data.frame() but having those two variables as strings is not helpful.2 fact2num() is used because it maintains the correct numerical value for each level of the factor.\nkeydf &lt;- keydf |&gt;\n  dplyr::mutate(age=forcats::fct_rev(age),\n                nage=FSA::fact2num(age)) |&gt;\n  dplyr::rename(fage=age)\nstr(keydf)\n\n#R|  'data.frame':  136 obs. of  4 variables:\n#R|   $ LCat: Factor w/ 17 levels \"35\",\"40\",\"45\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#R|   $ fage: Factor w/ 8 levels \"11\",\"10\",\"9\",..: 8 8 8 8 8 8 8 8 8 8 ...\n#R|   $ Freq: num  1 1 1 1 1 0.6 0 0 0 0 ...\n#R|   $ nage: num  4 4 4 4 4 4 4 4 4 4 ...\n\nlevels(keydf$fage)\n\n#R|  [1] \"11\" \"10\" \"9\"  \"8\"  \"7\"  \"6\"  \"5\"  \"4\"\n\n\nThere is no value below to having LCat as a factor so it is changed to a numeric using fact2num(). Finally, Freq is a proportion and not a “count” so it is changed to the more accurate prop below.\n\nkeydf &lt;- keydf |&gt;\n  dplyr::mutate(LCat=FSA::fact2num(LCat)) |&gt;\n  dplyr::rename(prop=Freq)\nstr(keydf)\n\n#R|  'data.frame':  136 obs. of  4 variables:\n#R|   $ LCat: num  35 40 45 50 55 60 65 70 75 80 ...\n#R|   $ fage: Factor w/ 8 levels \"11\",\"10\",\"9\",..: 8 8 8 8 8 8 8 8 8 8 ...\n#R|   $ prop: num  1 1 1 1 1 0.6 0 0 0 0 ...\n#R|   $ nage: num  4 4 4 4 4 4 4 4 4 4 ...\n\n\nAll of the previous modifications to the age-length key data.frame could be performed more efficiently in the following chain of commands.\n\nkeydf &lt;- as.data.frame(WR.key) |&gt;\n  dplyr::mutate(age=forcats::fct_rev(age),\n                nage=FSA::fact2num(age),\n                LCat=FSA::fact2num(LCat)) |&gt;\n  dplyr::rename(fage=age,\n                prop=Freq)\n\nFinally, many of the length-age combinations in the age-length key are 0. Some of the plots made below are better made without the zeroes, whereas some are better with the zeroes. Thus, a second data.frame without zeroes is needed.\n\nkeydf_nozeroes &lt;- keydf |&gt;\n  dplyr::filter(prop&gt;0)\n\n\n\nStacked Barplot\nThe age-length key may be visualized as a stacked bar (or column) plot with proportion at age on the y-axis, length categories on the x-axis, and stacks colored according to the factored age. Note that the data.frame without zeroes and fage is used. Further note that younger ages start at the bottom of the bars because the order of the levels was reversed in fage.\n\nggplot(data=keydf_nozeroes,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_col() +\n  theme_ALK()\n\n\n\n\n\n\n\n\nThis plot can be improved by providing better labels for the axes, providing more length labels on the x-axis, and minimizing the extra vertical and horizontal spaced caused by expanding the x- and y-axes.\n\nggplot(data=keydf_nozeroes,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_col() +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK()\n\n\n\n\n\n\n\n\nI also prefer to include age labels in the bars rather than having a legend. These labels may be added with geom_text().3 but the position needs to be controlled with position_stack() due to the stacking of the bars. vjust=0.5 is used in position_stack() so that the label will be placed at the halfway point (vertically) of the bar. With these labels, the legend is no longer needed and is removed within theme().\n3 geom_label() provides a slightly different look.\nggplot(data=keydf_nozeroes,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_col() +\n  geom_text(mapping=aes(label=fage),\n            size=3,position=position_stack(vjust=0.5)) +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\nBubble Plot\nAnother useful plot for visualizing the age-length key is to plot a point at each age-length position (for which fish existed) and scale the size of the point to the proportion found in the age-length key. The start of the plot is shown below, noting that the data.frame without zeroes and nage is used.\n\nggplot(data=keydf_nozeroes,mapping=aes(y=nage,x=LCat)) +\n  geom_point() +\n  scale_y_continuous(name=\"Age\",expand=expansion(mult=0.05),breaks=4:11) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.05),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nTwo modifications are required to scale the size of the points. First, the size attribute within geom_point() should be mapped to “proportions” from the age-length key data.frame (i.e., prop). I also mapped color to the factored ages (i.e., fage), but that is superfluous as all points corresponding to each age on the y-axis will be the same color.\n\nggplot(data=keydf_nozeroes,mapping=aes(y=nage,x=LCat)) +\n  geom_point(mapping=aes(size=prop,color=fage)) +\n  scale_y_continuous(name=\"Age\",expand=expansion(mult=0.05),breaks=4:11) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.05),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nThe second modification is to better control how the sizes of the points are determined with scale_size(). Below breaks= is used to create breaks in the range of values of the size= variable (i.e., prop) from 0 to 1 in steps of 0.1 (so, 10 breaks across the range of possible prop values). range= is then used to set a range of “size” values for the points. Here the range is set from 1 to 10 so that size=1 would be used for the smallest break in prop and size=10 would be used for the largest break in prop. In other words, the size of the points will range from 1 to 10 based on where the value of prop is in the breaks= sequence.4\n4 You will likely need to try different values for range= to get “bubble” sizes that you prefer.\nggplot(data=keydf_nozeroes,mapping=aes(y=nage,x=LCat)) +\n  geom_point(mapping=aes(size=prop,color=fage)) +\n  scale_size(range=c(1,10),breaks=seq(0,1,0.1)) +\n  scale_y_continuous(name=\"Age\",expand=expansion(mult=0.05),breaks=4:11) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.05),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\nLine Plot\n“Line plots” of the age-length key are easily constructed from the data.frame with zeroes using geom_line().5\n5 size= is used here to make the lines a little heavier than the default.\nggplot(data=keydf,mapping=aes(y=prop,x=LCat,color=fage)) +\n  geom_line(size=1) +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK()\n\n\n\n\n\n\n\n\nAgain I prefer to have labels on the plot rather than in a legend if possible. In alkPlot() from FSA the line plot has age labels at the peak of each line. Adding these labels requires a separate data.frame that contains the portion of the age-length key data.frame that corresponds to the first instance of the maximum prop for each age. This data.frame is constructed below.\n\nagelbldf &lt;- keydf |&gt;\n  group_by(fage) |&gt;\n  slice(which.max(prop)) |&gt;\n  as.data.frame()   # to remove tibble and grouping\nagelbldf\n\n#R|    LCat fage      prop nage\n#R|  1  110   11 0.2000000   11\n#R|  2  115   10 1.0000000   10\n#R|  3  100    9 0.2777778    9\n#R|  4  105    8 0.4285714    8\n#R|  5   95    7 0.7058824    7\n#R|  6   85    6 0.9090909    6\n#R|  7   65    5 1.0000000    5\n#R|  8   35    4 1.0000000    4\n\n\nThe age labels are included on the plot by first moving data= and mapping= from ggplot() to geom_line().6 Then include the second data.frame in geom_label()7 with the the label= aesthetic to place the labels. fill= was used to fill the label box with the same color as the line, size= was used to increase the size of the text, and alpha= was used to make the color of the label box semi-transparent so that the lines behind them could still be seen. Furthermore, the expansion for the x-axis and the top of the y-axis were increased to make room for the labels. and the now unnecessary legend was removed.\n6 This is needed when separate data.frames are used to make different aspects of the plot.7 geom_text() provides a slightly different look.\nggplot() +\n  geom_line(data=keydf,mapping=aes(y=prop,x=LCat,color=fage),\n            size=1) +\n  geom_label(data=agelbldf,mapping=aes(y=prop,x=LCat,label=fage,fill=fage),\n            size=4,alpha=0.5) +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.04))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.04),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\nArea Plot\nArea plots are similar to the barplot from above. However, I have found these plots to be the hardest to make look nice and to modify using base R plotting functions and, thus, I did not use them often. Here is a start for constructing an area plot with ggplot2 using geom_area().\n\nggplot(data=keydf,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_area() +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK()\n\n\n\n\n\n\n\n\nAs before, I prefer to label the agess within the areas, rather than as a legend. I could not find an automated way to do this. Thus, the general method for labeling the lines in the line plot will be followed, except that the data.frame will have to be created by manually filling the LCat and prop values for where the labels should be placed. In practice this will likely take some trial-and-error as the positions are “eye-balled.” Below I use tribble() from tibble because I like how the data can be entered by rows rather than by columns.\n\nagelbldf &lt;- tibble::tribble(\n  ~LCat,~prop,~lbl,\n   47, 0.50,  \"4\",\n   70, 0.50,  \"5\",\n   83, 0.50,  \"6\",\n   95, 0.50,  \"7\",\n  103, 0.62,  \"8\",\n  101, 0.85,  \"9\",\n  112, 0.70, \"10\",\n  110, 0.95, \"11\"\n)\n\nThis data.frame is then used with either geom_text() or geom_label() to add the labels to the appropriate areas. Note that vjust=0.5 and hjust=0.5 center the text label on the given coordinates.\n\nggplot() +\n  geom_area(data=keydf,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_text(data=agelbldf,mapping=aes(y=prop,x=LCat,label=lbl),\n             size=4,vjust=0.5,hjust=0.5) +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n \n\n\nWhy These Modifications?\nI prefer these plots to those produced by alkPlot() (even though I wrote the code for all of those plots) because the full arsenal of ggplot2 commands can be used to modify the plots and because these plots more readily handle some issues with age-length keys.\nIt is beyond the scope of this post to show all the ways these plots can be modified with ggplot2 code. A simple example is to replace the rather garish colors used above. For example, the fill color on the barplot can be changed to a scale of blues with scale_fill_brewer() as shown below. Many other color schemes can be used (see this as one (of many) demonstration of choosing colors.)\n\nggplot(data=keydf_nozeroes,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_col() +\n  geom_text(mapping=aes(label=fage),\n            size=3,position=position_stack(vjust=0.5)) +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nYou may want to avoid color altogether and use a grey scale when preparing for print publications. One could use scale_color_brewer(palette=\"Greys\") but this tends to leave the last age as white, which may not be effective. Instead use scale_color_grey() and use start= and end= to control the levels of grey (note that 0 is black and 1 is white).\n\nggplot(data=keydf_nozeroes,mapping=aes(y=nage,x=LCat)) +\n  geom_point(mapping=aes(size=prop,color=fage)) +\n  scale_size(range=c(1,10),breaks=seq(0,1,0.1)) +\n  scale_y_continuous(name=\"Age\",expand=expansion(mult=0.05),breaks=4:11) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.05),\n                     breaks=seq(0,1000,5)) +\n  scale_color_grey(start=0.8,end=0.2) +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nThis post originally started because a user noted that alkPlot() plots were not appropriate when some of the length classes were missing from the age-length key. For example, suppose that no fish existed from 55 to 70 cm in the original data.frame.\n\nWR.age &lt;- droplevels(subset(WR79, !is.na(age)))\nWR.age &lt;- subset(WR.age,len&lt;55 | len&gt;70)        # remove 55-70 cm fish\nWR.age$LCat &lt;- lencat(WR.age$len,w=5)\nraw &lt;- xtabs(~LCat+age,data=WR.age)\nWR.key &lt;- prop.table(raw, margin=1)\n\nSee how the alkPlot() plot does not clearly show the gap in the data for those lengths (i.e., the 50- and 70-cm bars are right next to each other).\n\nalkPlot(WR.key)\n\n\n\n\n\n\n\n\nHowever, the similar ggplot2 plot, though clearly requiring more code from the user, properly shows the missing length categories.\n\nWR.keydf &lt;- as.data.frame(WR.key) |&gt;\n  dplyr::mutate(age=forcats::fct_rev(age),\n                nage=FSA::fact2num(age),\n                LCat=FSA::fact2num(LCat)) |&gt;\n  dplyr::rename(fage=age,\n                prop=Freq) |&gt;\n  dplyr::filter(prop&gt;0)\n\nggplot(data=WR.keydf,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_col() +\n  geom_text(mapping=aes(label=fage),\n            size=3,position=position_stack(vjust=0.5)) +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n \n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{h._ogle2025,\n  author = {H. Ogle, Derek},\n  title = {Age-Length {Key} {Plots}},\n  date = {2025-01-05},\n  url = {https://fishr-core-team.github.io/fishR/blog/posts/2025-1-5_ALKPlots_GGplot/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nH. Ogle, D. 2025, January 5. Age-Length Key Plots. https://fishr-core-team.github.io/fishR/blog/posts/2025-1-5_ALKPlots_GGplot/."
  },
  {
    "objectID": "blog/posts/2025-1-5_ALKPlots_GGplot/index.html#easy-modifications",
    "href": "blog/posts/2025-1-5_ALKPlots_GGplot/index.html#easy-modifications",
    "title": "Age-Length Key Plots",
    "section": "Easy Modifications",
    "text": "Easy Modifications\nggplot2 provides a wealth of ways in which these plots can be modified. It is beyond the scope of this post to show all the ways these plots could be modified, but a simple example is to replace the rather garish colors used above. For example, the fill color can be changed to a scale of blues with scale_fill_brewer() as shown below. Many other color schemes can be used (see this as one (of many) demonstration of choosing colors.)\n\nggplot(data=keydf_nozeroes,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_col() +\n  geom_text(mapping=aes(label=fage),\n            size=3,position=position_stack(vjust=0.5)) +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_ALK() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nYou may want to avoid color altogether and use a grey scale when preparing for print publications. One could use scale_color_brewer(palette=\"Greys\") but this tends to leave the last age as white, which may not be effective. Instead use scale_color_grey() and use the start= and end= to control the level of grey to start and end with (note that 0 s black and 1 is white).\n\nggplot(data=keydf_nozeroes,mapping=aes(y=nage,x=LCat)) +\n  geom_point(mapping=aes(size=prop,color=fage)) +\n  scale_size(range=c(1,10),breaks=seq(0,1,0.1)) +\n  scale_y_continuous(name=\"Age\",expand=expansion(mult=0.05),breaks=4:11) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.05),\n                     breaks=seq(0,1000,5)) +\n  scale_color_grey(start=0.8,end=0.2) +\n  theme_ALK() +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "blog/posts/2025-1-5_ALKPlots_GGplot/index.html#handling-odd-age-length-keys",
    "href": "blog/posts/2025-1-5_ALKPlots_GGplot/index.html#handling-odd-age-length-keys",
    "title": "Age-Length Key Plots",
    "section": "Handling Odd Age-Length Keys",
    "text": "Handling Odd Age-Length Keys\nThis post started because a user noted that the plots from alkPlot() were not appropriate when some of the length classes were missing from the age-length key. For example, suppose that no fish existed from 55 to 70 cm in the original data.frame.\n\nWR.age &lt;- droplevels(subset(WR79, !is.na(age)))\nWR.age &lt;- subset(WR.age,len&lt;55 | len&gt;70)\nWR.age$LCat &lt;- lencat(WR.age$len,w=5)\nraw &lt;- xtabs(~LCat+age,data=WR.age)\nWR.key &lt;- prop.table(raw, margin=1)\nround(WR.key,3)\n\n#R|       age\n#R|  LCat      4     5     6     7     8     9    10    11\n#R|    35  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    40  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    45  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    50  1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    70  0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000\n#R|    75  0.000 0.889 0.111 0.000 0.000 0.000 0.000 0.000\n#R|    80  0.000 0.250 0.750 0.000 0.000 0.000 0.000 0.000\n#R|    85  0.000 0.000 0.909 0.091 0.000 0.000 0.000 0.000\n#R|    90  0.000 0.000 0.263 0.632 0.105 0.000 0.000 0.000\n#R|    95  0.000 0.000 0.059 0.706 0.176 0.000 0.059 0.000\n#R|    100 0.000 0.000 0.000 0.556 0.167 0.278 0.000 0.000\n#R|    105 0.000 0.000 0.000 0.286 0.429 0.143 0.143 0.000\n#R|    110 0.000 0.000 0.000 0.200 0.200 0.200 0.200 0.200\n#R|    115 0.000 0.000 0.000 0.000 0.000 0.000 1.000 0.000\n\n\nNote how the result from alkPlot() does not clearly show the gap in the data for those lengths (i.e., the 50- and 70-cm bars are right next to each other).\n\nalkPlot(WR.key)\n\n\n\n\n\n\n\n\nHowever, the similar plot using ggplot2 does properly show the missing length categories.\n\nWR.keydf &lt;- as.data.frame(WR.key) |&gt;\n  dplyr::mutate(age=forcats::fct_rev(age),\n                nage=FSA::fact2num(age),\n                LCat=FSA::fact2num(LCat)) |&gt;\n  dplyr::rename(fage=age,\n                prop=Freq) |&gt;\n  dplyr::filter(prop&gt;0)\n\nggplot(data=WR.keydf,mapping=aes(y=prop,x=LCat,fill=fage)) +\n  geom_col() +\n  geom_text(mapping=aes(label=fage),\n            size=3,position=position_stack(vjust=0.5)) +\n  scale_y_continuous(name=\"Proportion\",expand=expansion(mult=c(0,0.01))) + \n  scale_x_continuous(name=\"Total Length (cm)\",expand=expansion(mult=0.01),\n                     breaks=seq(0,1000,5)) +\n  theme_ALK() +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html",
    "href": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html",
    "title": "FSA Analyses for Multiple Groups",
    "section": "",
    "text": "A year or so ago a user of FSA asked me if there was a way to use group_by() from dplyr() with removal() from FSA to efficiently estimate fish abundance from removal data for multiple groups. There was not, as far as either of us could tell. My naive impression was that I would need a new version of removal() that used formula notation with a data= argument. So, I modified removal to use a formula and data=.\nHowever, this was not an adequate solution because I did not want to change the contents of the list that was returned from removal(). Furthermore required both coef() and confint() (S3 extractor functions for removal()) to extract the results the user wanted from this list. My solution for this was to modify confint() with incl.est= that would append the confidence intervals to the point estimate (extracted with coef()) when set to TRUE.\nHowever, confint() returned a matrix which I found difficult to work with within any of the dplyr functions. Thus, I further modified confint()1 with as.df= to return a data.frame rather than a matrix when set to TRUE. Both incl.est= and as.df= default to FALSE so that the original functionality is maintained (by default).2 Thus, to get the new functionality both incl.est= and as.df= must be explicitly set to TRUE by the user.\n1 And coef() and summary() while I was at it.2 Trying not to break user’s legacy code.These changes seemed to meet the user’s request. Thus, I made similar changes to depletion(), catchCurve(), and chapmanRobson(). In this post, I demonstrate how these new functions work vis-a-vis helper functions from dplyr (and, briefly, purrr).\nThe following packages are loaded for use below. Note that the modified removal(), depletion(), catchCurve(), and chapmanRobson() functionality requires FSA v0.9.6 or higher.3\n3 This version is at CRAN being processed when this post was written.\nlibrary(FSA)       # for depletion(), removal(), catchCurve(), chapmanRobson(), headtail()\nlibrary(tidyverse) # for dplyr, tidyr"
  },
  {
    "objectID": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html#depletion-estimates",
    "href": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html#depletion-estimates",
    "title": "FSA Analyses for Multiple Groups",
    "section": "Depletion Estimates",
    "text": "Depletion Estimates\nThe Pathfinder data9 from FSAdata contains the catch and effort for three Snapper species (Pristipomoides zonatus, Pristipomoides auricilla, and Etelis carbunculUs) in a depletion experiment around Pathfinder Reef in the Mariana Archipelago. Polovina (1985) used these data to demonstrate the need for a Leslie depletion model with variable recruitment (as was evident for Pristipomoides auricilla). Here these data are used to demonstrate how to efficiently compute population (No) and catchability (q) estimates for each species using depletion() from FSA with help from dplyr.\n9 See description here.\ndata(Pathfinder,package=\"FSAdata\")\nheadtail(Pathfinder)\n\n#R|       date effort Pzonatus Pauricilla Ecarbunculus\n#R|  1  10-Apr   27.5       98         12           42\n#R|  2  11-Apr   23.7      111         17           22\n#R|  3  12-Apr   21.3       47         12           41\n#R|  11  5-May   20.3       40         29           13\n#R|  12  6-May   22.8       35         35           21\n#R|  13  7-May   24.1       30         27           15\n\n\nThe data are provided in “wide” format where the catch of each species is presented as a column or variable in Pathfinder. The methods used below require data to be in “long” format where each row corresponds to one observation – in this case catch (and effort and other associated date) for one species (on one day). In some cases the data may have been entered in long format, but when it is not it must be converted to long format for the process used below.\nThere are several methods for converting from wide to long formats. In my mind, one of the easiest is pivot_longer() from tidyr.10 The names of columns that contain the values to be converted from wide to long are given to cols=. In this example, that will be the columns containing the catches of the separate species. Because those columns are contiguous in the original data.frame they can be referred to en masse by separating the name of the left-most column from the name for the right-most column with a colon. The three catch columns from the original data.frame will be moved to two columns in the resulting data.frame – one with the catch values and one with the spcies names. A name for the column of names can be given to names_to= and a name for the values column can be given to values_to=. Thus, Pathfinder is converted from wide to long format below.\n10 Some alternatives are reshape() from base R and melt() from reshape2.\nPathfinder &lt;- Pathfinder |&gt;\n  tidyr::pivot_longer(cols=Pzonatus:Ecarbunculus,\n                      names_to=\"species\",values_to=\"catch\")\nstr(Pathfinder)\n\n#R|  tibble [39 × 4] (S3: tbl_df/tbl/data.frame)\n#R|   $ date   : Factor w/ 13 levels \"10-Apr\",\"11-Apr\",..: 1 1 1 2 2 2 3 3 3 4 ...\n#R|   $ effort : num [1:39] 27.5 27.5 27.5 23.7 23.7 23.7 21.3 21.3 21.3 29.7 ...\n#R|   $ species: chr [1:39] \"Pzonatus\" \"Pauricilla\" \"Ecarbunculus\" \"Pzonatus\" ...\n#R|   $ catch  : int [1:39] 98 12 42 111 17 22 47 12 41 91 ...\n\n\nOnce the data are in long format, depletion() can be used within group_modify() as shown for removal(). However, note that depletion() requires a formula of the form catch~effort and that groups are defined by only the single variable species here.\n\nres &lt;- Pathfinder %&gt;%\n  dplyr::group_by(species) %&gt;%\n    dplyr::group_modify(~confint(depletion(catch~effort,data=.x),\n                                 incl.est=TRUE,as.df=TRUE)) %&gt;%\n    as.data.frame() # removes tibble and grouping structure\n\n\nres %&gt;% \n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"No\"), \\(x) round(x,digits=0))) %&gt;%\n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"p\"), \\(x) round(x,digits=3)))\n\n#R|         species   No No_LCI No_UCI            q        q_LCI         q_UCI\n#R|  1 Ecarbunculus  571    224    918  0.002499027  0.000399448  0.0045986060\n#R|  2   Pauricilla -165     34   -363 -0.003134290 -0.005368398 -0.0009001819\n#R|  3     Pzonatus 1062    795   1328  0.003678569  0.002225597  0.0051315404"
  },
  {
    "objectID": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html#removal-estimates",
    "href": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html#removal-estimates",
    "title": "FSA Analyses for Multiple Groups",
    "section": "Removal Estimates",
    "text": "Removal Estimates\nThe new functionality of removal() is demonstrated with the user’s (partial) data, entered manually below.4\n4 I usually enter data into a CSV file external to R. This is for demonstration only.\nd &lt;- data.frame(lake=factor(rep(c(\"Ash Tree\",\"Bark\",\"Clay\"),each=5)),\n                year=factor(rep(c(\"2010\",\"2011\",\"2010\",\"2011\",\"2010\",\"2011\"),\n                                times=c(2,3,3,2,2,3))),\n                pass=factor(c(1,2,1,2,3,1,2,3,1,2,1,2,1,2,3)),\n                catch=c(57,34,65,34,12,54,26,9,54,27,67,34,68,35,12))\nd\n\n#R|         lake year pass catch\n#R|  1  Ash Tree 2010    1    57\n#R|  2  Ash Tree 2010    2    34\n#R|  3  Ash Tree 2011    1    65\n#R|  4  Ash Tree 2011    2    34\n#R|  5  Ash Tree 2011    3    12\n#R|  6      Bark 2010    1    54\n#R|  7      Bark 2010    2    26\n#R|  8      Bark 2010    3     9\n#R|  9      Bark 2011    1    54\n#R|  10     Bark 2011    2    27\n#R|  11     Clay 2010    1    67\n#R|  12     Clay 2010    2    34\n#R|  13     Clay 2011    1    68\n#R|  14     Clay 2011    2    35\n#R|  15     Clay 2011    3    12\n\n\nThere are several things to note with these data. First, they are in “long” format where each row corresponds to one observation – in this case the catch of fish on a particular pass within a given year and lake. Second, the user wanted to estimated abundance from catches (i.e., removal) on multiple passes for all possible lake-year combinations; thus, “group” is defined by the combination of lake and year. Third, the number of passes was not consistent, with some year-lake combinations having two and others having three passes. This is not an issue as long as a removal method is used that can handle both numbers of removal. The default Carle-Strub method5 will be used here, so this will not be an issue. Fourth, it is important when data are in “long” format like this that the passes are ordered consecutively from first to last. That is the case here, but if it was not then the data.frame would need to be sorted on pass (within lake and year).\n5 See method= argument in removal() for other methods.6 If the variable is not called catch as it is here, then replace with the actual variable name.removal() from FSA is used to compute estimates of initial population abundance (No) and probability of capture (p) by using a formula of the form ~catch with the data.frame containing the variable in data=.6 The result of this function is assigned to an object which is then given to confint() to extract confidence intervals for the parameters. Including incl.est=TRUE in confint() will include the point estimate in the result. Below is an example of using depletion() for just Ash Tree lake in 2010.\n\ntmp &lt;- removal(~catch,data=dplyr::filter(d,lake==\"Ash Tree\",year==\"2010\"))\nconfint(tmp,incl.est=TRUE)\n\n#R|             Est    95% LCI     95% UCI\n#R|  No 130.0000000 78.8281665 181.1718335\n#R|  p    0.4482759  0.2107166   0.6858351\n\n\nThe object returned by confint() is a matrix by default, which generally prints nicely. However, the result may be returned as a data.frame, which as described previously will be needed with the dplyr functions used below, by including as.df=TRUE in confint().\n\nconfint(tmp,incl.est=TRUE,as.df=TRUE)\n\n#R|     No   No.LCI   No.UCI         p     p.LCI     p.UCI\n#R|  1 130 78.82817 181.1718 0.4482759 0.2107166 0.6858351\n\n\nremoval() can be used to compute parameter estimates for multiple groups by first giving the variable or variables that identify groups to group_by() and then giving that to group_modify() from dplyr. The first argument to group_modify(), when piped together with %&gt;%, is an expression that begins with ~ followed by a call to removal() nested within a call to confint(). In the removal() call data= must be set to .x. The call to confint() should include incl.est=TRUE to get the point estimates of the parameters and must include as.df=TRUE for group_modify() to work properly.7 Below I submit the result to as.data.frame() simply to remove the grouping structure and tibble class.8\n7 group_modify() requires the function defined after the ~ to return a data.frame.8 I prefer to work with data.frames rather than tibbles.\nres &lt;- d %&gt;%\n  dplyr::group_by(lake,year) %&gt;%\n  dplyr::group_modify(~confint(removal(~catch,data=.x),\n                               incl.est=TRUE,as.df=TRUE)) %&gt;%\n  as.data.frame() # removes tibble and grouping structure\n\nFor display purposes, the results for the two parameters are rounded to different numbers of digits.\n\nres %&gt;% \n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"No\"), \\(x) round(x,digits=0))) %&gt;%\n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"p\"), \\(x) round(x,digits=3)))\n\n#R|        lake year  No No.LCI No.UCI     p p.LCI p.UCI\n#R|  1 Ash Tree 2010 130     79    181 0.448 0.211 0.686\n#R|  2 Ash Tree 2011 121    110    132 0.558 0.440 0.676\n#R|  3     Bark 2010  95     87    103 0.589 0.464 0.715\n#R|  4     Bark 2011 103     74    132 0.533 0.314 0.752\n#R|  5     Clay 2010 130     96    164 0.523 0.324 0.723\n#R|  6     Clay 2011 125    114    136 0.564 0.449 0.679"
  },
  {
    "objectID": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html#catch-curves",
    "href": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html#catch-curves",
    "title": "FSA Analyses for Multiple Groups",
    "section": "Catch Curves",
    "text": "Catch Curves\nA similar process can be followed with catchCurve() to estimate mortality for several groups. However, the “long” format data.frame either needs to contain only data for ages on the descending limb for each group or groups that have the same ages on the descending limb. Both instances are illustrated below.\nFHCatfish from FSAdata11 contains numbers of Flathead Catfish (Pylodictis olivaris) captured by electrofishing in three rivers – Coosa River, AL; Ocmulgee River, GA; and Satilla River, GA – for ages ONLY on the descending limb of the catch curve.\n11 See description here.\ndata(FHCatfish,package=\"FSAdata\")\nheadtail(FHCatfish)\n\n#R|       river age abundance\n#R|  1    Coosa   2        25\n#R|  2    Coosa   3        24\n#R|  3    Coosa   4        18\n#R|  37 Satilla   6         1\n#R|  38 Satilla   7         1\n#R|  39 Satilla  10         1\n\n\nThe process here is essentially the same as it was for depletion() except noting that catchCurve() uses a formula of the form catch~age.\n\nres &lt;- FHCatfish %&gt;%\n  dplyr::group_by(river) %&gt;%\n  dplyr::group_modify(~confint(catchCurve(abundance~age,data=.x),\n                               incl.est=TRUE,as.df=TRUE)) %&gt;%\n  as.data.frame() # removes tibble and grouping structure\n\n\nres %&gt;% \n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"Z\"), \\(x) round(x,digits=3))) %&gt;%\n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"A\"), \\(x) round(x,digits=1)))\n\n#R|       river     Z Z_LCI Z_UCI    A A_LCI A_UCI\n#R|  1    Coosa 0.164 0.123 0.205 15.1  11.5  18.5\n#R|  2 Ocmulgee 0.269 0.194 0.344 23.6  17.6  29.1\n#R|  3  Satilla 0.683 0.195 1.171 49.5  17.7  69.0\n\n\nWalleyeKS from FSAdata contains the catch of Walleye at all observed ages in eight reservoirs in Kansas.12 The descending limb was determined to start at age-2 for fish from each reservoir, even though other ages appear in the data.frame. Thus, ages2use= must be used in catchCurve() to identify the ages on the descending limb. Here a sequence of ages beginning with 2 and ending with the maximum observed age was used.13 Otherwise the code below is similar to that above.\n12 See description here.13 This will result in a sequence of warnings from catchCurve() as some of the groups do not have fish of the maximum age.\ndata(WalleyeKS,package=\"FSAdata\")\nstr(WalleyeKS)\n\n#R|  'data.frame':  66 obs. of  3 variables:\n#R|   $ reservoir: Factor w/ 8 levels \"Cedar.Bluff\",..: 1 1 1 1 1 1 1 1 2 2 ...\n#R|   $ age      : int  0 1 2 3 4 5 6 7 0 1 ...\n#R|   $ catch    : int  78 70 104 52 33 13 4 2 131 28 ...\n\n( maxage &lt;- max(WalleyeKS$age) )\n\n#R|  [1] 11\n\nres &lt;- WalleyeKS %&gt;%\n  dplyr::group_by(reservoir) %&gt;%\n  dplyr::group_modify(~confint(catchCurve(catch~age,data=.x,ages2use=2:11),\n                               incl.est=TRUE,as.df=TRUE)) %&gt;%\n  as.data.frame() # removes tibble and grouping structure\n\nres %&gt;% \n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"Z\"), \\(x) round(x,digits=3))) %&gt;%\n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"A\"), \\(x) round(x,digits=1)))\n\n#R|      reservoir     Z Z_LCI Z_UCI    A A_LCI A_UCI\n#R|  1 Cedar.Bluff 0.811 0.664 0.958 55.6  48.5  61.6\n#R|  2      Cheney 0.551 0.174 0.928 42.4  16.0  60.5\n#R|  3  Glen.Elder 0.855 0.516 1.194 57.5  40.3  69.7\n#R|  4      Kirwin 0.941 0.493 1.388 61.0  38.9  75.1\n#R|  5    Lovewell 0.544 0.279 0.809 41.9  24.3  55.5\n#R|  6      Marion 0.613 0.346 0.879 45.8  29.3  58.5\n#R|  7     Webster 0.567 0.132 1.002 43.3  12.4  63.3\n#R|  8      Wilson 0.719 0.555 0.884 51.3  42.6  58.7"
  },
  {
    "objectID": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html#chapman-robson",
    "href": "blog/posts/2025-1-8_FSA_Mult_Groups/index.html#chapman-robson",
    "title": "FSA Analyses for Multiple Groups",
    "section": "Chapman-Robson",
    "text": "Chapman-Robson\nThe process for the Chapman-Robson method is the same as that for the catch-curve except that chapmanRobson() is used instead of catchCurve().14\n14 Also note that S is returned rather than A.\nres &lt;- WalleyeKS %&gt;%\n  dplyr::group_by(reservoir) %&gt;%\n  dplyr::group_modify(~confint(chapmanRobson(catch~age,data=.x,ages2use=2:11),\n                               incl.est=TRUE,as.df=TRUE)) %&gt;%\n  as.data.frame() # removes tibble and grouping structure\n\nres %&gt;% \n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"S\"), \\(x) round(x,digits=1))) %&gt;%\n  dplyr::mutate(dplyr::across(dplyr::starts_with(\"Z\"), \\(x) round(x,digits=3)))\n\n#R|      reservoir    S S_LCI S_UCI     Z Z_LCI Z_UCI\n#R|  1 Cedar.Bluff 46.9  42.0  51.9 0.754 0.662 0.846\n#R|  2      Cheney 53.5  48.1  58.9 0.623 0.162 1.085\n#R|  3  Glen.Elder 45.1  42.7  47.4 0.796 0.665 0.927\n#R|  4      Kirwin 39.0  31.1  47.0 0.930 0.732 1.129\n#R|  5    Lovewell 59.5  56.6  62.3 0.519 0.215 0.823\n#R|  6      Marion 59.2  56.5  61.9 0.524 0.308 0.739\n#R|  7     Webster 57.7  50.8  64.7 0.546 0.361 0.731\n#R|  8      Wilson 47.1  42.7  51.5 0.751 0.583 0.919"
  }
]